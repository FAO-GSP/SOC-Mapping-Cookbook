<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Soil Organic Carbon Mapping Cookbook</title>
  <meta name="description" content="The soilll cookbook.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Soil Organic Carbon Mapping Cookbook" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="The soilll cookbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Soil Organic Carbon Mapping Cookbook" />
  
  <meta name="twitter:description" content="The soilll cookbook." />
  <meta name="twitter:image" content="images/cover.png" />

<meta name="author" content="Food and Agriculture Organization of the United Nations, Rome, 2018">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="covariates.html">
<link rel="next" href="chvalidation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>List of contributors</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#editorial-board"><i class="fa fa-check"></i>Editorial Board</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributing-authors"><i class="fa fa-check"></i>Contributing Authors</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="presentation.html"><a href="presentation.html"><i class="fa fa-check"></i><b>1</b> Presentation</a><ul>
<li class="chapter" data-level="1.1" data-path="presentation.html"><a href="presentation.html#how-to-use-this-book"><i class="fa fa-check"></i><b>1.1</b> How to use this book</a></li>
<li class="chapter" data-level="1.2" data-path="presentation.html"><a href="presentation.html#the-fyrom-soil-database"><i class="fa fa-check"></i><b>1.2</b> The FYROM soil database</a></li>
<li class="chapter" data-level="1.3" data-path="presentation.html"><a href="presentation.html#foreword-to-the-first-edition"><i class="fa fa-check"></i><b>1.3</b> Foreword to the first edition</a></li>
<li class="chapter" data-level="1.4" data-path="presentation.html"><a href="presentation.html#foreword-to-the-second-edition"><i class="fa fa-check"></i><b>1.4</b> Foreword to the second edition</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="soil-property-maps.html"><a href="soil-property-maps.html"><i class="fa fa-check"></i><b>2</b> Soil property maps</a><ul>
<li class="chapter" data-level="2.1" data-path="soil-property-maps.html"><a href="soil-property-maps.html#definitions-and-objectives"><i class="fa fa-check"></i><b>2.1</b> Definitions and objectives</a></li>
<li class="chapter" data-level="2.2" data-path="soil-property-maps.html"><a href="soil-property-maps.html#generic-mapping-of-soil-grids-upscaling-of-plot-level-measurements-and-estimates"><i class="fa fa-check"></i><b>2.2</b> Generic mapping of soil grids: upscaling of plot-Level measurements and estimates</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html"><i class="fa fa-check"></i><b>3</b> Setting-up the software environment</a><ul>
<li class="chapter" data-level="3.1" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#use-of-r-rstudio-and-r-packages"><i class="fa fa-check"></i><b>3.1</b> Use of R, RStudio and R Packages</a><ul>
<li class="chapter" data-level="3.1.1" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#obtaining-and-installing-r"><i class="fa fa-check"></i><b>3.1.1</b> Obtaining and installing R</a></li>
<li class="chapter" data-level="3.1.2" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#obtaining-and-installing-rstudio"><i class="fa fa-check"></i><b>3.1.2</b> Obtaining and installing RStudio</a></li>
<li class="chapter" data-level="3.1.3" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#getting-started-with-r"><i class="fa fa-check"></i><b>3.1.3</b> Getting started with R</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#r-packages"><i class="fa fa-check"></i><b>3.2</b> R packages</a><ul>
<li class="chapter" data-level="3.2.1" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#finding-r-packages"><i class="fa fa-check"></i><b>3.2.1</b> Finding R packages</a></li>
<li class="chapter" data-level="3.2.2" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#most-used-r-packages-for-digital-soil-mapping"><i class="fa fa-check"></i><b>3.2.2</b> Most used R packages for digital soil mapping</a></li>
<li class="chapter" data-level="3.2.3" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#packages-used-in-this-cookbook"><i class="fa fa-check"></i><b>3.2.3</b> Packages used in this cookbook</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#r-and-spatial-data"><i class="fa fa-check"></i><b>3.3</b> R and spatial data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#reading-shapefiles"><i class="fa fa-check"></i><b>3.3.1</b> Reading shapefiles</a></li>
<li class="chapter" data-level="3.3.2" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#coordinate-reference-systems-in-r"><i class="fa fa-check"></i><b>3.3.2</b> Coordinate reference systems in R</a></li>
<li class="chapter" data-level="3.3.3" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#working-with-rasters"><i class="fa fa-check"></i><b>3.3.3</b> Working with rasters</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="setting-up-the-software-environment.html"><a href="setting-up-the-software-environment.html#other-dsm-related-software-and-tools"><i class="fa fa-check"></i><b>3.4</b> Other DSM-related software and tools</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="preparation.html"><a href="preparation.html"><i class="fa fa-check"></i><b>4</b> Preparation of local soil data</a><ul>
<li class="chapter" data-level="4.1" data-path="preparation.html"><a href="preparation.html#soil-profiles-and-soil-augers"><i class="fa fa-check"></i><b>4.1</b> Soil profiles and soil augers</a></li>
<li class="chapter" data-level="4.2" data-path="preparation.html"><a href="preparation.html#soil-database"><i class="fa fa-check"></i><b>4.2</b> Soil database</a><ul>
<li class="chapter" data-level="4.2.1" data-path="preparation.html"><a href="preparation.html#type-of-soil-database"><i class="fa fa-check"></i><b>4.2.1</b> Type of soil database</a></li>
<li class="chapter" data-level="4.2.2" data-path="preparation.html"><a href="preparation.html#technical-steps-loading-soil-data-from-tables-in-r"><i class="fa fa-check"></i><b>4.2.2</b> Technical steps – Loading soil data from tables in R</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="preparation.html"><a href="preparation.html#completeness-of-measurements-and-estimates"><i class="fa fa-check"></i><b>4.3</b> Completeness of measurements and estimates</a><ul>
<li class="chapter" data-level="4.3.1" data-path="preparation.html"><a href="preparation.html#stones"><i class="fa fa-check"></i><b>4.3.1</b> Stones</a></li>
<li class="chapter" data-level="4.3.2" data-path="preparation.html"><a href="preparation.html#bulk-density"><i class="fa fa-check"></i><b>4.3.2</b> Bulk density</a></li>
<li class="chapter" data-level="4.3.3" data-path="preparation.html"><a href="preparation.html#soil-carbon-analysis"><i class="fa fa-check"></i><b>4.3.3</b> Soil carbon analysis</a></li>
<li class="chapter" data-level="4.3.4" data-path="preparation.html"><a href="preparation.html#carbonates"><i class="fa fa-check"></i><b>4.3.4</b> Carbonates</a></li>
<li class="chapter" data-level="4.3.5" data-path="preparation.html"><a href="preparation.html#depth"><i class="fa fa-check"></i><b>4.3.5</b> Depth</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="preparation.html"><a href="preparation.html#soil-depth-estimate"><i class="fa fa-check"></i><b>4.4</b> Soil depth estimate</a><ul>
<li class="chapter" data-level="4.4.1" data-path="preparation.html"><a href="preparation.html#completeness-of-depth-estimate"><i class="fa fa-check"></i><b>4.4.1</b> Completeness of depth estimate</a></li>
<li class="chapter" data-level="4.4.2" data-path="preparation.html"><a href="preparation.html#EqualAreaSplines"><i class="fa fa-check"></i><b>4.4.2</b> Technical steps - Equal-area splines using R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="covariates.html"><a href="covariates.html"><i class="fa fa-check"></i><b>5</b> Preparation of spatial covariates</a><ul>
<li class="chapter" data-level="5.1" data-path="covariates.html"><a href="covariates.html#dem-derived-covariates"><i class="fa fa-check"></i><b>5.1</b> DEM-derived covariates</a></li>
<li class="chapter" data-level="5.2" data-path="covariates.html"><a href="covariates.html#parent-material"><i class="fa fa-check"></i><b>5.2</b> Parent material</a></li>
<li class="chapter" data-level="5.3" data-path="covariates.html"><a href="covariates.html#soil-maps"><i class="fa fa-check"></i><b>5.3</b> Soil maps</a><ul>
<li class="chapter" data-level="5.3.1" data-path="covariates.html"><a href="covariates.html#global-hwsd-soil-property-maps"><i class="fa fa-check"></i><b>5.3.1</b> Global HWSD soil property maps</a></li>
<li class="chapter" data-level="5.3.2" data-path="covariates.html"><a href="covariates.html#technical-steps---rasterizing-a-vector-layer-in-r"><i class="fa fa-check"></i><b>5.3.2</b> Technical steps - Rasterizing a vector layer in R</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="covariates.html"><a href="covariates.html#land-cover-and-land-use"><i class="fa fa-check"></i><b>5.4</b> Land cover and land use</a><ul>
<li class="chapter" data-level="5.4.1" data-path="covariates.html"><a href="covariates.html#globcover-global"><i class="fa fa-check"></i><b>5.4.1</b> GlobCover (Global)</a></li>
<li class="chapter" data-level="5.4.2" data-path="covariates.html"><a href="covariates.html#landsat-geocover-global"><i class="fa fa-check"></i><b>5.4.2</b> Landsat GeoCover (Global)</a></li>
<li class="chapter" data-level="5.4.3" data-path="covariates.html"><a href="covariates.html#globeland30-global"><i class="fa fa-check"></i><b>5.4.3</b> GlobeLand30 (Global)</a></li>
<li class="chapter" data-level="5.4.4" data-path="covariates.html"><a href="covariates.html#corine-land-cover-europe-only"><i class="fa fa-check"></i><b>5.4.4</b> CORINE land cover (Europe only)</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="covariates.html"><a href="covariates.html#climate"><i class="fa fa-check"></i><b>5.5</b> Climate</a><ul>
<li class="chapter" data-level="5.5.1" data-path="covariates.html"><a href="covariates.html#worldclim-v1.4-and-v2-global"><i class="fa fa-check"></i><b>5.5.1</b> WorldClim V1.4 and V2 (Global)</a></li>
<li class="chapter" data-level="5.5.2" data-path="covariates.html"><a href="covariates.html#gridded-agro-meteorological-data-in-europe-europe"><i class="fa fa-check"></i><b>5.5.2</b> Gridded agro-meteorological data in Europe (Europe)</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="covariates.html"><a href="covariates.html#GSOCDataRepo"><i class="fa fa-check"></i><b>5.6</b> GSOCMap - data repository (ISRIC, 2017)</a><ul>
<li class="chapter" data-level="5.6.1" data-path="covariates.html"><a href="covariates.html#covariates-and-empty-mask"><i class="fa fa-check"></i><b>5.6.1</b> Covariates and empty mask</a></li>
<li class="chapter" data-level="5.6.2" data-path="covariates.html"><a href="covariates.html#data-specifications"><i class="fa fa-check"></i><b>5.6.2</b> Data specifications</a></li>
<li class="chapter" data-level="5.6.3" data-path="covariates.html"><a href="covariates.html#data-access"><i class="fa fa-check"></i><b>5.6.3</b> Data access</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="covariates.html"><a href="covariates.html#extending-the-soil-property-table-for-spatial-statistics"><i class="fa fa-check"></i><b>5.7</b> Extending the soil property table for spatial statistics</a></li>
<li class="chapter" data-level="5.8" data-path="covariates.html"><a href="covariates.html#preparation-of-a-soil-property-table-for-spatial-statistics"><i class="fa fa-check"></i><b>5.8</b> Preparation of a soil property table for spatial statistics</a></li>
<li class="chapter" data-level="5.9" data-path="covariates.html"><a href="covariates.html#overlay-soil-covariates"><i class="fa fa-check"></i><b>5.9</b> Technical steps - Overlay covariates and soil points data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mappingMethods.html"><a href="mappingMethods.html"><i class="fa fa-check"></i><b>6</b> Mapping methods</a><ul>
<li class="chapter" data-level="6.1" data-path="mappingMethods.html"><a href="mappingMethods.html#convUpscaling"><i class="fa fa-check"></i><b>6.1</b> Conventional upscaling using soil maps</a><ul>
<li class="chapter" data-level="6.1.1" data-path="mappingMethods.html"><a href="mappingMethods.html#overview"><i class="fa fa-check"></i><b>6.1.1</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="mappingMethods.html"><a href="mappingMethods.html#diversity-of-national-soil-legacy-data-sets"><i class="fa fa-check"></i><b>6.2</b> Diversity of national soil legacy data sets</a><ul>
<li class="chapter" data-level="6.2.1" data-path="mappingMethods.html"><a href="mappingMethods.html#technical-steps---class-matching"><i class="fa fa-check"></i><b>6.2.1</b> Technical steps - Class-matching</a></li>
<li class="chapter" data-level="6.2.2" data-path="mappingMethods.html"><a href="mappingMethods.html#technical-steps---geo-matching"><i class="fa fa-check"></i><b>6.2.2</b> Technical steps - Geo-matching</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="mappingMethods.html"><a href="mappingMethods.html#RK"><i class="fa fa-check"></i><b>6.3</b> Regression-Kriging</a><ul>
<li class="chapter" data-level="6.3.1" data-path="mappingMethods.html"><a href="mappingMethods.html#overview-1"><i class="fa fa-check"></i><b>6.3.1</b> Overview</a></li>
<li class="chapter" data-level="6.3.2" data-path="mappingMethods.html"><a href="mappingMethods.html#assumptions"><i class="fa fa-check"></i><b>6.3.2</b> Assumptions</a></li>
<li class="chapter" data-level="6.3.3" data-path="mappingMethods.html"><a href="mappingMethods.html#pre-processing-of-covariates"><i class="fa fa-check"></i><b>6.3.3</b> Pre-processing of covariates</a></li>
<li class="chapter" data-level="6.3.4" data-path="mappingMethods.html"><a href="mappingMethods.html#the-terminology"><i class="fa fa-check"></i><b>6.3.4</b> The terminology</a></li>
<li class="chapter" data-level="6.3.5" data-path="mappingMethods.html"><a href="mappingMethods.html#interpret-the-key-results-of-multiple-regression"><i class="fa fa-check"></i><b>6.3.5</b> Interpret the Key Results of Multiple Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="mappingMethods.html"><a href="mappingMethods.html#using-the-results-of-a-regression-analysis-to-make-predictions"><i class="fa fa-check"></i><b>6.3.6</b> Using the results of a regression analysis to make predictions</a></li>
<li class="chapter" data-level="6.3.7" data-path="mappingMethods.html"><a href="mappingMethods.html#technical-steps---regression-kriging"><i class="fa fa-check"></i><b>6.3.7</b> Technical steps - Regression-kriging</a></li>
<li class="chapter" data-level="6.3.8" data-path="mappingMethods.html"><a href="mappingMethods.html#technical-steps---cross-validation-of-regression-kriging-models"><i class="fa fa-check"></i><b>6.3.8</b> Technical steps - Cross-validation of regression-kriging models</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="mappingMethods.html"><a href="mappingMethods.html#rf"><i class="fa fa-check"></i><b>6.4</b> Data mining: random forest</a><ul>
<li class="chapter" data-level="6.4.1" data-path="mappingMethods.html"><a href="mappingMethods.html#overview-2"><i class="fa fa-check"></i><b>6.4.1</b> Overview</a></li>
<li class="chapter" data-level="6.4.2" data-path="mappingMethods.html"><a href="mappingMethods.html#random-forest"><i class="fa fa-check"></i><b>6.4.2</b> Random forest</a></li>
<li class="chapter" data-level="6.4.3" data-path="mappingMethods.html"><a href="mappingMethods.html#conceptual-model-and-data-preparation"><i class="fa fa-check"></i><b>6.4.3</b> Conceptual model and data preparation</a></li>
<li class="chapter" data-level="6.4.4" data-path="mappingMethods.html"><a href="mappingMethods.html#software"><i class="fa fa-check"></i><b>6.4.4</b> Software</a></li>
<li class="chapter" data-level="6.4.5" data-path="mappingMethods.html"><a href="mappingMethods.html#tunning-random-forest-model-parameters"><i class="fa fa-check"></i><b>6.4.5</b> Tunning random forest model parameters</a></li>
<li class="chapter" data-level="6.4.6" data-path="mappingMethods.html"><a href="mappingMethods.html#technical-steps---random-forest"><i class="fa fa-check"></i><b>6.4.6</b> Technical steps - Random forest</a></li>
<li class="chapter" data-level="6.4.7" data-path="mappingMethods.html"><a href="mappingMethods.html#technical-steps---using-quantile-regression-forest-to-estimate-uncertainty"><i class="fa fa-check"></i><b>6.4.7</b> Technical steps - Using quantile regression forest to estimate uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="mappingMethods.html"><a href="mappingMethods.html#svm"><i class="fa fa-check"></i><b>6.5</b> Data mining: support vector machines</a><ul>
<li class="chapter" data-level="6.5.1" data-path="mappingMethods.html"><a href="mappingMethods.html#overview-3"><i class="fa fa-check"></i><b>6.5.1</b> Overview</a></li>
<li class="chapter" data-level="6.5.2" data-path="mappingMethods.html"><a href="mappingMethods.html#technical-steps---fitting-an-svm-model-to-predict-the-soc"><i class="fa fa-check"></i><b>6.5.2</b> Technical steps - fitting an SVM model to predict the SOC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chvalidation.html"><a href="chvalidation.html"><i class="fa fa-check"></i><b>7</b> Validation</a><ul>
<li class="chapter" data-level="7.1" data-path="chvalidation.html"><a href="chvalidation.html#what-is-validation"><i class="fa fa-check"></i><b>7.1</b> What is validation?</a></li>
<li class="chapter" data-level="7.2" data-path="chvalidation.html"><a href="chvalidation.html#QualMeasures"><i class="fa fa-check"></i><b>7.2</b> Map quality measures</a><ul>
<li class="chapter" data-level="7.2.1" data-path="chvalidation.html"><a href="chvalidation.html#quality-measures-for-quantitative-soil-maps"><i class="fa fa-check"></i><b>7.2.1</b> Quality measures for quantitative soil maps</a></li>
<li class="chapter" data-level="7.2.2" data-path="chvalidation.html"><a href="chvalidation.html#quality-measures-for-qualitative-soil-maps"><i class="fa fa-check"></i><b>7.2.2</b> Quality measures for qualitative soil maps</a></li>
<li class="chapter" data-level="7.2.3" data-path="chvalidation.html"><a href="chvalidation.html#estimating-the-map-quality-measures-and-associated-uncertainty"><i class="fa fa-check"></i><b>7.2.3</b> Estimating the map quality measures and associated uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chvalidation.html"><a href="chvalidation.html#graphical-map-quality-measures"><i class="fa fa-check"></i><b>7.3</b> Graphical map quality measures</a></li>
<li class="chapter" data-level="7.4" data-path="chvalidation.html"><a href="chvalidation.html#validationMeth"><i class="fa fa-check"></i><b>7.4</b> Validation methods and statistical inference</a><ul>
<li class="chapter" data-level="7.4.1" data-path="chvalidation.html"><a href="chvalidation.html#addProbSampling"><i class="fa fa-check"></i><b>7.4.1</b> Additional probability sampling</a></li>
<li class="chapter" data-level="7.4.2" data-path="chvalidation.html"><a href="chvalidation.html#data-splitting"><i class="fa fa-check"></i><b>7.4.2</b> Data-splitting</a></li>
<li class="chapter" data-level="7.4.3" data-path="chvalidation.html"><a href="chvalidation.html#xval"><i class="fa fa-check"></i><b>7.4.3</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="chvalidation.html"><a href="chvalidation.html#TS:validation"><i class="fa fa-check"></i><b>7.5</b> Technical steps - Validation</a></li>
<li class="chapter" data-level="7.6" data-path="chvalidation.html"><a href="chvalidation.html#dataSplit"><i class="fa fa-check"></i><b>7.6</b> Technical steps - Data-splitting</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="evaluation.html"><a href="evaluation.html"><i class="fa fa-check"></i><b>8</b> Model evaluation in digital soil mapping</a><ul>
<li class="chapter" data-level="8.1" data-path="evaluation.html"><a href="evaluation.html#technical-steps---model-correlations-and-spatial-differences"><i class="fa fa-check"></i><b>8.1</b> Technical steps - Model correlations and spatial differences</a></li>
<li class="chapter" data-level="8.2" data-path="evaluation.html"><a href="evaluation.html#technical-steps---model-evaluation"><i class="fa fa-check"></i><b>8.2</b> Technical steps - Model evaluation</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="uncertainty.html"><a href="uncertainty.html"><i class="fa fa-check"></i><b>9</b> Uncertainty</a><ul>
<li class="chapter" data-level="9.1" data-path="uncertainty.html"><a href="uncertainty.html#sourcesuncert"><i class="fa fa-check"></i><b>9.1</b> Sources of uncertainty</a><ul>
<li class="chapter" data-level="9.1.1" data-path="uncertainty.html"><a href="uncertainty.html#attribute-uncertainty-of-soil-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Attribute uncertainty of soil measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="uncertainty.html"><a href="uncertainty.html#positional-uncertainty-of-soil-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Positional uncertainty of soil measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="uncertainty.html"><a href="uncertainty.html#uncertainty-in-covariates"><i class="fa fa-check"></i><b>9.1.3</b> Uncertainty in covariates</a></li>
<li class="chapter" data-level="9.1.4" data-path="uncertainty.html"><a href="uncertainty.html#uncertainty-in-models-predicting-soil-properties-from-covariates-and-soil-point-data"><i class="fa fa-check"></i><b>9.1.4</b> Uncertainty in models predicting soil properties from covariates and soil point data</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="uncertainty.html"><a href="uncertainty.html#uncertainty-and-spatial-data-quality"><i class="fa fa-check"></i><b>9.2</b> Uncertainty and spatial data quality</a></li>
<li class="chapter" data-level="9.3" data-path="uncertainty.html"><a href="uncertainty.html#quantifying-prediction-uncertainty"><i class="fa fa-check"></i><b>9.3</b> Quantifying prediction uncertainty</a><ul>
<li class="chapter" data-level="9.3.1" data-path="uncertainty.html"><a href="uncertainty.html#uncertainty-characterised-by-probability-distributions"><i class="fa fa-check"></i><b>9.3.1</b> Uncertainty characterised by probability distributions</a></li>
<li class="chapter" data-level="9.3.2" data-path="uncertainty.html"><a href="uncertainty.html#propagation-of-model-uncertainty"><i class="fa fa-check"></i><b>9.3.2</b> Propagation of model uncertainty</a></li>
<li class="chapter" data-level="9.3.3" data-path="uncertainty.html"><a href="uncertainty.html#propagation-of-attribute-positional-and-covariate-uncertaintypropuncertainty"><i class="fa fa-check"></i><b>9.3.3</b> Propagation of attribute, positional and covariate uncertainty{propUncertainty}</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="data-sharing.html"><a href="data-sharing.html"><i class="fa fa-check"></i><b>10</b> Data sharing</a><ul>
<li class="chapter" data-level="10.1" data-path="data-sharing.html"><a href="data-sharing.html#export-formats"><i class="fa fa-check"></i><b>10.1</b> Export formats</a><ul>
<li class="chapter" data-level="10.1.1" data-path="data-sharing.html"><a href="data-sharing.html#type-of-soil-data-and-their-formatting"><i class="fa fa-check"></i><b>10.1.1</b> Type of soil data and their formatting</a></li>
<li class="chapter" data-level="10.1.2" data-path="data-sharing.html"><a href="data-sharing.html#general-gis-data-formats-vector-raster-table"><i class="fa fa-check"></i><b>10.1.2</b> General GIS data formats: vector, raster, table</a></li>
<li class="chapter" data-level="10.1.3" data-path="data-sharing.html"><a href="data-sharing.html#recommended-gis-data-exchange-formats"><i class="fa fa-check"></i><b>10.1.3</b> Recommended GIS data exchange formats</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="data-sharing.html"><a href="data-sharing.html#web-services-serving-soil-data-using-web-technologywebservice"><i class="fa fa-check"></i><b>10.2</b> Web services: serving soil data using web technology{webService}</a><ul>
<li class="chapter" data-level="10.2.1" data-path="data-sharing.html"><a href="data-sharing.html#third-party-services"><i class="fa fa-check"></i><b>10.2.1</b> Third-party services</a></li>
<li class="chapter" data-level="10.2.2" data-path="data-sharing.html"><a href="data-sharing.html#geoserver-web-serving-and-web-processing"><i class="fa fa-check"></i><b>10.2.2</b> GeoServer (web serving and web processing)</a></li>
<li class="chapter" data-level="10.2.3" data-path="data-sharing.html"><a href="data-sharing.html#visualizing-data-using-leaflet-andor-google-earth"><i class="fa fa-check"></i><b>10.2.3</b> Visualizing data using Leaflet and/or Google Earth</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="data-sharing.html"><a href="data-sharing.html#preparing-soil-data-for-distribution"><i class="fa fa-check"></i><b>10.3</b> Preparing soil data for distribution</a><ul>
<li class="chapter" data-level="10.3.1" data-path="data-sharing.html"><a href="data-sharing.html#metadata"><i class="fa fa-check"></i><b>10.3.1</b> Metadata</a></li>
<li class="chapter" data-level="10.3.2" data-path="data-sharing.html"><a href="data-sharing.html#exporting-data-final-tips"><i class="fa fa-check"></i><b>10.3.2</b> Exporting data: final tips</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="data-sharing.html"><a href="data-sharing.html#export-formats-1"><i class="fa fa-check"></i><b>10.4</b> Export formats</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="technical-overview-and-the-checklist.html"><a href="technical-overview-and-the-checklist.html"><i class="fa fa-check"></i><b>11</b> Technical overview and the checklist</a><ul>
<li class="chapter" data-level="11.1" data-path="technical-overview-and-the-checklist.html"><a href="technical-overview-and-the-checklist.html#point-dataset"><i class="fa fa-check"></i><b>11.1</b> Point dataset</a></li>
<li class="chapter" data-level="11.2" data-path="technical-overview-and-the-checklist.html"><a href="technical-overview-and-the-checklist.html#covariates-1"><i class="fa fa-check"></i><b>11.2</b> Covariates</a></li>
<li class="chapter" data-level="11.3" data-path="technical-overview-and-the-checklist.html"><a href="technical-overview-and-the-checklist.html#statistical-inference"><i class="fa fa-check"></i><b>11.3</b> Statistical inference</a></li>
<li class="chapter" data-level="11.4" data-path="technical-overview-and-the-checklist.html"><a href="technical-overview-and-the-checklist.html#spatial-interpolation"><i class="fa fa-check"></i><b>11.4</b> Spatial interpolation</a></li>
<li class="chapter" data-level="11.5" data-path="technical-overview-and-the-checklist.html"><a href="technical-overview-and-the-checklist.html#calculation-of-stocks"><i class="fa fa-check"></i><b>11.5</b> Calculation of stocks</a></li>
<li class="chapter" data-level="11.6" data-path="technical-overview-and-the-checklist.html"><a href="technical-overview-and-the-checklist.html#evaluation-of-output-and-quality-assessment"><i class="fa fa-check"></i><b>11.6</b> Evaluation of output and quality assessment</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deliverables.html"><a href="deliverables.html"><i class="fa fa-check"></i><b>12</b> Deliverables</a></li>
<li class="chapter" data-level="13" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html"><i class="fa fa-check"></i><b>13</b> Compendium of the code examples included in the cookbook</a><ul>
<li class="chapter" data-level="13.1" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:PreparationProfiles"><i class="fa fa-check"></i><b>13.1</b> Data preparation for soil profiles</a></li>
<li class="chapter" data-level="13.2" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:PreparationAuger"><i class="fa fa-check"></i><b>13.2</b> Data preparation for top soil or auger samples</a></li>
<li class="chapter" data-level="13.3" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:merging"><i class="fa fa-check"></i><b>13.3</b> Merging top soil and soil profiles Databases</a></li>
<li class="chapter" data-level="13.4" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:data-splitting"><i class="fa fa-check"></i><b>13.4</b> Data-splitting</a></li>
<li class="chapter" data-level="13.5" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:Rasterizing"><i class="fa fa-check"></i><b>13.5</b> Rasterizing a vector layer in R</a></li>
<li class="chapter" data-level="13.6" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:Overlay"><i class="fa fa-check"></i><b>13.6</b> Overlay covariates and soil points Data</a></li>
<li class="chapter" data-level="13.7" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:RK"><i class="fa fa-check"></i><b>13.7</b> Fitting a regression-kriging model to predict the OCS</a></li>
<li class="chapter" data-level="13.8" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:xvalRK"><i class="fa fa-check"></i><b>13.8</b> Cross-validation of regression-kriging models</a></li>
<li class="chapter" data-level="13.9" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:rf"><i class="fa fa-check"></i><b>13.9</b> Fitting a random forest model to predict the SOC</a></li>
<li class="chapter" data-level="13.10" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:quantreg"><i class="fa fa-check"></i><b>13.10</b> Using quantile regression forest to estimate uncertainty</a></li>
<li class="chapter" data-level="13.11" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:svm"><i class="fa fa-check"></i><b>13.11</b> Fitting a SVM model to predict the SOC</a></li>
<li class="chapter" data-level="13.12" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:Validation"><i class="fa fa-check"></i><b>13.12</b> Validation</a></li>
<li class="chapter" data-level="13.13" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:Graphs"><i class="fa fa-check"></i><b>13.13</b> Graphical map quality measures</a></li>
<li class="chapter" data-level="13.14" data-path="compendium-of-the-code-examples-included-in-the-cookbook.html"><a href="compendium-of-the-code-examples-included-in-the-cookbook.html#cd:Evaluation"><i class="fa fa-check"></i><b>13.14</b> Model evaluation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Soil Organic Carbon Mapping Cookbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mappingMethods" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Mapping methods</h1>
<p><em>R. Baritz, M. Guevara, V.L. Mulder, G.F. Olmedo, C. Thine, R.R. Vargas, Y. Yigini</em></p>
<p>In this Chapter, we want to introduce five different approaches for obtaining the SOC map for FYROM. The first two methods presented in Section <a href="mappingMethods.html#convUpscaling">6.1</a> are classified as conventional upscaling and the presented methods in the Sections <a href="mappingMethods.html#RK">6.3</a>, <a href="mappingMethods.html#rf">6.4</a>, and <a href="mappingMethods.html#svm">6.5</a> are approaches from DSM.</p>
<p>The first method is class-matching. In this approach, we derive average SOC stocks per class either from the soil type for which a national map exists, or through the combination with other spatial covariates such as land use category, climate type, biome, etc. This approach is used in the absence of spatial coordinates of the source data. The second conventional upscaling method is geo-matching, were upscaling is based on averaged SOC values per mapping unit.</p>
<p>Furthermore, we present three methods from DSM. Regression-kriging (RK) is a hybrid model with both, a deterministic and a stochastic component <span class="citation">(Hengl, Heuvelink, and Rossiter <a href="#ref-hengl2007regression">2007</a>)</span>. Next method is called random forest (RF). This one is an ensemble of regression trees based on bagging. This machine learning algorithm uses a different combination of prediction factors to train multiple regression trees <span class="citation">(Breiman <a href="#ref-Breiman1996">1996</a>)</span>. The last method is called support vector machines (SVM). This method applies a simple linear method to the data but in a high-dimensional feature space, non-linearly related to the input space <span class="citation">(Karatzoglou, Meyer, and Hornik <a href="#ref-Karatzoglou2006">2006</a>)</span>. We present this diversity of methods because there is no <em>best</em> mapping method for DSM, and testing and selection has to be done for every data scenario <span class="citation">(Guevara et al. <a href="#ref-guevara_2018">2018</a>)</span>.</p>
<p>The authors of this Chapter used <strong>R</strong> packages. To run the code provided in this Chapter, the following packages need to be installed in the <strong>R</strong> user library. If the packages are not yet installed, the <code>install.packages()</code> function can be used.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1"><span class="co"># Installing packages for Chapter &#39;Mapping Methods&#39;</span></a>
<a class="sourceLine" id="cb49-2" data-line-number="2"><span class="kw">install.packages</span>(<span class="kw">c</span>(<span class="st">&quot;sp&quot;</span>, <span class="st">&quot;car&quot;</span>, <span class="st">&quot;automap&quot;</span>,</a>
<a class="sourceLine" id="cb49-3" data-line-number="3">                   <span class="st">&quot;randomForest&quot;</span>, <span class="st">&quot;caret&quot;</span>, <span class="st">&quot;Metrics&quot;</span>,</a>
<a class="sourceLine" id="cb49-4" data-line-number="4">                   <span class="st">&quot;quantregForest&quot;</span>, <span class="st">&quot;snow&quot;</span>, <span class="st">&quot;reshape&quot;</span>,</a>
<a class="sourceLine" id="cb49-5" data-line-number="5">                   <span class="st">&quot;e1071&quot;</span>))</a></code></pre></div>
<div id="convUpscaling" class="section level2">
<h2><span class="header-section-number">6.1</span> Conventional upscaling using soil maps</h2>
<p><em>R. Baritz, V.L. Mulder</em></p>
<div id="overview" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Overview</h3>
<p>The two conventional upscaling methods in the context of SOC mapping, are described by <span class="citation">Lettens et al. (<a href="#ref-lettens2004soil">2004</a>)</span>. Details about weighted averaging can be found in <span class="citation">Hiederer (<a href="#ref-hiederer2013mapping">2013</a>)</span>. Different conventional upscaling approaches were applied in many countries e.g. by <span class="citation">Krasilnikov et al. (<a href="#ref-krasilnikov2013soils">2013</a>)</span> (Mexico), <span class="citation">Greve et al. (<a href="#ref-greve2007generating">2007</a>)</span> (Denmark), <span class="citation">Kolli et al. (<a href="#ref-kolli2009stocks">2009</a>)</span> (Estonia), <span class="citation">Arrouays, Deslais, and Badeau (<a href="#ref-arrouays2001carbon">2001</a>)</span> (France), and <span class="citation">Bhatti, Apps, and Tarnocai (<a href="#ref-bhatti2002estimates">2002</a>)</span> (Canada). Because the structure of soil map databases differs between countries (e.g. definition of the soil mapping unit, stratification, soil associations, dominating and co-dominating soils, typical and estimate soil properties for different depths), it is difficult to define a generic methodology for the use of these maps for mapping soil property information.</p>
<p>However, the essential principle which is commonly used, is to combine soil property data from local observations with soil maps via class-matching and geo-matching.</p>
</div>
</div>
<div id="diversity-of-national-soil-legacy-data-sets" class="section level2">
<h2><span class="header-section-number">6.2</span> Diversity of national soil legacy data sets</h2>
<p>In order to develop a representative and large national soil database, very often, data from different sources (e.g. soil surveys or projects in different parts of the country at different times) are combined. The following case of Belgium demonstrates, how available legacy databases could be combined. Three different sources are used to compile an overview of national SOC stocks:</p>
<ul>
<li><p><strong>Data source 1</strong>: Soil profile database with 13,000 points of genetic horizons; for each site, there is information about the soil series, map coordinates, and land use class; for each horizon, there is information about depth and thickness, textural fractions and class, volume percentage of rock fragments; analytically, there is the organic carbon content and inorganic carbon content.</p></li>
<li><p><strong>Data source 2</strong>: Forest soil data base which includes ectorganic horizons. According to their national definition, the term <em>ectorganic</em> designates the surface horizons with an organic matter content of at least 30%, thus, it includes both the litter layer and the organic soil layers. For the calculation of SOC stocks for the ectorganic layer, no fixed-depth was used, instead, the measured thickness of the organic layers and litter layers was applied.</p></li>
<li><p><strong>Data source 3</strong>: 15,000 soil surface samples were used (upper 20 cm of mineral soil); carbon measurements are available per depth class.</p></li>
</ul>
<p>From all data sources, SOC stocks for peat soils were calculated separately.</p>
<div id="technical-steps---class-matching" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Technical steps - Class-matching</h3>
<p><strong>Step 1 - Data preparation</strong></p>
<ol style="list-style-type: decimal">
<li>Separate the database for forests, peat, and other land uses. If only horizons are provided, derive or estimate average depth of horizons per soil type, and add upper and lower depth.</li>
<li>Check the completeness of parameters per depth using the solum depth to code empty cells.</li>
<li>Correct the organic carbon in case total carbon was determined (total carbon minus inorganic carbon concentration).</li>
<li>Correct the Walkley and Black method for incomplete oxidation (using the correction factor of 1.32 for the quantification of the organic carbon content in the soil samples) <span class="citation">(Walkley and Black <a href="#ref-walkley1934examination">1934</a>)</span>.</li>
<li>If BD measured is lacking, select proper PTFs and estimate BD. Publications about the choice of the best suited PTF for specific physio-geographic conditions are available.</li>
<li>If the stone content is missing, investigate using other data sources or literature, to which a correction for stones should be applied.</li>
<li>If possible, derive the standard average stone content for different soils/horizons/depths, or used published soil profiles, as a simple correction factor.</li>
<li>Calculate SOC stocks for all mineral and peat soils over 0 cm - 30 cm, and optionally for forest organic layers, and peat between &gt;30 cm and &lt;100 cm.</li>
</ol>
<p><strong>Step 2 - Preparatory GIS operations</strong></p>
<ol style="list-style-type: decimal">
<li>Prepare the covariates.</li>
<li>Identify the properties of covariates for each point observation using geo-matching.</li>
<li>For mapping using <strong>geo-matching of all points</strong>: Extract the covariate information to all georeferenced sample sites. The SOC values from all points within the unit are then averaged. It is assumed that the points represent the real variability of soil types within the units.</li>
</ol>
<p><strong>Step 3 - Mapping using class-matching of points in agreement with classes</strong></p>
<p>Through class-matching, only those points or profiles are attributed to a soil or landscape unit if both the soil and the land use class are the same. Class-matching thus can be performed regardless of the profile location. Before averaging, a weighing factor can be introduced according to the area proportions of dominant, co-dominant and associated soils. Each profile needs to be matched to its soil type/landscape type, and the SOC value averaged.</p>
<ol style="list-style-type: decimal">
<li>Determine a soil or landscape unit (e.g. national soil legend stratified by climate area and mainland cover type such as forest, grassland, cropland).</li>
<li>Calculate average SOC stocks from all soils which match the soil/landscape unit.</li>
<li>Present the soil/landscape map with SOC stocks, do not classify SOC stocks (tons/hectare) into groups (e.g. &lt; 50, 50 - 100, &gt; 100).</li>
</ol>
<blockquote>
<p>Note: Pre-classified SOC maps cannot be integrated into a global GSOCmap legend.</p>
</blockquote>
</div>
<div id="technical-steps---geo-matching" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Technical steps - Geo-matching</h3>
<p>Because of its importance, geo-matching as a mapping method is described in more detail. It is neccessary to first prepare the working environment with the pre-processed input data. The following Section presents different geo-matching procedures.</p>
<ol style="list-style-type: decimal">
<li>Setting up software and working environment.</li>
<li>Geo-matching SOC with WRB soil map (step-by-step, using the soil Map of FYROM and the demonstration data presented above).</li>
<li>Geo-matching SOC with other environmental variables: land use.</li>
<li>Finally, the development of landscape Units <span class="citation">(Lettens et al. <a href="#ref-lettens2004soil">2004</a>)</span> is outlined.</li>
</ol>
<p>This example was developed for QGIS and focusses on SOC mapping using vector data. QGIS 2.18 with GRASS 7.05 will be used. For more information, see also the following links:</p>
<ul>
<li><a href="https://gis.stackexchange.com" class="uri">https://gis.stackexchange.com</a></li>
<li><a href="http://www.qgis.org/" class="uri">http://www.qgis.org/</a></li>
<li><a href="http://www.qgisforum.org/" class="uri">http://www.qgisforum.org/</a></li>
</ul>
<p><strong>Step 1 - Setting up a QGIS project</strong></p>
<ol style="list-style-type: decimal">
<li>Install QGIS and supporting software; download the software at <a href="http://www.qgis.org/en/site/forusers/download.html" class="uri">http://www.qgis.org/en/site/forusers/download.html</a> (select correct version for Windows, MacOS or Linux, 32 or 64 bit).</li>
<li>Create a work folder, e.g. D:\GSOC\practical_matching. Copy the folder with the FYROM demonstration data into this folder.</li>
<li>Start QGIS Desktop with GRASS. Figure <a href="mappingMethods.html#fig:qgis">6.1</a> shows the start screen of QGIS Desktop. In the upper left panel, there is the <strong>Browser Panel</strong>, which lists the geodata used for this example. In the bottom left, the layer information is given in the <strong>Layers Panel</strong> for the layers displayed on the right.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:qgis"></span>
<img src="images/Conv_upscaling1.png" alt="QGIS Desktop with the Browser Panel, the Layers Panel and the display of layers" width="80%" />
<p class="caption">
Figure 6.1: QGIS Desktop with the Browser Panel, the Layers Panel and the display of layers
</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li>Load the FYROM soil map. Right-click the file in the <strong>Browser panel</strong> and add the map to your project.</li>
<li>Display the soil classes. Right-click on the file in the <strong>Layers Panel</strong> and select <em>Properties</em>. Go to the <em>Style</em> and change from <em>Single Symbol</em> to <em>Categorized</em> as shown in Figure <a href="mappingMethods.html#fig:layerprop">6.2</a>. Select the column WRB and press the icon <em>Classify</em> and change the colors if you want. Next, apply the change and finish by clicking the <strong>OK-Button</strong>.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:layerprop"></span>
<img src="images/Conv_upscaling2.png" alt="Changing layer properties for the FYROM soil map" width="80%" />
<p class="caption">
Figure 6.2: Changing layer properties for the FYROM soil map
</p>
</div>
<ol start="6" style="list-style-type: decimal">
<li>Ensure the correct projection for this project. Go to select <em>Project</em>, <em>Project Properties</em>, <em>CRS</em>.
In this case, you automatically use the local projection for FYROM. The EPSG code is 3909 which corresponds to MGI 1901/Balkans Zone 7 as shown in Figure <a href="mappingMethods.html#fig:qgisepsg">6.3</a>).</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:qgisepsg"></span>
<img src="images/Conv_upscaling3.png" alt="Project properties and projection settings" width="80%" />
<p class="caption">
Figure 6.3: Project properties and projection settings
</p>
</div>
<ol start="7" style="list-style-type: decimal">
<li>Save the project in the created folder. Load and display the pre-processed SOC point data. If a shapefile already exists, this is done the same way as described in Step 4. If you have the data as a text file, you need to create a vector layer out of that file. Go to <em>Layer</em>, <em>Add Layer</em>, <em>Add Delimited Text Layer</em>. Select the correct file and proper CRS projection. The layer should be added to your <strong>Layers Panel</strong> and displayed on top of the soil map.</li>
</ol>
<p><strong>Step 2 - Geo-Matching SOC with WRB soil map</strong></p>
<p>In <strong>Step 2</strong> you will make a SOC map, based on the FYROM soil map and the SOC values at the sampled points, following three steps. First you extract the soil map information for the point data, then you obtain the mean and standard deviation of the SOC stocks per soil class, based on the point data, and last you assign these values to the corresponding soil map units. The steps are described in detail below.</p>
<ol style="list-style-type: decimal">
<li>Extract the soil map information to the soil profile data by <em>Join Attributes by Location</em>. Select <em>Vector</em>, <em>Data Management Tools</em>, and <em>Join Attributes by Location</em>. Here, the target vector layers are the soil point data, and the join vector layer is the FYROM soil map. The geometric predicate is <em>intersects</em>. Figure <a href="mappingMethods.html#fig:joinloc">6.4</a> shows how to specify within the joined table to keep only matching records. Save the joined layer as a new file.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:joinloc"></span>
<img src="images/Conv_upscaling4.png" alt="Join attributes by location" width="80%" />
<p class="caption">
Figure 6.4: Join attributes by location
</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>Check the newly generated file, open the attribute table. The new file is added to the <strong>Layers Panel</strong>. Right-click on the file and open the attribute table. The information from the FYROM soil map is now added to the soil point data.</li>
<li>Most likely, the SOC values in the table are not numeric and thus statistics cannot be calculated. Check the data format, right-click on the file in the <strong>Layers Panel</strong> and check the type name of the SOC field under the tab <em>Fields</em>. If they are not integer then change the format.</li>
<li>Change of the data format: Open the attribute table and start editing (the pencil symbol in the upper left corner of your table). Open the field calculator and follow these instructions as shown in Figure <a href="mappingMethods.html#fig:fieldcalc">6.5</a>):</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Checkbox: Create a new field</li>
<li>Output field name: Specify the name of your field</li>
<li>Output field type: Decimal Number (real)</li>
<li>Output field length: 10, precision: 3</li>
<li>Expression: to_real(“SOC”), the to_real function can be found under <em>Conversions</em> and the SOC field is found under <em>Fields and Values</em></li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:fieldcalc"></span>
<img src="images/Conv_upscaling5.png" alt="Example field calculator" width="80%" />
<p class="caption">
Figure 6.5: Example field calculator
</p>
</div>
<ol start="5" style="list-style-type: decimal">
<li>After calculating the field, save the edits and leave the editing mode prior to closing the table. If changes are not saved, the added field will be lost.</li>
<li>Calculate the median SOC stock per soil type. Go to the tab <em>Vector</em> and select <em>Group Stats</em>. Select the layer from the spatial join you made in Step 2. Add the field SOC and median to the box with <em>Values</em> and the field WRB to the <em>Rows</em>. Make sure the box with <em>Use only selected features</em> is <strong>not</strong> checked. Now calculate the statistics. A table will be given in the left pane (see Figure <a href="mappingMethods.html#fig:groupstats">6.6</a>). Save this file as CSV and repeat the same for the standard deviation.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:groupstats"></span>
<img src="images/Conv_upscaling6.png" alt="Calculate group statistics" width="80%" />
<p class="caption">
Figure 6.6: Calculate group statistics
</p>
</div>
<ol start="7" style="list-style-type: decimal">
<li>Join the mean and standard deviation of SOC to the soil map. First, add the files generated during Step 6 to the <strong>Layers Panel</strong>. In the <strong>Layers Panel</strong>, right-click on the FYROM soil map. Go to <em>Properties</em>, select <em>Joins</em> and add a new join for both the median and standard deviation of SOC. The Join and Target Field are both WRB.</li>
<li>Display the SOC maps. Go to the layer properties of the FYROM soil map. Go to <em>Style</em> and change the legend to a graduated legend. In the column, you indicate the assigned SOC values. Probably this is not a integer number and so you have to convert this number again to a numeric values. You can do this with the box next to the box as depicted in Figure <a href="mappingMethods.html#fig:legendstyle">6.7</a>. Change the number of classes to e.g. 10 classes, change the mode of the legend and change the color scheme if you want and apply the settings. Now you have a map with the median SOC stocks per WRB soil class.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:legendstyle"></span>
<img src="images/Conv_upscaling7.png" alt="Change the legend style to display the SOC values" width="80%" />
<p class="caption">
Figure 6.7: Change the legend style to display the SOC values
</p>
</div>
<ol start="9" style="list-style-type: decimal">
<li>In order to generate a proper layout, go to <em>Project</em> and select <em>New Print Composer</em> (see Figure <a href="mappingMethods.html#fig:mapcomp">6.8</a>):</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Add map using <em>Layout</em> and <em>Add Map</em>. Define a square on the canvas and the selected map will be displayed.</li>
<li>Similarly, title, scale bar, legend and a north arrow can be added. Specific properties can be changed in the box <em>Item properties</em>.</li>
<li>When the map is finished, it can be exported as an image or PDF file.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:mapcomp"></span>
<img src="images/Conv_upscaling8.png" alt="Example of the Map composer" width="80%" />
<p class="caption">
Figure 6.8: Example of the Map composer
</p>
</div>
<ol start="10" style="list-style-type: decimal">
<li>Repeat the Steps 2 to 8 but now for the standard deviation of the SOC stocks.</li>
<li>Save the file as a new shapefile. Go to <strong>Layers Panel</strong>, <em>Save As</em> and select ESRI SHP format and make sure that you define the symbology export <em>Feature Symbology</em>. Now, a shapefile is generated, with both the median and standard deviation SOC stock per soil type. Redundant fields can be removed after the new file is created.</li>
</ol>
<p><strong>Step 3 - Geo-Matching SOC with other environmental variables: Land use</strong></p>
<ol style="list-style-type: decimal">
<li>Start a new project and add the soil point data and FYROM soil map layers from the <strong>Browser Panel</strong>.</li>
<li>Add the land use raster file to the <strong>Layers Panel</strong>. This is a raster file with 1 km resolution and projected in lat-long degrees (WGS84). For more information about this product see the online information from worldgrids: <a href="http://worldgrids.org/doku.php/wiki:glcesa3" class="uri">http://worldgrids.org/doku.php/wiki:glcesa3</a>.</li>
<li>Change the projection to the MGI 1901/Balkans Zone 7. Go to <em>Raster</em>, <em>Projections</em>, <em>Warp</em> and select the proper projection and a suitable file name, e.g. LU_projected_1km. Tick the checkbox for the resampling method and choose <em>Near</em>. This is the nearest neighbor and most suitable for a transformation of categorical data, such as land use (see Figure <a href="mappingMethods.html#fig:changeproj">6.9</a>).</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:changeproj"></span>
<img src="images/Conv_upscaling9.png" alt="Change the projection of a raster file" width="80%" />
<p class="caption">
Figure 6.9: Change the projection of a raster file
</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li>In order to geo-match the soil point data with land use data, the raster file needs to be converted into a vector file. Go to <em>Raster</em>, <em>Conversions</em>, and select <em>Polygonize</em>. Set a proper output filename, e.g. LU_polygon_1km, and check the tickbox for <em>Fieldname</em>.</li>
<li>Change the legend style into categories (Step 1-5). Now, the steps from the previous Section need to be repeated, using the land use polygon map instead of using the FYROM soil map.</li>
<li>Join attributes by location using the soil point data and the polygon land use map.</li>
<li>Calculate the median and standard deviation of SOC by using the Group Statistics for SOC and the land use classes and save the files as CSV file.</li>
<li>Add the generated CSV files to the <strong>Layers Panel</strong>.</li>
<li>Join the files with the land use polygon map, generated in Steps 3 and 4.</li>
<li>Change the classes in the legend and inspect the histogram with the median SOC values. Try to find a proper definition of the class boundaries (Steps 2 to 8).</li>
</ol>
<p><strong>Step 4 - Joining landscape units and soil mapping units to support class- and geo-matching (optional)</strong></p>
<p>In this Section, it is outlined how SOC stocks can be mapped following the method outlined by <span class="citation">Lettens et al. (<a href="#ref-lettens2004soil">2004</a>)</span>. The general idea is that the landscape is stratified into more or less homogenous units and subsequently, the SOC stocks are obtained following the procedure outlined earlier in this practical. <span class="citation">Lettens et al. (<a href="#ref-lettens2004soil">2004</a>)</span> outlines a method to stratify the landscape into homogeneous strata with respect to land use and soil type, as was explained earlier. In order to obtain such strata, the soil map and the land use map need to be combined. This can be done using various types of software, e.g. ArcMap, GRASS, QGIS or <strong>R</strong>.</p>
<p>When using the GIS software, the only thing that needs to be done, is intersecting the vector files and dissolving the newly created polygon features. Depending on the software and the quality of your shapefile, you may experience problems with the geometry of your shapefile. Generally, ArcMap and GRASS correct the geometry when the shapefile is loaded, while QGIS does not do this automatically. There are various ways to correct the geometry, however, correcting the geometry falls outside the scope of this training. Therefore, we give some hints on how to correct your geometry prior to using the functions <em>Intersect</em> and <em>Dissolve</em>.</p>
<ol style="list-style-type: decimal">
<li>Change the land use raster map to 5 km resolution: Right-click the Lu_project_1km file, select <em>Save as</em>. Change the resolution to 5000 meters. Scroll down, check the <em>Pyramids</em> box, and change the resampling method to <em>Nearest Neighbour</em>.</li>
<li>Convert the raster map to a polygon map and add the file to the <strong>Layers Panel</strong>.</li>
<li>Check the validity of the soil map and land use map. Go to <em>Vector</em>, <em>Geometry Tools</em>, and select <em>Check Validity</em>.
Below you find the instructions in case you have no problems with your geometry.</li>
<li>Intersect the soil map and the land use map. In ArcGIS and QGIS you can use <em>Intersection</em> function following <em>Vector</em>, <em>Geoprocessing Tools</em> (in GRASS you have to use the function <em>Overlay</em> from the <em>Vector</em> menu).</li>
<li>Dissolve the newly generated polygons vie <em>Vector</em>, <em>Geoprocessing Tools</em>, select <em>Dissolve</em>.</li>
<li>Next, this layer can be used to continue with the class-matching or geo-matching procedures.</li>
</ol>
<p><strong>How to correct your geometry When encountering problems?</strong></p>
<ul>
<li>Run the <em>v_clean</em> tool from GRASS within QGIS. Open the <em>Processing Toolbox</em>, <em>GRASS GIS 5 Commands</em>, <em>Vector</em>, and select <em>v.clean</em>.</li>
<li>Install the plugin <strong>Processing LWGEOM Provider</strong>. Go to the plugins menu and search for the plugin and install it. You can find the newly installed tool in the <em>Processing Toolbox</em> by typing the name in the <em>Search</em> function.</li>
<li>Manually correct the error nodes of the vector features.</li>
</ul>


</div>
</div>
<div id="RK" class="section level2">
<h2><span class="header-section-number">6.3</span> Regression-Kriging</h2>
<p><em>G.F. Olmedo &amp; Y. Yigini</em></p>
<div id="overview-1" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Overview</h3>
<p>Regression-kriging is a spatial interpolation technique that combines a regression of the dependent variable (target variable) on predictors (i.e. the environmental covariates) with kriging of the prediction residuals. In other words, RK is a hybrid method that combines either a simple or a multiple-linear regression model with ordinary kriging of the prediction residuals.</p>
<p>A multiple regression analysis models the relationship of multiple predictor variables and one dependent variable, i.e. it models the deterministic trend between the target variable and environmental covariates. The modeled relationship between predictors and target are summarized in the regression equation, which can then be applied to a different data set in which the target values are unknown but the predictor variables are known. The regression equation predicts the value of the dependent variable using a linear function of the independent variables.</p>
<p>In this Section, we review the RK method for DSM. First, the deterministic part of the trend is modeled using a regression model. Next, the prediction residuals are kriged. In the regression phase of a RK technique, there is a continuous random variable called the dependent variable (target) <span class="math inline">\(Y\)</span>, which is in our case SOC, and a number of independent variables <span class="math inline">\({x_1, x_2,\dots,x_p}\)</span> which are selected covariates. Our purpose is to predict the value of the dependent variable using a linear function of the independent variables. The values of the independent variables (environmental covariates) are known quantities for purposes of prediction, the model is explained in detail.</p>
</div>
<div id="assumptions" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Assumptions</h3>
<p>Standard linear regression models with standard estimation techniques make a number of assumptions  about the predictor variables, the response variables, and their relationship. One must review the assumptions made when using the model.</p>
<ul>
<li><p><strong>Linearity</strong>: The mean value of <span class="math inline">\(Y\)</span> for each specific combination of the <span class="math inline">\(X\)</span>’s is a linear function of the <span class="math inline">\(X\)</span>’s. In practice this assumption can virtually never be confirmed; fortunately, multiple regression procedures are not greatly affected by minor deviations from this assumption. If curvature in the relationships is evident, one may consider either transforming the variables or explicitly allowing for nonlinear components.\</p></li>
<li><p><strong>Normality Assumption</strong>: It is assumed in multiple regression that the residuals (predicted minus observed values) are distributed normally (i.e., follow the normal distribution). Again, even though most tests (specifically the F-test) are quite robust with regard to violations of this assumption, it is always a good idea, before drawing final conclusions, to review the distributions of the major variables of interest. You can produce histograms of the residuals as well as normal probability plots, in order to inspect the distribution of the residual values.\</p></li>
<li><p><strong>Collinearity</strong>: There is not perfect collinearity in any combination of the <span class="math inline">\(X\)</span>’s. A higher degree of collinearity, or overlap, among independent variables, can cause problems in multiple linear regression models. Collinearity (also multicollinearity) is a phenomenon in which two or more predictors in a multiple regression models are highly correlated. Collinearity causes increase in variances and relatedly increases inaccuracy.\</p></li>
<li><p><strong>Distribution of the Errors</strong>: The error term is normally distributed with a mean of zero and constant variance.\</p></li>
<li><p><strong>Homoscedasticity</strong>: The variance of the error term is constant for all combinations of <span class="math inline">\(X\)</span>’s. The term homoscedasticity means <em>same scatter</em>. Its antonym is heteroscedasticity which means <em>different scatter</em>.</p></li>
</ul>
</div>
<div id="pre-processing-of-covariates" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Pre-processing of covariates</h3>
<p>Before using the selected predictors, multicollinearity assumption must be reviewed. As an assumption, there is not perfect collinearity in any combination of the <span class="math inline">\(X\)</span>’s. A higher degree of collinearity, or overlap, among independent variables, can cause problems in multiple linear regression models. The multicollinearity of a number of variables can be assessed using Variance Inflation Factor (VIF).</p>
<p>In <strong>R</strong>, the function <code>vif()</code> from <strong>caret</strong> package can estimate the VIF . There are several rules of thumb to establish when there is a serious multi-collinearity (e.g. when the VIF square root is over 2). The principal component analysis (PCA) can be used to overcome multicollinearity issues.</p>
<p>Principal components analysis can cope with data containing large numbers of covariates that are highly collinear which is the common case in environmental predictors. Often the principal components with higher variances are selected as regressors. However, for the purpose of predicting the outcome, the principal components with low variances may also be important, in some cases even more important.</p>
<p>The PCA and the linear regression (PCR) method may be coarsely divided into three main steps:</p>
<ol style="list-style-type: decimal">
<li>Run PCA on the data matrix for the predictors to obtain the principal components, and then select a subset of the principal components for further use.</li>
<li>Regress the dependent variable on the selected principal components as covariates, linear regression to get estimated regression coefficients.</li>
<li>Transforming the data back to the scale of the actual covariates, using the selected PCA loadings.</li>
</ol>
</div>
<div id="the-terminology" class="section level3">
<h3><span class="header-section-number">6.3.4</span> The terminology</h3>
<ul>
<li><strong>Dependent variable <span class="math inline">\(Y\)</span></strong>:  What we are trying to predict (e.g. SOC content).</li>
<li><strong>Independent variables <span class="math inline">\(X\)</span>’s (Predictors)</strong>: Variables that we believe influence or explain the dependent variable (Covariates: environmental covariates, DEM derived covariates, soil maps, land cover maps, climate maps). The data sources for the environmental predictors are provided in Chapter <a href="covariates.html#covariates">5</a>.</li>
<li><strong>Coefficients <span class="math inline">\(\beta\)</span></strong>: Values, computed by the multiple regression tool, reflect the relationship and strength of each independent variable to the dependent variable.</li>
<li><strong>Residuals <span class="math inline">\(\varepsilon\)</span></strong>: The portion of the dependent variable that cannot be explained by the model; the model under/over predictions.</li>
</ul>
<div class="figure">
<img src="images/RKequation.png" alt="Linear regression model" />
<p class="caption">Linear regression model</p>
</div>
<p>Before we proceed with the regression analysis, it is advisable to inspect the histogram of the dependent/target variable, in order to see if it needs to be transformed before fitting the regression model. The data for the selected soil property is normal when the frequency distribution of the values follow a bell-shaped curve (Gaussian distribution) which is symmetric around its mean. Normality tests may be used to assess normality. If a normality test indicates that data are not normally distributed, it may be necessary to transform the data to meet the normality assumption.</p>
<p>Both, the normality tests and the data transformation can be easily performed using any commercial or open source statistical tool (<strong>R</strong>, SPSS, MINITAB…).</p>
<p>The main steps for the multiple linear regression analysis are shown in the Figure <a href="mappingMethods.html#fig:workflowRK">6.10</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:workflowRK"></span>
<img src="images/RKworkflow.png" alt="Workflow for regression-kriging" width="80%" />
<p class="caption">
Figure 6.10: Workflow for regression-kriging
</p>
</div>
<ol style="list-style-type: decimal">
<li>The first step is to prepare a map showing the spatial distribution of the sample locations and the corresponding soil property information, e.g. soil organic matter and environmental properties. The first can be achieved as outlined in Section [Overlay covariates and spatial data]. The overlaying operation can be performed in <strong>R</strong>, ArcGIS, SAGA GIS or QGIS.</li>
<li>The essential part of multiple regression analysis is to build a regression model by using the environmental predictors. After extracting the values of explanatory maps and target variables into the single table, we can now start fitting multiple regression model using the table that contains data from dependent variable and predictors.</li>
<li>In particular cases, stepwise multiple linear regression (MLR) can be used to eliminate insignificant predictors. The stepwise MLR usually selects predictors that have the strongest linear correlations with the target variable, which reflect the highest predictive capacity.</li>
<li>Kriging of the residuals (prediction errors): In RK, the regression model detrends the data, produces the residuals which we need to krige and to be added to the regression model predictions.</li>
</ol>
</div>
<div id="interpret-the-key-results-of-multiple-regression" class="section level3">
<h3><span class="header-section-number">6.3.5</span> Interpret the Key Results of Multiple Regression</h3>
<p>Regression analysis generates an equation to describe the statistical relationship between one or more predictor variables and the response variable. The <span class="math inline">\(R\)</span>-squared, <span class="math inline">\(p\)</span>-values and coefficients that appear in the output for linear regression analysis must also be reviewed. Before accepting the result of a linear regression it is important to evaluate its suitability at explaining the data. One of the many ways to do this is to visually examine the residuals. If the model is appropriate, then the residual errors should be random and normally distributed.</p>
<p><strong><span class="math inline">\(R\)</span>-squared</strong></p>
<p><span class="math inline">\(R\)</span>-sqared (<span class="math inline">\(R^2\)</span>) is the percentage of variation in the response that is explained by the model. The higher the <span class="math inline">\(R^2\)</span> value is, the better the model fits your data. <span class="math inline">\(R^2\)</span> ranges between 0% and 100%. <span class="math inline">\(R^2\)</span> usually increases when additional predictors are added in the model.</p>
<p><strong><span class="math inline">\(p\)</span>-value</strong></p>
<p>To determine whether the association between the dependent and each predictor in the model is statistically significant, compare the <span class="math inline">\(p\)</span>-value for the term to your significance level to assess the null hypothesis. Usually, a significance level of 0.05 works well.</p>
<ul>
<li><strong><span class="math inline">\(p\)</span>-value <span class="math inline">\(\leq\)</span> significance level</strong>: The relationship is statistically significant. If the <span class="math inline">\(p\)</span>-value is less than or equal to the significance level, we can conclude that there is a statistically significant relationship between the dependent variable and the predictor.</li>
<li><strong><span class="math inline">\(p\)</span>-value &gt; significance level</strong>: The relationship is not statistically significant. If the <span class="math inline">\(p\)</span>-value is greater than the significance level, you cannot conclude that there is a statistically significant relationship between the dependent variable and the predictor. You may want to refit the model without the predictor.</li>
</ul>
<p><strong>Residuals</strong></p>
<p>We can plot the residuals which can help us determine whether the model is adequate and meets the assumptions of the analysis. If the model is appropriate, then the residual errors should be random and normally distributed. We can plot residuals versus fits to verify the assumption that the residuals are randomly distributed and have constant variance. Ideally, the points should fall randomly around zero, with no recognizable patterns in the points.</p>
<p>The diagnostic plots for the model should be evaluated to confirm if all the assumptions of linear regression are met. After the abovementioned assumptions are validated, we can proceed with making the prediction map using the model with significant predictors.</p>
</div>
<div id="using-the-results-of-a-regression-analysis-to-make-predictions" class="section level3">
<h3><span class="header-section-number">6.3.6</span> Using the results of a regression analysis to make predictions</h3>
<p>The purpose of a regression analysis, of course, is to develop a model that can be used to make the prediction of a dependent variable. The derived regression equation is to be used to create the prediction map for the dependent variable.</p>
<blockquote>
<p>Raster calculation can be easily performed using <strong>raster</strong> package in <strong>R</strong> or ArcGIS using the <strong>Raster Calculator</strong> tool (is called <strong>Map Algebra</strong> in the prior versions).</p>
</blockquote>
</div>
<div id="technical-steps---regression-kriging" class="section level3">
<h3><span class="header-section-number">6.3.7</span> Technical steps - Regression-kriging</h3>
<p><strong>Requirements</strong></p>
<p>The following computational steps from the previous Chapters are required to implement RK in <strong>R</strong>:</p>
<ol style="list-style-type: decimal">
<li>Setting-up the software environment.</li>
<li>Obtaining and installing <strong>RStudio</strong>.</li>
<li>Installing and loading the required <strong>R</strong> packages.</li>
<li>Preparation of local soil property data.</li>
<li>Preparation of spatial covariates (DEM-derived covariates, land cover/land use, climate, parent material).</li>
</ol>
<p><strong>Step 1 - Setting working space and initial steps</strong></p>
<p>One of the first steps should be setting our working directory. If you read/write files from/to disk, this takes place in the working directory. If we do not set the working directory, we could easily write files to an undesirable file location. The following example shows how to set the working directory in <strong>R</strong> to our folder which contains data for the study area (point data, covariates).</p>
<p>Note that we must use the forward slash / or double backslash \\ in <strong>R</strong>! Single backslash \ will not work. Now we can check if the working directory has been correctly set by using the <code>getwd()</code> function:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" data-line-number="1"><span class="kw">getwd</span>()</a></code></pre></div>
<p><strong>Step 2 - Data preparation</strong></p>
<p><strong>Point dataset</strong></p>
<p>We previously applied spline function to produce continuous soil information to a given soil depth (0 cm - 30 cm) in the Section <a href="preparation.html#EqualAreaSplines">4.4.2</a>. Spline function basically imports soil profile data (including instances where layers are not contiguous), fits it to a mass-preserving spline and outputs attribute means for a given depth. The output file should contain the profile ID <code>id</code>, upper (surface) and lower depth (30 cm), estimated value for the selected soil attribute (Value) and tmse (estimated mean squared error of the spline).</p>
<p>If you used the Spline Tool V2, the coordinates were not kept in the output file. The coordinates should be added back in the data table. You can use profile IDs to add the <code>X</code> and <code>Y</code> columns back. Once your point dataset is ready, copy this table into your working directory as a .csv file.</p>
<p><strong>Environmental predictors (covariates)</strong></p>
<p>In the Chapter <a href="covariates.html#covariates">5</a>, we presented and prepared several global and continental datasets. In addition to these datasets, numerous covariate layers have been prepared by ISRIC for the GSOCmap project. These are GIS raster layers of various biophysical earth surface properties for each country in the world. Some of these layers will be used as predictors in this Section. Please download the covariates for your own study area from GSOCmap Data Repository as explained in Section <a href="covariates.html#GSOCDataRepo">5.6</a>.</p>
<p>In Section <a href="covariates.html#overlay-soil-covariates">5.9</a>, a table with the points values after data preparation and the values of our spatial predictors was prepared. This step involves loading this table.</p>
<p>Now we will import our point dataset using <code>read.csv()</code> function. The easiest way to create a data frame is to read in data from a file. This is done using the function <code>read.csv()</code>, which works with comma delimited files. Data can be read in from other file formats as well, using different functions, but <code>read.csv()</code> is the most commonly used approach. <strong>R</strong> is very flexible in how it reads in data from text files (<code>read.table()</code>, <code>read.csv()</code>, <code>read.csv2()</code>, <code>read.delim()</code>, <code>read.delim2()</code>). Please type <code>?read.table()</code> for help.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1"><span class="co"># Load data</span></a>
<a class="sourceLine" id="cb51-2" data-line-number="2">dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/MKD_RegMatrix.csv&quot;</span>)</a>
<a class="sourceLine" id="cb51-3" data-line-number="3"></a>
<a class="sourceLine" id="cb51-4" data-line-number="4">dat<span class="op">$</span>LCEE10 &lt;-<span class="st"> </span><span class="kw">as.factor</span>(dat<span class="op">$</span>LCEE10)</a>
<a class="sourceLine" id="cb51-5" data-line-number="5">dat<span class="op">$</span>soilmap &lt;-<span class="st"> </span><span class="kw">as.factor</span>(dat<span class="op">$</span>soilmap)</a>
<a class="sourceLine" id="cb51-6" data-line-number="6"></a>
<a class="sourceLine" id="cb51-7" data-line-number="7"><span class="co"># Explore the data structure</span></a>
<a class="sourceLine" id="cb51-8" data-line-number="8"><span class="kw">str</span>(dat)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    2897 obs. of  23 variables:
##  $ id       : Factor w/ 2897 levels &quot;P0003&quot;,&quot;P0007&quot;,..: 1 2 3 4..
##  $ Y        : num  42 42 42.1 42 42 ...
##  $ X        : num  20.8 20.8 20.8 20.9 20.9 ...
##  $ SOC      : num  26.38 6.15 3.94 3.26 2.29 ...
##  $ BLD      : num  0.73 1.17 1.3 1.34 1.41 ...
##  $ CRFVOL   : num  8 18.6 31.9 21.7 14.5 ...
##  $ OCSKGM   : num  5.32 1.75 1.04 1.03 0.83 ...
##  $ meaERROR : num  2.16 2.85 2.65 3.16 3.63 2.83 2.94 2.49 2.77..
##  $ OCSKGMlog: num  1.6712 0.5591 0.0429 0.0286 -0.1862 ...
##  $ B04CHE3  : num  574 553 693 743 744 ...
##  $ B07CHE3  : num  38.5 37.8 42.1 43.7 43.7 ...
##  $ B13CHE3  : num  111.6 125 99.8 118.1 121 ...
##  $ B14CHE3  : num  59.2 60.3 42.4 39.9 38.7 ...
##  $ DEMENV5  : int  2327 2207 1243 1120 1098 1492 1413 1809 1731..
##  $ LCEE10   : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 3 2 1 2 2 2..
##  $ PRSCHE3  : num  998 1053 780 839 844 ...
##  $ SLPMRG5  : int  13 36 6 25 30 24 15 17 20 43 ...
##  $ TMDMOD3  : int  282 280 285 288 289 287 286 286 287 286 ...
##  $ TMNMOD3  : int  272 270 277 279 279 277 277 273 274 273 ...
##  $ TWIMRG5  : int  61 62 81 66 65 72 68 67 65 59 ...
##  $ VBFMRG5  : int  0 0 14 0 0 0 0 0 0 0 ...
##  $ VDPMRG5  : int  311 823 10048 1963 -173 -400 -9 -692 -1139 2..
##  $ soilmap  : Factor w/ 20 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 6 14 14 3..</code></pre>
<p>Since we will be working with spatial data we need to define the coordinates for the imported data. Using the <code>coordinates()</code> function from the <strong>sp</strong>  package we can define the columns in the data frame to refer to spatial coordinates. Here the coordinates are listed in columns <code>X</code> and <code>Y</code>.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1"><span class="kw">library</span>(sp)</a>
<a class="sourceLine" id="cb53-2" data-line-number="2"></a>
<a class="sourceLine" id="cb53-3" data-line-number="3"><span class="co"># Promote to SpatialPointsDataFrame</span></a>
<a class="sourceLine" id="cb53-4" data-line-number="4"><span class="kw">coordinates</span>(dat) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>Y</a>
<a class="sourceLine" id="cb53-5" data-line-number="5"></a>
<a class="sourceLine" id="cb53-6" data-line-number="6"><span class="kw">class</span>(dat)</a></code></pre></div>
<pre><code>## [1] &quot;SpatialPointsDataFrame&quot;
## attr(,&quot;package&quot;)
## [1] &quot;sp&quot;</code></pre>
<p><code>SpatialPointsDataFrame</code> structure is essentially the same as a data frame, except that additional <em>spatial</em> elements have been added or partitioned into slots. Some important ones being the bounding box (sort of like the spatial extent of the data), and the coordinate reference system <code>proj4string()</code>, which we need to define for the sample dataset. To define the CRS, we must know where our data are from, and what was the corresponding CRS used when recording the spatial information in the field. For this data set, the CRS used was WGS84 (EPSG:4326).</p>
<p>To clearly tell <strong>R</strong> this information we define the CRS which describes a reference system in a way understood by the <a href="http://trac.osgeo.org/proj/">PROJ.4</a> projection library. An interface to the PROJ.4 library is available in the <strong>rgdal</strong> package. As an alternative to using PROJ.4 character strings, we can use the corresponding yet simpler EPSG code. <strong>rgdal</strong> also recognizes these codes. If you are unsure of the PROJ.4 or EPSG code for the spatial data that you have but know the CRS, you should consult <a href="http://spatialreference.org/" class="uri">http://spatialreference.org/</a> for assistance.</p>
<blockquote>
<p><strong>CRS</strong>: Please note that, when working with spatial data, it is very important that the CRS of the point data and covariates are the same.</p>
</blockquote>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb55-1" data-line-number="1">dat<span class="op">@</span>proj4string &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="dt">projargs =</span> <span class="st">&quot;+init=epsg:4326&quot;</span>)</a>
<a class="sourceLine" id="cb55-2" data-line-number="2"></a>
<a class="sourceLine" id="cb55-3" data-line-number="3">dat<span class="op">@</span>proj4string</a></code></pre></div>
<pre><code>## CRS arguments:
##  +init=epsg:4326 +proj=longlat +datum=WGS84 +no_defs
## +ellps=WGS84 +towgs84=0,0,0</code></pre>
<p>Now we will import the covariates. When the covariate layers are in common resolution and extent, rather than working with individual rasters it is better to stack them all into a single <strong>R</strong> object. In this example, we use 13 covariates from the GSOCmap Data Repository and a rasterized version of the soil type map. The rasterization of vectorial data was covered in <a href="covariates.html#technical-steps---rasterizing-a-vector-layer-in-r">Technical Steps - Rasterizing a vector layer in R</a>. The file containing all the covariates was prepared at the end of Chapter <a href="covariates.html#covariates">5</a>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1"><span class="kw">load</span>(<span class="dt">file =</span> <span class="st">&quot;covariates.RData&quot;</span>)</a>
<a class="sourceLine" id="cb57-2" data-line-number="2"></a>
<a class="sourceLine" id="cb57-3" data-line-number="3"><span class="kw">names</span>(covs)</a></code></pre></div>
<pre><code>##  [1] &quot;B04CHE3&quot; &quot;B07CHE3&quot; &quot;B13CHE3&quot; &quot;B14CHE3&quot; &quot;DEMENV5&quot; &quot;LCEE10&quot; 
##  [7] &quot;PRSCHE3&quot; &quot;SLPMRG5&quot; &quot;TMDMOD3&quot; &quot;TMNMOD3&quot; &quot;TWIMRG5&quot; &quot;VBFMRG5&quot;
## [13] &quot;VDPMRG5&quot; &quot;soilmap&quot;</code></pre>
<p><strong>Step 3 - Fitting the MLR model</strong></p>
<p>It would be better to progress with a data frame of just the data and covariates required for the modeling. In this case, we will subset the columns SOC, the covariates and the spatial coordinates (<code>X</code> and <code>Y</code>).</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1">datdf &lt;-<span class="st"> </span>dat<span class="op">@</span>data</a>
<a class="sourceLine" id="cb59-2" data-line-number="2"></a>
<a class="sourceLine" id="cb59-3" data-line-number="3">datdf &lt;-<span class="st"> </span>datdf[, <span class="kw">c</span>(<span class="st">&quot;OCSKGM&quot;</span>, <span class="kw">names</span>(covs))]</a></code></pre></div>
<p>Let’s fit a linear model with all available covariates.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" data-line-number="1"><span class="co"># Fit a multiple linear regression model between the log-transformed</span></a>
<a class="sourceLine" id="cb60-2" data-line-number="2"><span class="co"># values of OCS and the top 20 covariates</span></a>
<a class="sourceLine" id="cb60-3" data-line-number="3">model.MLR &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(OCSKGM) <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> datdf)</a></code></pre></div>
<p>From the summary of our fitted model (<code>model.MLR</code>) above, it seems only a few of the covariates are significant in describing the spatial variation of the target variable. To determine the most predictive model we can run a stepwise regression  using the <code>step()</code> function. With this function, we can also specify the mode of stepwise search, can be one of <em>both</em>, <em>backward</em>, or <em>foreward</em>.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1"><span class="co"># Stepwise variable selection</span></a>
<a class="sourceLine" id="cb61-2" data-line-number="2">model.MLR.step &lt;-<span class="st"> </span><span class="kw">step</span>(model.MLR, <span class="dt">direction=</span><span class="st">&quot;both&quot;</span>)</a></code></pre></div>
<p>Comparing the summary of both the full and stepwise linear models, there is very little difference between the models such as the <span class="math inline">\(R^2\)</span>. Both models explain about 23% of variation of the target variable. Obviously, the full model is more complex as it has more parameters than the step model.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1"><span class="co"># Summary and ANOVA of the new model using stepwise covariates</span></a>
<a class="sourceLine" id="cb62-2" data-line-number="2"><span class="co"># selection</span></a>
<a class="sourceLine" id="cb62-3" data-line-number="3"><span class="kw">summary</span>(model.MLR.step)</a>
<a class="sourceLine" id="cb62-4" data-line-number="4"><span class="kw">anova</span>(model.MLR.step)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(OCSKGM) ~ B04CHE3 + B07CHE3 + B13CHE3 + DEMENV5 + 
##     LCEE10 + PRSCHE3 + SLPMRG5 + TMDMOD3 + TMNMOD3 + VBFMRG5 + 
##     VDPMRG5 + soilmap, data = datdf)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3625 -0.2637  0.0368  0.3111  1.8859 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  9.166e+00  4.343e+00   2.111  0.03489 *  
## B04CHE3     -5.877e-03  1.099e-03  -5.347 9.64e-08 ***
## B07CHE3      1.110e-01  3.460e-02   3.209  0.00135 ** 
## B13CHE3     -4.361e-03  1.640e-03  -2.659  0.00788 ** 
## DEMENV5     -1.882e-04  8.926e-05  -2.108  0.03508 *  
## LCEE102      9.745e-02  3.369e-02   2.893  0.00385 ** 
## LCEE103      1.399e-01  5.490e-02   2.548  0.01088 *  
## LCEE104     -3.612e-02  4.360e-02  -0.829  0.40741    
## PRSCHE3      9.174e-04  3.139e-04   2.923  0.00350 ** 
## SLPMRG5     -2.440e-03  1.508e-03  -1.619  0.10559    
## TMDMOD3     -5.584e-02  7.612e-03  -7.336 2.86e-13 ***
## TMNMOD3      2.467e-02  1.291e-02   1.911  0.05611 .  
## VBFMRG5      4.941e-04  9.867e-05   5.008 5.84e-07 ***
## VDPMRG5     -2.696e-05  4.360e-06  -6.185 7.11e-10 ***
## soilmap2    -3.848e-01  4.885e-01  -0.788  0.43090    
## soilmap3    -2.094e-01  4.825e-01  -0.434  0.66429    
## soilmap4    -1.955e-01  4.886e-01  -0.400  0.68919    
## soilmap5    -6.323e-02  5.202e-01  -0.122  0.90327    
## soilmap6     2.087e-01  4.841e-01   0.431  0.66649    
## soilmap7     1.459e-01  4.857e-01   0.300  0.76398    
## soilmap8    -1.875e-01  4.848e-01  -0.387  0.69904    
## soilmap9    -3.278e-01  4.817e-01  -0.681  0.49617    
## soilmap10   -1.001e-01  4.833e-01  -0.207  0.83590    
## soilmap11   -3.587e-01  4.874e-01  -0.736  0.46188    
## soilmap12   -2.135e-01  4.822e-01  -0.443  0.65797    
## soilmap13    3.091e-01  5.563e-01   0.556  0.57855    
## soilmap14   -2.224e-01  4.828e-01  -0.461  0.64506    
## soilmap15   -1.905e-01  4.876e-01  -0.391  0.69600    
## soilmap16   -3.482e-01  4.831e-01  -0.721  0.47112    
## soilmap17   -1.478e-01  4.838e-01  -0.306  0.75996    
## soilmap18   -8.714e-02  4.830e-01  -0.180  0.85684    
## soilmap19   -5.491e-01  5.082e-01  -1.080  0.28002    
## soilmap20   -3.123e-01  4.846e-01  -0.644  0.51934    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4806 on 2864 degrees of freedom
## Multiple R-squared:  0.2472, Adjusted R-squared:  0.2388 
## F-statistic: 29.38 on 32 and 2864 DF,  p-value: &lt; 2.2e-16
## 
## Analysis of Variance Table
## 
## Response: log(OCSKGM)
##             Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
## B04CHE3      1 111.35 111.347 482.0082 &lt; 2.2e-16 ***
## B07CHE3      1   2.33   2.335  10.1060  0.001494 ** 
## B13CHE3      1   1.64   1.642   7.1059  0.007726 ** 
## DEMENV5      1   5.07   5.067  21.9339 2.953e-06 ***
## LCEE10       3  17.22   5.740  24.8497 7.201e-16 ***
## PRSCHE3      1   4.91   4.910  21.2530 4.201e-06 ***
## SLPMRG5      1   1.97   1.971   8.5305  0.003520 ** 
## TMDMOD3      1   3.75   3.749  16.2300 5.756e-05 ***
## TMNMOD3      1   0.60   0.602   2.6081  0.106428    
## VBFMRG5      1  12.75  12.750  55.1943 1.433e-13 ***
## VDPMRG5      1  10.80  10.797  46.7375 9.880e-12 ***
## soilmap     19  44.83   2.359  10.2135 &lt; 2.2e-16 ***
## Residuals 2864 661.60   0.231                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>In those two models above, we used all available points. It is important to test the performance of a model based upon an external validation.</p>
<p>Let’s fit a new model using a random subset of the available data. We will sample 70% of the SOC data for the model calibration data set.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1"><span class="co"># Graphical diagnosis of the regression analysis</span></a>
<a class="sourceLine" id="cb64-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb64-3" data-line-number="3"><span class="kw">plot</span>(model.MLR.step)</a></code></pre></div>
<pre><code>## Warning: not plotting observations with leverage one:
##   340

## Warning: not plotting observations with leverage one:
##   340</code></pre>
<p><img src="SOCMapping_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</a></code></pre></div>
<p>
</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" data-line-number="1"><span class="co"># Collinearity test using variance inflation factors</span></a>
<a class="sourceLine" id="cb67-2" data-line-number="2"><span class="kw">library</span>(car)</a>
<a class="sourceLine" id="cb67-3" data-line-number="3"><span class="kw">vif</span>(model.MLR.step)</a></code></pre></div>
<pre><code>##              GVIF Df GVIF^(1/(2*Df))
## B04CHE3 17.352988  1        4.165692
## B07CHE3 17.580993  1        4.192969
## B13CHE3 14.463943  1        3.803149
## DEMENV5 14.325207  1        3.784866
## LCEE10   3.349841  3        1.223219
## PRSCHE3 20.323563  1        4.508166
## SLPMRG5  3.201225  1        1.789197
## TMDMOD3  6.724708  1        2.593204
## TMNMOD3  6.172634  1        2.484479
## VBFMRG5  4.182605  1        2.045142
## VDPMRG5  1.933334  1        1.390444
## soilmap 16.784795 19        1.077047</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1"><span class="co"># Problematic covariates should have sqrt (VIF) &gt; 2</span></a>
<a class="sourceLine" id="cb69-2" data-line-number="2"><span class="kw">sqrt</span>(<span class="kw">vif</span>(model.MLR.step))</a></code></pre></div>
<pre><code>##             GVIF       Df GVIF^(1/(2*Df))
## B04CHE3 4.165692 1.000000        2.041003
## B07CHE3 4.192969 1.000000        2.047674
## B13CHE3 3.803149 1.000000        1.950166
## DEMENV5 3.784866 1.000000        1.945473
## LCEE10  1.830257 1.732051        1.105992
## PRSCHE3 4.508166 1.000000        2.123244
## SLPMRG5 1.789197 1.000000        1.337609
## TMDMOD3 2.593204 1.000000        1.610343
## TMNMOD3 2.484479 1.000000        1.576223
## VBFMRG5 2.045142 1.000000        1.430085
## VDPMRG5 1.390444 1.000000        1.179171
## soilmap 4.096925 4.358899        1.037809</code></pre>
<p>Colinear: Temperature seasonality at 1 km (B04CHE3) and Temperature Annual Range [°C] at 1 km (B07CHE3).</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" data-line-number="1"><span class="co"># Removing B07CHE3 from the stepwise model</span></a>
<a class="sourceLine" id="cb71-2" data-line-number="2">model.MLR.step &lt;-<span class="st"> </span><span class="kw">update</span>(model.MLR.step, . <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>B07CHE3)</a>
<a class="sourceLine" id="cb71-3" data-line-number="3"></a>
<a class="sourceLine" id="cb71-4" data-line-number="4"><span class="co"># Test the vif again</span></a>
<a class="sourceLine" id="cb71-5" data-line-number="5"><span class="kw">sqrt</span>(<span class="kw">vif</span>(model.MLR.step))</a></code></pre></div>
<pre><code>##             GVIF       Df GVIF^(1/(2*Df))
## B04CHE3 2.268418 1.000000        1.506127
## B13CHE3 3.624615 1.000000        1.903842
## DEMENV5 3.645817 1.000000        1.909402
## LCEE10  1.818077 1.732051        1.104762
## PRSCHE3 4.460815 1.000000        2.112064
## SLPMRG5 1.783090 1.000000        1.335324
## TMDMOD3 2.572322 1.000000        1.603846
## TMNMOD3 2.299838 1.000000        1.516522
## VBFMRG5 2.015123 1.000000        1.419550
## VDPMRG5 1.387165 1.000000        1.177780
## soilmap 3.840284 4.358899        1.036043</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" data-line-number="1"><span class="co"># Summary of the new model using stepwise covariates selection</span></a>
<a class="sourceLine" id="cb73-2" data-line-number="2"><span class="kw">summary</span>(model.MLR.step)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(OCSKGM) ~ B04CHE3 + B13CHE3 + DEMENV5 + LCEE10 + 
##     PRSCHE3 + SLPMRG5 + TMDMOD3 + TMNMOD3 + VBFMRG5 + VDPMRG5 + 
##     soilmap, data = datdf)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3857 -0.2662  0.0390  0.3096  1.9418 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.536e+01  3.896e+00   3.943 8.23e-05 ***
## B04CHE3     -2.919e-03  5.994e-04  -4.869 1.18e-06 ***
## B13CHE3     -5.955e-03  1.566e-03  -3.804 0.000146 ***
## DEMENV5     -2.651e-04  8.612e-05  -3.079 0.002099 ** 
## LCEE102      1.006e-01  3.373e-02   2.983 0.002879 ** 
## LCEE103      1.250e-01  5.479e-02   2.281 0.022598 *  
## LCEE104     -2.756e-02  4.358e-02  -0.632 0.527224    
## PRSCHE3      1.063e-03  3.111e-04   3.417 0.000642 ***
## SLPMRG5     -2.041e-03  1.505e-03  -1.356 0.175085    
## TMDMOD3     -5.275e-02  7.563e-03  -6.974 3.80e-12 ***
## TMNMOD3      8.998e-03  1.197e-02   0.752 0.452305    
## VBFMRG5      4.401e-04  9.738e-05   4.519 6.47e-06 ***
## VDPMRG5     -2.792e-05  4.357e-06  -6.410 1.70e-10 ***
## soilmap2    -4.166e-01  4.892e-01  -0.852 0.394494    
## soilmap3    -1.925e-01  4.832e-01  -0.398 0.690372    
## soilmap4    -1.959e-01  4.894e-01  -0.400 0.688957    
## soilmap5    -6.440e-02  5.211e-01  -0.124 0.901641    
## soilmap6     2.098e-01  4.849e-01   0.433 0.665224    
## soilmap7     1.337e-01  4.865e-01   0.275 0.783490    
## soilmap8    -1.838e-01  4.856e-01  -0.379 0.705074    
## soilmap9    -3.376e-01  4.825e-01  -0.700 0.484097    
## soilmap10   -9.951e-02  4.841e-01  -0.206 0.837162    
## soilmap11   -3.570e-01  4.882e-01  -0.731 0.464661    
## soilmap12   -2.104e-01  4.830e-01  -0.436 0.663172    
## soilmap13    2.675e-01  5.570e-01   0.480 0.631148    
## soilmap14   -2.312e-01  4.836e-01  -0.478 0.632615    
## soilmap15   -1.733e-01  4.883e-01  -0.355 0.722668    
## soilmap16   -3.620e-01  4.839e-01  -0.748 0.454464    
## soilmap17   -1.564e-01  4.846e-01  -0.323 0.746854    
## soilmap18   -6.759e-02  4.837e-01  -0.140 0.888886    
## soilmap19   -5.479e-01  5.090e-01  -1.076 0.281873    
## soilmap20   -3.061e-01  4.853e-01  -0.631 0.528351    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4814 on 2865 degrees of freedom
## Multiple R-squared:  0.2445, Adjusted R-squared:  0.2363 
## F-statistic:  29.9 on 31 and 2865 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1"><span class="co"># Outlier test using the Bonferroni test</span></a>
<a class="sourceLine" id="cb75-2" data-line-number="2"><span class="kw">outlierTest</span>(model.MLR.step)</a></code></pre></div>
<pre><code>##       rstudent unadjusted p-value Bonferonni p
## 1967 -7.114604         1.4122e-12   4.0897e-09
## 705  -6.795188         1.3108e-11   3.7959e-08
## 1154 -6.045856         1.6789e-09   4.8621e-06
## 1395 -5.112379         3.3907e-07   9.8194e-04
## 709  -5.078342         4.0515e-07   1.1733e-03
## 1229 -4.842237         1.3520e-06   3.9155e-03
## 1136 -4.468410         8.1860e-06   2.3707e-02</code></pre>
<p><strong>Step 4 - Prediction and residual kriging</strong></p>
<p>Now we can make the predictions and plot the map. We can use either our DSM data table for covariate values or <code>covs</code> object for making our prediction. Using stack avoids the step of arranging all covariates into a table format. If multiple rasters are being used, it is necessary to have them arranged as a rasterStack object. This is useful as it also ensures all the rasters are of the same extent and resolution. Here we can use the raster predict function such as below using the covStack raster stack as we created in the Step 3.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1"><span class="co"># Project point data</span></a>
<a class="sourceLine" id="cb77-2" data-line-number="2">dat &lt;-<span class="st"> </span><span class="kw">spTransform</span>(dat, <span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:6204&quot;</span>))</a>
<a class="sourceLine" id="cb77-3" data-line-number="3"></a>
<a class="sourceLine" id="cb77-4" data-line-number="4"><span class="co"># Project covariates to VN-2000 UTM 48N</span></a>
<a class="sourceLine" id="cb77-5" data-line-number="5">covs &lt;-<span class="st"> </span><span class="kw">projectRaster</span>(covs, <span class="dt">crs =</span> <span class="kw">CRS</span>(<span class="st">&quot;+init=epsg:6204&quot;</span>),</a>
<a class="sourceLine" id="cb77-6" data-line-number="6">                      <span class="dt">method=</span><span class="st">&#39;ngb&#39;</span>)</a>
<a class="sourceLine" id="cb77-7" data-line-number="7"></a>
<a class="sourceLine" id="cb77-8" data-line-number="8">covs<span class="op">$</span>LCEE10 &lt;-<span class="st"> </span><span class="kw">as.factor</span>(covs<span class="op">$</span>LCEE10)</a>
<a class="sourceLine" id="cb77-9" data-line-number="9">covs<span class="op">$</span>soilmap &lt;-<span class="st"> </span><span class="kw">as.factor</span>(covs<span class="op">$</span>soilmap)</a></code></pre></div>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1"><span class="co"># Promote covariates to spatial grid dataframe.</span></a>
<a class="sourceLine" id="cb78-2" data-line-number="2"><span class="co"># Takes some time and a lot of memory!</span></a>
<a class="sourceLine" id="cb78-3" data-line-number="3">covs.sp &lt;-<span class="st"> </span><span class="kw">as</span>(covs, <span class="st">&quot;SpatialGridDataFrame&quot;</span>)</a>
<a class="sourceLine" id="cb78-4" data-line-number="4">covs.sp<span class="op">$</span>LCEE10 &lt;-<span class="st"> </span><span class="kw">as.factor</span>(covs.sp<span class="op">$</span>LCEE10)</a>
<a class="sourceLine" id="cb78-5" data-line-number="5">covs.sp<span class="op">$</span>soilmap &lt;-<span class="st"> </span><span class="kw">as.factor</span>(covs.sp<span class="op">$</span>soilmap)</a></code></pre></div>
<p> </p>
<pre><code>## Checking if any bins have less than 5 points, merging bins when necessary...
## 
## Selected:
##   model      psill    range
## 1   Nug 0.16735323    0.000
## 2   Sph 0.06746394 6646.127
## 
## Tested models, best first:
##    Tested.models kappa      SSerror
## 1            Sph     0 3.964233e-07
## 25           Ste    10 4.182590e-07
## 24           Ste     5 4.266696e-07
## 23           Ste     2 4.501717e-07
## 22           Ste   1.9 4.519816e-07
## 21           Ste   1.8 4.539405e-07
## 20           Ste   1.7 4.560649e-07
## 19           Ste   1.6 4.583753e-07
## 18           Ste   1.5 4.608930e-07
## 17           Ste   1.4 4.636455e-07
## 16           Ste   1.3 4.666616e-07
## 3            Gau     0 4.680499e-07
## 15           Ste   1.2 4.699776e-07
## 14           Ste   1.1 4.736337e-07
## 13           Ste     1 4.776785e-07
## 12           Ste   0.9 4.821684e-07
## 11           Ste   0.8 4.871689e-07
## 10           Ste   0.7 4.927610e-07
## 9            Ste   0.6 4.990387e-07
## 2            Exp     0 5.061150e-07
## 8            Ste   0.5 5.061153e-07
## 7            Ste   0.4 5.141265e-07
## 6            Ste   0.3 5.232363e-07
## 5            Ste   0.2 5.336428e-07
## 4            Ste  0.05 1.148336e-06
## [using universal kriging]</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1"><span class="co"># RK model</span></a>
<a class="sourceLine" id="cb80-2" data-line-number="2"><span class="kw">library</span>(automap)</a>
<a class="sourceLine" id="cb80-3" data-line-number="3"></a>
<a class="sourceLine" id="cb80-4" data-line-number="4"><span class="co"># Run regression-kriging prediction.</span></a>
<a class="sourceLine" id="cb80-5" data-line-number="5"><span class="co"># This step can take hours!</span></a>
<a class="sourceLine" id="cb80-6" data-line-number="6">OCS.krige &lt;-<span class="st"> </span><span class="kw">autoKrige</span>(<span class="dt">formula =</span></a>
<a class="sourceLine" id="cb80-7" data-line-number="7">                         <span class="kw">as.formula</span>(model.MLR.step<span class="op">$</span>call<span class="op">$</span>formula),</a>
<a class="sourceLine" id="cb80-8" data-line-number="8">                       <span class="dt">input_data =</span> dat,</a>
<a class="sourceLine" id="cb80-9" data-line-number="9">                       <span class="dt">new_data =</span> covs.sp,</a>
<a class="sourceLine" id="cb80-10" data-line-number="10">                       <span class="dt">verbose =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb80-11" data-line-number="11">                       <span class="dt">block =</span> <span class="kw">c</span>(<span class="dv">1000</span>, <span class="dv">1000</span>))</a>
<a class="sourceLine" id="cb80-12" data-line-number="12"></a>
<a class="sourceLine" id="cb80-13" data-line-number="13">OCS.krige</a></code></pre></div>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" data-line-number="1"><span class="co"># Convert prediction and standard deviation to rasters</span></a>
<a class="sourceLine" id="cb81-2" data-line-number="2"><span class="co"># and back-tansform the vlaues</span></a>
<a class="sourceLine" id="cb81-3" data-line-number="3">RKprediction &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">raster</span>(OCS.krige<span class="op">$</span>krige_output[<span class="dv">1</span>]))</a>
<a class="sourceLine" id="cb81-4" data-line-number="4">RKpredsd &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">raster</span>(OCS.krige<span class="op">$</span>krige_output[<span class="dv">3</span>]))</a></code></pre></div>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" data-line-number="1"><span class="kw">plot</span>(RKprediction)</a></code></pre></div>
<div class="figure"><span id="fig:predRK"></span>
<img src="SOCMapping_files/figure-html/predRK-1.png" alt="Prediction FYROM map from regression-kriging model" width="672" />
<p class="caption">
Figure 6.11: Prediction FYROM map from regression-kriging model
</p>
</div>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1"><span class="kw">plot</span>(RKpredsd)</a></code></pre></div>
<div class="figure"><span id="fig:predSDRK"></span>
<img src="SOCMapping_files/figure-html/predSDRK-1.png" alt="Standard deviation values for FYROM map from regression-kriging model" width="672" />
<p class="caption">
Figure 6.12: Standard deviation values for FYROM map from regression-kriging model
</p>
</div>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1"><span class="co"># Save results as *.tif files</span></a>
<a class="sourceLine" id="cb84-2" data-line-number="2"><span class="kw">writeRaster</span>(RKprediction, <span class="dt">filename =</span> <span class="st">&quot;results/MKD_OCSKGM_RK.tif&quot;</span>,</a>
<a class="sourceLine" id="cb84-3" data-line-number="3">            <span class="dt">overwrite =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb84-4" data-line-number="4"></a>
<a class="sourceLine" id="cb84-5" data-line-number="5"><span class="kw">writeRaster</span>(RKpredsd, <span class="dt">filename =</span> <span class="st">&quot;results/MKD_OCSKGM_RKpredsd.tif&quot;</span>,</a>
<a class="sourceLine" id="cb84-6" data-line-number="6">            <span class="dt">overwrite =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1"><span class="co"># Save the model</span></a>
<a class="sourceLine" id="cb85-2" data-line-number="2"><span class="kw">saveRDS</span>(model.MLR.step, <span class="dt">file=</span><span class="st">&quot;results/RKmodel.Rds&quot;</span>)</a></code></pre></div>
</div>
<div id="technical-steps---cross-validation-of-regression-kriging-models" class="section level3">
<h3><span class="header-section-number">6.3.8</span> Technical steps - Cross-validation of regression-kriging models</h3>
<p>Cross-validation  is introduced in Section <a href="chvalidation.html#xval">7.4.3</a>. In RK models, <span class="math inline">\(n\)</span>-fold cross-validation and leave-one-out cross-validation can be run using the <code>krige.cv()</code> included in <strong>gstat</strong> <strong>R</strong> package. In this example, we will apply 10 fold cross-validation to our RK model.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1"><span class="co"># autoKrige.cv() does not removes the duplicated points</span></a>
<a class="sourceLine" id="cb86-2" data-line-number="2"><span class="co"># We have to do it manually before running the cross-validation</span></a>
<a class="sourceLine" id="cb86-3" data-line-number="3">dat =<span class="st"> </span>dat[<span class="kw">which</span>(<span class="op">!</span><span class="kw">duplicated</span>(dat<span class="op">@</span>coords)), ]</a>
<a class="sourceLine" id="cb86-4" data-line-number="4"></a>
<a class="sourceLine" id="cb86-5" data-line-number="5">OCS.krige.cv &lt;-<span class="st"> </span><span class="kw">autoKrige.cv</span>(<span class="dt">formula =</span> </a>
<a class="sourceLine" id="cb86-6" data-line-number="6">                            <span class="kw">as.formula</span>(model.MLR.step<span class="op">$</span>call<span class="op">$</span>formula), </a>
<a class="sourceLine" id="cb86-7" data-line-number="7">                            <span class="dt">input_data =</span> dat, <span class="dt">nfold =</span> <span class="dv">10</span>)</a></code></pre></div>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1"><span class="kw">summary</span>(OCS.krige.cv)</a></code></pre></div>
<pre><code>##             [,1]    
## mean_error  0.004726
## me_mean     -0.3918 
## MAE         0.3402  
## MSE         0.2692  
## MSNE        0.9504  
## cor_obspred 0.4471  
## cor_predres -0.3132 
## RMSE        0.5188  
## RMSE_sd     0.9418  
## URMSE       0.5188  
## iqr         0.5223</code></pre>


</div>
</div>
<div id="rf" class="section level2">
<h2><span class="header-section-number">6.4</span> Data mining: random forest</h2>
<p><em>M. Guevara, C. Thine, G.F. Olmedo &amp; R.R. Vargas</em></p>
<div id="overview-2" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Overview</h3>
<p>Data mining uses different forms of statistics, such as machine learning, to explore data matrices for a particular situation, from specific information sources, and with a specific objective. Data mining is used in DSM frameworks to generate spatial and temporal predictions of soil properties or classes in places where no information is available.</p>
<p>Under a data mining-based DSM framework, the exploration of statistical relationships (linear and non-linear) between soil observational data and soil environmental predictors is generally performed by the means of machine learning. Machine learning methods represent a branch of statistics that can be used to automatically extract information from available data, including the non-linear and hidden relationships of high dimensional spaces or hyper-volumes of information when high performance or distributed computing resources are available. Machine learning methods do not rely on statistical assumptions about the spatial structure of soil variability or the empirical relationship of soil available data and its environmental predictors. Therefore machine learning methods are also suitable for digital soil mapping under limited and sparse scenarios of data availability, although in practice the statistical performance of machine learning (or any statistical method) is reduced by a low representativeness of a soil property or class in the statistical space given available data. Machine learning methods can be used for (supervised and unsupervised) regression (e.g., predicting soil organic carbon) or classification (e.g., predicting soil type classes) on digital soil mapping. Machine learning methods can be roughly divided into four main groups: linear-based (e.g., MLR), kernel-based (e.g., kernel weighted nearest neighbors or SVM), probabilistic-based (e.g., Bayesian statistics) and tree-based (e.g., classification and regression trees).</p>
<p>Random forest is a tree-based machine learning algorithm that is popular in DSM because it has proven to be efficient mapping soil properties across a wide range of data scenarios and scales of soil variability. RF can be implemented using open source platforms and this Chapter is devoted to provide a reproducible example of this machine learning algorithm applied to SOC mapping across FYROM.</p>
</div>
<div id="random-forest" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Random forest</h3>
<p>Random forest is a decision-tree-based machine learning method used in DSM for uncovering the statistical relationship between a dependent variable (e.g., soil property) and its predictors. Decision-tree-based models (also known as classifiers) are literally like trees (e.g., with stem, many branches, and leaves). The leaves are the prediction outcomes (final decisions) that flow from higher levels based on decision rules through the stem and the branches <span class="citation">(Breiman et al. <a href="#ref-breiman1984classification">1984</a>)</span>. The decision tree model recursively splits the data into final uniform groups (classes) or unique values based on a set of rules (e.g., based on probability values and hypothesis testing). In random forest, there are many decision trees and each tree recursively splits randomly selected sub-samples from the data (see Figure <a href="mappingMethods.html#fig:rfschema">6.13</a>). The name for RF originates from the fact that the original data is first randomly split into sub-samples, and many decision trees (or forest) are used to model the sub-samples.</p>
<div class="figure" style="text-align: center"><span id="fig:rfschema"></span>
<img src="images/randomForestconcept.png" alt="Schematical representation of data splitting to generate the random subsets used to train regression trees within a random forest model (ensemble of regression trees)" width="80%" />
<p class="caption">
Figure 6.13: Schematical representation of data splitting to generate the random subsets used to train regression trees within a random forest model (ensemble of regression trees)
</p>
</div>
<p>Random forest has been tested by many researchers on DSM (<span class="citation">Poggio, Gimona, and Brewer (<a href="#ref-poggio2013regional">2013</a>)</span>; <span class="citation">Rad et al. (<a href="#ref-rad2014updating">2014</a>)</span>, and references therein). For SOC mapping (which for large areas usually rely on sparse datasets), it holds a lot of promises when compared to other prediction models, because it is practically free of assumptions. RF has shown accuracy of spatial predictions and, given the random selection of subsets and prediction factors, reduce potential over-fitting and data noise <span class="citation">(Wiesmeier et al. <a href="#ref-wiesmeier2011digital">2011</a>)</span>. Over-fitting and data noise are important uncertainty sources across high dimensionally spaces used to represent the soil forming environment. Thus, the advent of open-source platforms and freely downloadable ancillary data (e.g. <a href="http://worldgrids.org/" class="uri">http://worldgrids.org/</a>) to represent the soil forming environment makes of RF and other such models increasingly appealing for DSM.</p>
<p>To predict continuous data (such as carbon density), RF generates an averaged ensemble of regression trees based on bagging, which is the statistical term for the random selection of subsets and predictors to generate each regression tree. Bagging is a bootstrapping aggregation technique where each sample is different from the original data set but resembles it in distribution and variability <span class="citation">(Breiman <a href="#ref-Breiman2001">2001</a>, <a href="#ref-Breiman1996">1996</a>)</span>. Each tree contributes to weight the statistical relationship between a dependent variable (e.g. soil property) and its prediction factors (e.g., terrain attributes, remote sensing, climate layers and/or legacy maps). Each tree (generated using a different subset of available data and random combinations of the prediction factors) is internally evaluated by an out-of-bag cross validation form which allows assessing the relative importance of the available prediction factors. Thus, higher weight is given to the most accurate trees (which use the most informative prediction factors). The final prediction of new data is the weighted average of all generated trees.</p>
<p>This method has been used to generate accurate predictions of SOC from the plot to the global scale and also in a country-specific basis <span class="citation">(Hengl et al. <a href="#ref-hengl2014soilgrids1km">2014</a>; Hengl T. <a href="#ref-hengl2017">2017</a>; Bonfatti et al. <a href="#ref-Bonfatti2016">2016</a>; Guevara et al. <a href="#ref-guevara_2018">2018</a>)</span>. RF can be implemented for DSM using open source code <span class="citation">(e.g., <strong>R</strong> package <strong>randomForest</strong>, see Breiman <a href="#ref-breiman2017cutler">2017</a>)</span> and public sources of environmental information.</p>
<p>The objective of this Chapter is to demonstrate a reproducible framework for the implementation of the RF algorithm applied for SOC predictive mapping, including the uncertainty of model estimates and using open source platforms for statistical computing.</p>
</div>
<div id="conceptual-model-and-data-preparation" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Conceptual model and data preparation</h3>
<p>To use RF for digital soil organic carbon mapping the SCORPAN (Soils, Climate, Organisms, Relief, Parent material, Age and N space) conceptual model <span class="citation">(McBratney, Santos, and Minasny <a href="#ref-mcbratney2003digital">2003</a>; Florinsky <a href="#ref-Florinsky2012">2012</a>)</span> will have take the following form:</p>
<p><span class="math inline">\(\ \text{SOC}_{x,y~t} \sim \text{randomForest} (E_{x,y~t})\)</span></p>
<p>where soil organic carbon estimates (<span class="math inline">\(SOC\)</span>) for a specific site (<span class="math inline">\(x,y\)</span>) and for a specific period of time (<span class="math inline">\(t\)</span>) can be modeled as a Random forest (randomForest) function of the soil forming environment (<span class="math inline">\(\ Ex,y~t\)</span>), which is represented by the SOC prediction factors (e.g., terrain attributes, remote sensing, climate layers and/or legacy maps).</p>
<p>To feed the right side of the equation, <span class="math inline">\(SOC_{x,y~t}\)</span> is usually represented in a tabular form or a geospatial object (e.g., shapefile) with three fundamental columns. Two columns represent the spatial coordinates <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (e.g. latitude and longitude) that are used to extract the values of the prediction factors for the representative locations of the <span class="math inline">\(SOC\)</span> estimates. <span class="math inline">\(SOC\)</span> estimates are represented in a third column (see previous Chapters of this book dealing with the transformation of soil carbon density to mass units). The left side of the equation is generally represented by gridded (raster) files, so all available sources of information should be first harmonized into a common pixel size and coordinate reference system.</p>
</div>
<div id="software" class="section level3">
<h3><span class="header-section-number">6.4.4</span> Software</h3>
<p>For the RF implementation, we will use the platform for statistical computing <strong>R</strong>. This open source object-oriented software relies on specific-contributor libraries. There are several libraries for the implementation of the RF algorithm in <strong>R</strong> as well as several variants of the method that can be used to solve DSM problems.</p>
<p>In this Section, we will show the use of Random forest using the <strong>randomForest</strong>, the <strong>quantregForest</strong>, the <strong>raster</strong> and the <strong>caret</strong> <strong>R</strong> packages. The quantile regression forest <span class="citation">(<strong>quantregForest</strong>; Meinshausen <a href="#ref-meinshausen2006quantile">2006</a>)</span> has two main advantages. First, it can be used to extract the variance of all the trees generated by RF, not just the mean (as in the original <strong>randomForest</strong> package), and therefore we can calculate the dispersion of the full conditional distribution of SOC as a function of the prediction factors, which given available data, represent the RF model uncertainty. Second, the quantile regression forest approach can also run in parallel using all available computational resources, in a way that we can predict and estimate the uncertainty of predictions at reasonable time frames whit large datasets.</p>
</div>
<div id="tunning-random-forest-model-parameters" class="section level3">
<h3><span class="header-section-number">6.4.5</span> Tunning random forest model parameters</h3>
<p>Two important parameters of RF are <em>mtry</em> and <em>ntree</em>. The <em>mtry</em> parameter controls the number of prediction factors that are randomly used on each tree, while the <em>ntree</em> parameter controls the number of trees generated by RF. These two parameters can be selected by the means of cross-validation to maximize the prediction capacity of RF.</p>
<p>We will use the <strong>caret</strong> package to select the most appropriate values for these parameters using 10-fold cross-validation <span class="citation">(Kuhn et al. <a href="#ref-kuhn2017caret">2017</a>)</span>. Tunning the main parameters of RF (or any other model) can be time-consuming in computational terms because implies the need to run and internally validate an independent model for each possible combination of parameter values. Thus, tunning the RF parameters would be relevant, given available data, to achieve the best possible accuracy of predictions.</p>
</div>
<div id="technical-steps---random-forest" class="section level3">
<h3><span class="header-section-number">6.4.6</span> Technical steps - Random forest</h3>
<p>We will use the FYSOM dataset for this exercise (see previous Chapters of this book dealing with data preparation). The first dataset contains in a tabular form the <em>OCSKGM</em> values and the values of the prediction factors for the same locations (e.g., <em>x</em>, <em>y</em>, <em>OCSKGM</em>, covariate<sub>1</sub>, covariate<sub>2</sub> etc.) while the second database is represented by a stack of raster files containing prediction factors across all the area of interest at the spatial resolution of 0.0083º (approx. 1 km). Import the datasets and load in <strong>R</strong> all our libraries of interest.</p>
<p><strong>Step 1 - Data preparation</strong></p>
<p><strong>Point dataset</strong></p>
<p>We previously applied spline function to produce continuous soil information to a given soil depth (0 cm - 30 cm) in the Section <a href="preparation.html#EqualAreaSplines">4.4.2</a>. Spline function basically imports soil profile data (including instances where layers are not contiguous), fits it to a mass-preserving spline and outputs attribute means for a given depth. The output file should contain the profile ID <code>id</code>, upper (surface) and lower depth (30 cm), estimated value for the selected soil attribute (Value) and tmse (estimated mean squared error of the spline).</p>
<p>If you used the Spline Tool V2, the coordinates were not kept in the output file. The coordinates should be added back in the data table. You can use profile IDs to add the <code>X</code> and <code>Y</code> columns back. Once your point dataset is ready, copy this table into your working directory as a .csv file.</p>
<p><strong>Environmental predictors (covariates)</strong></p>
<p>In the Chapter <a href="covariates.html#covariates">5</a>, we presented and prepared several global and continental datasets. In addition to these datasets, numerous covariate layers have been prepared by ISRIC for the GSOCmap project. These are GIS raster layers of various biophysical earth surface properties for each country in the world. Some of these layers will be used as predictors in this Section. Please download the covariates for your own study area from GSOCmap Data Repository as explained in Section <a href="covariates.html#GSOCDataRepo">5.6</a>.</p>
<p>In Section <a href="covariates.html#overlay-soil-covariates">5.9</a>, a table with the points values after data preparation and the values of our spatial predictors was prepared. This step involves loading this table.</p>
<p>Now we will import our point dataset using <code>read.csv()</code> function. The easiest way to create a data frame is to read in data from a file. This is done using the function <code>read.csv()</code>, which works with comma delimited files. Data can be read in from other file formats as well, using different functions, but <code>read.csv()</code> is the most commonly used approach. <strong>R</strong> is very flexible in how it reads in data from text files (<code>read.table()</code>, <code>read.csv()</code>, <code>read.csv2()</code>, <code>read.delim()</code>, <code>read.delim2()</code>). Please type <code>?read.table()</code> for help.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" data-line-number="1"><span class="co"># Load data</span></a>
<a class="sourceLine" id="cb89-2" data-line-number="2">dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/MKD_RegMatrix.csv&quot;</span>)</a>
<a class="sourceLine" id="cb89-3" data-line-number="3"></a>
<a class="sourceLine" id="cb89-4" data-line-number="4">dat<span class="op">$</span>LCEE10 &lt;-<span class="st"> </span><span class="kw">as.factor</span>(dat<span class="op">$</span>LCEE10)</a>
<a class="sourceLine" id="cb89-5" data-line-number="5">dat<span class="op">$</span>soilmap &lt;-<span class="st"> </span><span class="kw">as.factor</span>(dat<span class="op">$</span>soilmap)</a>
<a class="sourceLine" id="cb89-6" data-line-number="6"></a>
<a class="sourceLine" id="cb89-7" data-line-number="7"><span class="co"># Explore the data structure</span></a>
<a class="sourceLine" id="cb89-8" data-line-number="8"><span class="kw">str</span>(dat)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    2897 obs. of  23 variables:
##  $ id       : Factor w/ 2897 levels &quot;P0003&quot;,&quot;P0007&quot;,..: 1 2 3 4..
##  $ Y        : num  42 42 42.1 42 42 ...
##  $ X        : num  20.8 20.8 20.8 20.9 20.9 ...
##  $ SOC      : num  26.38 6.15 3.94 3.26 2.29 ...
##  $ BLD      : num  0.73 1.17 1.3 1.34 1.41 ...
##  $ CRFVOL   : num  8 18.6 31.9 21.7 14.5 ...
##  $ OCSKGM   : num  5.32 1.75 1.04 1.03 0.83 ...
##  $ meaERROR : num  2.16 2.85 2.65 3.16 3.63 2.83 2.94 2.49 2.77..
##  $ OCSKGMlog: num  1.6712 0.5591 0.0429 0.0286 -0.1862 ...
##  $ B04CHE3  : num  574 553 693 743 744 ...
##  $ B07CHE3  : num  38.5 37.8 42.1 43.7 43.7 ...
##  $ B13CHE3  : num  111.6 125 99.8 118.1 121 ...
##  $ B14CHE3  : num  59.2 60.3 42.4 39.9 38.7 ...
##  $ DEMENV5  : int  2327 2207 1243 1120 1098 1492 1413 1809 1731..
##  $ LCEE10   : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 3 2 1 2 2 2..
##  $ PRSCHE3  : num  998 1053 780 839 844 ...
##  $ SLPMRG5  : int  13 36 6 25 30 24 15 17 20 43 ...
##  $ TMDMOD3  : int  282 280 285 288 289 287 286 286 287 286 ...
##  $ TMNMOD3  : int  272 270 277 279 279 277 277 273 274 273 ...
##  $ TWIMRG5  : int  61 62 81 66 65 72 68 67 65 59 ...
##  $ VBFMRG5  : int  0 0 14 0 0 0 0 0 0 0 ...
##  $ VDPMRG5  : int  311 823 10048 1963 -173 -400 -9 -692 -1139 2..
##  $ soilmap  : Factor w/ 20 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 6 14 14 3..</code></pre>
<p>Since we will be working with spatial data we need to define the coordinates for the imported data. Using the <code>coordinates()</code> function from the <strong>sp</strong> package we can define the columns in the data frame to refer to spatial coordinates. Here the coordinates are listed in columns <code>X</code> and <code>Y</code>.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1"><span class="kw">library</span>(sp)</a>
<a class="sourceLine" id="cb91-2" data-line-number="2"></a>
<a class="sourceLine" id="cb91-3" data-line-number="3"><span class="co"># Promote to spatialPointsDataFrame</span></a>
<a class="sourceLine" id="cb91-4" data-line-number="4"><span class="kw">coordinates</span>(dat) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>Y</a>
<a class="sourceLine" id="cb91-5" data-line-number="5"></a>
<a class="sourceLine" id="cb91-6" data-line-number="6"><span class="kw">class</span>(dat)</a></code></pre></div>
<pre><code>## [1] &quot;SpatialPointsDataFrame&quot;
## attr(,&quot;package&quot;)
## [1] &quot;sp&quot;</code></pre>
<p><code>SpatialPointsDataFrame</code> structure is essentially the same as a data frame, except that additional <em>spatial</em> elements have been added or partitioned into slots. Some important ones being the bounding box (sort of like the spatial extent of the data), and the coordinate reference system <code>proj4string()</code>, which we need to define for the sample dataset. To define the CRS, we must know where our data are from, and what was the corresponding CRS used when recording the spatial information in the field. For this data set, the CRS used was WGS84 (EPSG:4326).</p>
<p>To clearly tell <strong>R</strong> this information we define the CRS which describes a reference system in a way understood by the <a href="http://trac.osgeo.org/proj/">PROJ.4</a> projection library. An interface to the PROJ.4 library is available in the <strong>rgdal</strong> package. As an alternative to using PROJ.4 character strings, we can use the corresponding yet simpler EPSG code. <strong>rgdal</strong> also recognizes these codes. If you are unsure of the PROJ.4 or EPSG code for the spatial data that you have but know the CRS, you should consult <a href="http://spatialreference.org/" class="uri">http://spatialreference.org/</a> for assistance.</p>
<blockquote>
<p><strong>CRS</strong>: Please note that, when working with spatial data, it is very important that the CRS of the point data and covariates are the same.</p>
</blockquote>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1">dat<span class="op">@</span>proj4string &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="dt">projargs =</span> <span class="st">&quot;+init=epsg:4326&quot;</span>)</a>
<a class="sourceLine" id="cb93-2" data-line-number="2"></a>
<a class="sourceLine" id="cb93-3" data-line-number="3">dat<span class="op">@</span>proj4string</a></code></pre></div>
<pre><code>## CRS arguments:
##  +init=epsg:4326 +proj=longlat +datum=WGS84 +no_defs
## +ellps=WGS84 +towgs84=0,0,0</code></pre>
<p>Now we will import the covariates. When the covariate layers are in common resolution and extent, rather than working with individual rasters it is better to stack them all into a single <strong>R</strong> object. In this example, we use 13 covariates from the GSOCmap Data Repository and a rasterized version of the soil type map. The rasterization of vectorial data was covered in <a href="covariates.html#technical-steps---rasterizing-a-vector-layer-in-r">Technical Steps - Rasterizing a vector layer in R</a>. The file containing all the covariates was prepared at the end of Chapter <a href="covariates.html#covariates">5</a>.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" data-line-number="1"><span class="kw">load</span>(<span class="dt">file =</span> <span class="st">&quot;covariates.RData&quot;</span>)</a>
<a class="sourceLine" id="cb95-2" data-line-number="2"></a>
<a class="sourceLine" id="cb95-3" data-line-number="3"><span class="kw">names</span>(covs)</a></code></pre></div>
<pre><code>##  [1] &quot;B04CHE3&quot; &quot;B07CHE3&quot; &quot;B13CHE3&quot; &quot;B14CHE3&quot; &quot;DEMENV5&quot; &quot;LCEE10&quot; 
##  [7] &quot;PRSCHE3&quot; &quot;SLPMRG5&quot; &quot;TMDMOD3&quot; &quot;TMNMOD3&quot; &quot;TWIMRG5&quot; &quot;VBFMRG5&quot;
## [13] &quot;VDPMRG5&quot; &quot;soilmap&quot;</code></pre>
<p>Random forest does not have assumptions about the statistical distribution of the response variable, but it is a good practice prior to model building to analyze the statistical distribution of the response variable (e.g., if is normal or not) and its relationships with the prediction factors. Soil organic carbon tends to have a log-normal distribution with a right-skew, and transforming the original values to its natural logarithm would generate a normal distribution of soil organic carbon values.</p>
<p>For further analysis, we will use the dataset transformed to its natural logarithm (<em>OCSKGMlog</em>) because this transformation, given this dataset, increases the correlation of the response variable and the covariate space.</p>
<p>Keep in mind that selecting the most appropriate prediction factors is required to generate an interpretable model and high accuracy of prediction in places where no information is available. Variable selection ideally should incorporate expert soil knowledge about the study area and statistical criteria (e.g., just to use the best-correlated predictors). Multivariate analysis (e.g., principal component analysis) is a widely used approach to identify informative predictors. Here we use this combination of prediction factors to be consistent with other Chapters of this book and because they were previously selected for this exercise using expert knowledge about the spatial variability of soil organic carbon.</p>
<p>Now, we will build a working hypothesis from our conceptual model, using all the continuous prediction factors for <em>OCSKGMlog</em>:</p>
<p><em>OCSKGMlog</em> ~ randomForest <em>B04CHE3 + B07CHE3 + B13CHE3 + B14CHE3 + DEMENV5 + LCEE10 + PRSCHE3 + SLPMRG5 + TMDMOD3 + TMNMOD3 + TWIMRG5 + VBFMRG5 + VDPMRG5</em></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" data-line-number="1"><span class="co"># For its use on R we need to define a model formula</span></a>
<a class="sourceLine" id="cb97-2" data-line-number="2">fm =<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(<span class="st">&quot;log(OCSKGM) ~&quot;</span>, <span class="kw">paste0</span>(<span class="kw">names</span>(covs[[<span class="op">-</span><span class="dv">14</span>]]),</a>
<a class="sourceLine" id="cb97-3" data-line-number="3">                                            <span class="dt">collapse =</span> <span class="st">&quot;+&quot;</span>))) </a></code></pre></div>
<p>This is the <strong>R</strong> syntax to define a model formula required for the model structure, where soil organic carbon transformed to its natural logarithm (<em>OCSKGMlog</em>) can be predicted as a function of the available prediction factors, each explained in Chapter <a href="covariates.html#covariates">5</a> of this book, e.g., B04CHE3, B07CHE3, B13CHE3, B14CHE3, DEMENV5 , LCEE10, PRSCHE3, SLPMRG5, TMDMOD3, TMNMOD3, TWIMRG5, VBFMRG5, VDPMRG5.</p>
<p>Note that the variable soil map is categorical, so is not included in the correlation analysis. In fact, although soil type polygon maps are in theory powerful predictors for <em>OCSKGM</em> we will not use this map for this exercise, because not all categories in the map are represented by available <em>OCSKGM</em> estimates, therefore this map requires a generalization of soil type units in function of the classes represented by the sites of <em>OCSKGM</em> estimates, which is beyond the scope of this Chapter.</p>
<p>Ideally, the number of observations across all the categories of soil type or any other factorial variable should be balanced. Another alternative to using an unbalanced categorical map is by generating dummy variables, where each category in the map becomes an independent binomial predictor variable (e.g., only 0 and 1 values) as is explained in the following Chapter. The risk of doing so rely upon the potential underestimation of the spatial variability of the target variable under each category with a low density of available data.</p>
<p><strong>Step 2 - Tuning parameters</strong></p>
<p>Now we will use the cross-validation strategy implemented in the train function of the <strong>caret</strong> package <span class="citation">(Kuhn et al. <a href="#ref-kuhn2017caret">2017</a>)</span>, which default is 10-fold. The result of this function includes information to select the best <em>mtry</em> parameter and to decide the appropriate number of trees. The out-of-bag RMSE will be used to select the optimal <em>mtry</em> model. To analyze the <em>ntree</em> parameter we will plot the number of trees against the out-of-bag RMSE, an optimal <em>ntree</em> can be selected with the number of trees when these relationships stabilizes at the minimum possible RMSE (in the y axis). Reduncing the number of trees will reduce the computational demand, which is specially important when dealing with large databases. In the presence of multidimensional and highly correlated prediction factors, avoiding an excessive number of trees will also reduce the risk of model overfitting.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb98-2" data-line-number="2"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb98-3" data-line-number="3"></a>
<a class="sourceLine" id="cb98-4" data-line-number="4"><span class="co"># Default 10-fold cross-validation</span></a>
<a class="sourceLine" id="cb98-5" data-line-number="5">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">savePred=</span>T)</a>
<a class="sourceLine" id="cb98-6" data-line-number="6"></a>
<a class="sourceLine" id="cb98-7" data-line-number="7"><span class="co"># Search for the best mtry parameter</span></a>
<a class="sourceLine" id="cb98-8" data-line-number="8">rfmodel &lt;-<span class="st"> </span><span class="kw">train</span>(fm, <span class="dt">data=</span>dat<span class="op">@</span>data, <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="dt">trControl =</span> ctrl, </a>
<a class="sourceLine" id="cb98-9" data-line-number="9">             <span class="dt">importance=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb98-10" data-line-number="10"></a>
<a class="sourceLine" id="cb98-11" data-line-number="11"><span class="co"># This is a very useful function to compare and test different </span></a>
<a class="sourceLine" id="cb98-12" data-line-number="12"><span class="co"># prediction algorithms.</span></a>
<a class="sourceLine" id="cb98-13" data-line-number="13"><span class="co"># Type names(getModelInfo()) to see all the </span></a>
<a class="sourceLine" id="cb98-14" data-line-number="14"><span class="co"># possibilitites implemented on this function.</span></a></code></pre></div>
<p>The object derived from the train function can be used to generate predictions of <em>OCSKGMlog</em> at the spatial resolution of the prediction factors. Before generating predictions, we will plot the most important predictors sorted in decreasing order of importance. From the variable importance plot, MSE represent an informative measure for variable selection. It is the increase in error (mean squared error, MSE) of predictions which was estimated with out-of-bag-cross validation as a result of prediction factor being permuted with values randomly shuffled. This is one of the strategies that RF uses to reduce overfitting.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1"><span class="co"># Variable importance plot, compare with the correlation matrix</span></a>
<a class="sourceLine" id="cb99-2" data-line-number="2"><span class="co"># Select the best prediction factors and repeat  </span></a>
<a class="sourceLine" id="cb99-3" data-line-number="3"><span class="kw">varImpPlot</span>(rfmodel[<span class="dv">11</span>][[<span class="dv">1</span>]])</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-56"></span>
<img src="SOCMapping_files/figure-html/unnamed-chunk-56-1.png" alt="Model Decreasing Error and Node Purity for the RF model" width="672" />
<p class="caption">
Figure 6.14: Model Decreasing Error and Node Purity for the RF model
</p>
</div>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1"><span class="co"># Check if the error stabilizes </span></a>
<a class="sourceLine" id="cb100-2" data-line-number="2"><span class="kw">plot</span>(rfmodel[<span class="dv">11</span>][[<span class="dv">1</span>]])</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-57"></span>
<img src="SOCMapping_files/figure-html/unnamed-chunk-57-1.png" alt="Select ntree" width="672" />
<p class="caption">
Figure 6.15: Select ntree
</p>
</div>
<p>Random forest users are encouraged to compare and test the prediction capacity of different combinations of prediction factors in order to reduce the complexity of the model and the statistical redundancy of environmental information on further applications of predicted OCSKGM maps (e.g., quantifying the carbon dynamics). The resulting map of our RF model needs to be validated using the independent dataset to complement the results of the cross-validation (e.g., RMSE and explained variance) derived using the train function and to have a more comprehensive interpretation of accuracy and bias. Note how the RMSE and the explained variance derived from the independent validations are slightly lower than the values obtained using cross-validation.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb101-1" data-line-number="1"><span class="co"># Make a prediction across all FYROM</span></a>
<a class="sourceLine" id="cb101-2" data-line-number="2"><span class="co"># Note that the units are still in log</span></a>
<a class="sourceLine" id="cb101-3" data-line-number="3">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(covs, rfmodel)</a></code></pre></div>
<pre><code>## Warning in .local(object, ...): not sure if the correct factor
## levels are used here</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" data-line-number="1"><span class="co"># Back transform predictions log transformed</span></a>
<a class="sourceLine" id="cb103-2" data-line-number="2">pred &lt;-<span class="st"> </span><span class="kw">exp</span>(pred)</a>
<a class="sourceLine" id="cb103-3" data-line-number="3"></a>
<a class="sourceLine" id="cb103-4" data-line-number="4"><span class="co"># Save the result as a tiff file</span></a>
<a class="sourceLine" id="cb103-5" data-line-number="5"><span class="kw">writeRaster</span>(pred, <span class="dt">filename =</span> <span class="st">&quot;results/MKD_OCSKGM_rf.tif&quot;</span>,</a>
<a class="sourceLine" id="cb103-6" data-line-number="6">            <span class="dt">overwrite=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb103-7" data-line-number="7"></a>
<a class="sourceLine" id="cb103-8" data-line-number="8"><span class="kw">plot</span>(pred)</a></code></pre></div>
<div class="figure"><span id="fig:rf-pred"></span>
<img src="SOCMapping_files/figure-html/rf-pred-1.png" alt="SOC prediction map for FYROM using a random forest model" width="672" />
<p class="caption">
Figure 6.16: SOC prediction map for FYROM using a random forest model
</p>
</div>
</div>
<div id="technical-steps---using-quantile-regression-forest-to-estimate-uncertainty" class="section level3">
<h3><span class="header-section-number">6.4.7</span> Technical steps - Using quantile regression forest to estimate uncertainty</h3>
<p>Ideally, a digital soil map should include a spatial explicit metric of uncertainty. The uncertainty can be roughly divided into four main components, uncertainty in soil data, uncertainty in soil covariates, uncertainty in the model and uncertainty in variations of available data. Here, we show an approach to estimate the sensitivity of the model to available data and the uncertainty of the model. The first two are beyond of the aim of this Chapter. For the third and fourth we will generate a reproducible example.</p>
<p><strong>Step 1 - Split the data in into training and testing subsets</strong></p>
<p>To analyze the sensitivity of the model to available data we need to randomly split the data several times (e.g., 10 or more, is possible until the variance stabilizes) in training and testing subsets. A model generation and prediction are made on each split, in a way that the dispersion of the predicted values at the pixel level will represent the uncertainty and the sensitivity of the model to variations in available data.</p>
<p>This process increases computational demand and memory since it will repeat n times (10 in this example) the model and the prediction using each time a different random combination of data for training and testing the models. As larger the sample and the number of realizations the more robust our validation strategy. For this example, we will use only 10 realizations and random splits of 25% of available data.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" data-line-number="1"><span class="kw">library</span>(Metrics)</a>
<a class="sourceLine" id="cb104-2" data-line-number="2"></a>
<a class="sourceLine" id="cb104-3" data-line-number="3"><span class="co"># Generate an empty dataframe</span></a>
<a class="sourceLine" id="cb104-4" data-line-number="4">validation &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">rmse=</span><span class="kw">numeric</span>(), <span class="dt">r2=</span><span class="kw">numeric</span>())</a>
<a class="sourceLine" id="cb104-5" data-line-number="5"></a>
<a class="sourceLine" id="cb104-6" data-line-number="6"><span class="co"># Sensitivity to the dataset</span></a>
<a class="sourceLine" id="cb104-7" data-line-number="7"><span class="co"># Start a loop with 10 model realizations</span></a>
<a class="sourceLine" id="cb104-8" data-line-number="8"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>){</a>
<a class="sourceLine" id="cb104-9" data-line-number="9">  <span class="co"># We will build 10 models using random samples of 25%  </span></a>
<a class="sourceLine" id="cb104-10" data-line-number="10">  smp_size &lt;-<span class="st"> </span><span class="kw">floor</span>(<span class="fl">0.25</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(dat))</a>
<a class="sourceLine" id="cb104-11" data-line-number="11">  train_ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">seq_len</span>(<span class="kw">nrow</span>(dat)), <span class="dt">size =</span> smp_size)</a>
<a class="sourceLine" id="cb104-12" data-line-number="12">  train &lt;-<span class="st"> </span>dat[train_ind, ]</a>
<a class="sourceLine" id="cb104-13" data-line-number="13">  test &lt;-<span class="st"> </span>dat[<span class="op">-</span>train_ind, ]</a>
<a class="sourceLine" id="cb104-14" data-line-number="14">  modn &lt;-<span class="st"> </span><span class="kw">train</span>(fm, <span class="dt">data=</span>train<span class="op">@</span>data, <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, </a>
<a class="sourceLine" id="cb104-15" data-line-number="15">                <span class="dt">trControl =</span> ctrl)</a>
<a class="sourceLine" id="cb104-16" data-line-number="16">  pred &lt;-<span class="st"> </span><span class="kw">stack</span>(pred, <span class="kw">predict</span>(covs, modn))</a>
<a class="sourceLine" id="cb104-17" data-line-number="17">  test<span class="op">$</span>pred &lt;-<span class="st"> </span><span class="kw">extract</span>(pred[[i<span class="op">+</span><span class="dv">1</span>]], test)</a>
<a class="sourceLine" id="cb104-18" data-line-number="18">  <span class="co"># Store the results in a dataframe</span></a>
<a class="sourceLine" id="cb104-19" data-line-number="19">  validation[i, <span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">rmse</span>(test<span class="op">$</span>OCSKGMlog, test<span class="op">$</span>pred)</a>
<a class="sourceLine" id="cb104-20" data-line-number="20">  validation[i, <span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">cor</span>(test<span class="op">$</span>OCSKGMlog, test<span class="op">$</span>pred)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb104-21" data-line-number="21">}</a></code></pre></div>
<p><strong>Step 2 - Build a snsitivity map</strong></p>
<p>The validation plot shows the regression line between observed and predicted of a model that uses 75% (black line) and the regression lines of 10 model realizations, each with a different random combination of data for training and validating the models.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" data-line-number="1"><span class="co"># The sensitivity map is the dispersion of all individual models</span></a>
<a class="sourceLine" id="cb105-2" data-line-number="2">sensitivity &lt;-<span class="st"> </span><span class="kw">calc</span>(pred[[<span class="op">-</span><span class="dv">1</span>]], sd)</a>
<a class="sourceLine" id="cb105-3" data-line-number="3"></a>
<a class="sourceLine" id="cb105-4" data-line-number="4"><span class="kw">plot</span>(sensitivity, <span class="dt">col=</span><span class="kw">rev</span>(<span class="kw">topo.colors</span>(<span class="dv">10</span>)), </a>
<a class="sourceLine" id="cb105-5" data-line-number="5">     <span class="dt">main=</span><span class="st">&#39;Sensitivity based on 10 realizations using 25% samples&#39;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-59"></span>
<img src="SOCMapping_files/figure-html/unnamed-chunk-59-1.png" alt="Sensitivity based on 10 realizations using 25% samples" width="672" />
<p class="caption">
Figure 6.17: Sensitivity based on 10 realizations using 25% samples
</p>
</div>
<p>The RMSE and the explained variance of the models is stored in the object validation (type <code>summary(validation)</code>). The standard deviation of all the ten predictions allows generating a map of model sensitivity to available data.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" data-line-number="1"><span class="co"># Sensitivity of validation metrics</span></a>
<a class="sourceLine" id="cb106-2" data-line-number="2"><span class="kw">summary</span>(validation)</a></code></pre></div>
<pre><code>##       rmse              r2        
##  Min.   :0.4745   Min.   :0.2150  
##  1st Qu.:0.4771   1st Qu.:0.2296  
##  Median :0.4796   Median :0.2338  
##  Mean   :0.4806   Mean   :0.2359  
##  3rd Qu.:0.4852   3rd Qu.:0.2405  
##  Max.   :0.4884   Max.   :0.2571</code></pre>
<p><strong>Step 3 - Analyze the sensitivity of the map</strong></p>
<p><strong>Step 4 - Estimate the full conditional distribution of OCSKGMlog</strong></p>
<p>Finally, we will estimate the model uncertainty, represented by the full conditional distribution of the response variable (<em>OCSKGMlog</em>) as a function of the selected prediction factors using the quantile regression forest package <strong>quantregForest</strong> of <strong>R</strong>. This approach has proven to be efficient for DSM across large areas <span class="citation">(Vaysse and Lagacherie <a href="#ref-vaysse2017using">2017</a>)</span>.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1"><span class="co"># Use quantile regression forest to estimate the full conditional </span></a>
<a class="sourceLine" id="cb108-2" data-line-number="2"><span class="co"># distribution of OCSKGMlog, note that we are using the mtry </span></a>
<a class="sourceLine" id="cb108-3" data-line-number="3"><span class="co"># parameter that was selected by the train function of the caret </span></a>
<a class="sourceLine" id="cb108-4" data-line-number="4"><span class="co"># package, assuming that the 75% of data previously used well </span></a>
<a class="sourceLine" id="cb108-5" data-line-number="5"><span class="co"># resembles the statistical distribution of the entire data </span></a>
<a class="sourceLine" id="cb108-6" data-line-number="6"><span class="co"># population. Otherwise, repeat the train function with all </span></a>
<a class="sourceLine" id="cb108-7" data-line-number="7"><span class="co"># available data (using the object dat that instead of train) </span></a>
<a class="sourceLine" id="cb108-8" data-line-number="8"><span class="co"># to select mtry.</span></a>
<a class="sourceLine" id="cb108-9" data-line-number="9"><span class="kw">library</span>(quantregForest)</a></code></pre></div>
<pre><code>## Loading required package: RColorBrewer</code></pre>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" data-line-number="1">model &lt;-<span class="st"> </span><span class="kw">quantregForest</span>(<span class="dt">y=</span>dat<span class="op">@</span>data<span class="op">$</span>OCSKGMlog, <span class="dt">x=</span>dat<span class="op">@</span>data[,<span class="dv">8</span><span class="op">:</span><span class="dv">20</span>], </a>
<a class="sourceLine" id="cb110-2" data-line-number="2">                        <span class="dt">ntree=</span><span class="dv">500</span>, <span class="dt">keep.inbag=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb110-3" data-line-number="3">                        <span class="dt">mtry =</span> <span class="kw">as.numeric</span>(rfmodel<span class="op">$</span>bestTune))                        </a></code></pre></div>
<p><strong>Step 5 - Estimate the probability distribution function for each pixel</strong></p>
<p>This method will calculate a probability distribution function for each pixel and therefore can be time-consuming. Therefore we will run it using parallel computing. Note that the code to run in parallel this analysis can also be passed to the previous predictions (predict function). The result will be a map of the standard deviation of the distribution calculated for each pixel, which represents the extreme values that a prediction can take for a specific site (e.g., pixel) given available data and predictors. Note that this analysis is performed using all available data and a second map of OCSKGM is created.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" data-line-number="1"><span class="kw">library</span>(snow)</a>
<a class="sourceLine" id="cb111-2" data-line-number="2"></a>
<a class="sourceLine" id="cb111-3" data-line-number="3"><span class="co"># Estimate model uncertainty at the pixel level using parallel </span></a>
<a class="sourceLine" id="cb111-4" data-line-number="4"><span class="co"># computing</span></a>
<a class="sourceLine" id="cb111-5" data-line-number="5"><span class="co"># Define number of cores to use</span></a>
<a class="sourceLine" id="cb111-6" data-line-number="6"><span class="kw">beginCluster</span>()</a></code></pre></div>
<pre><code>## 16 cores detected, using 15</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1"><span class="co"># Estimate model uncertainty</span></a>
<a class="sourceLine" id="cb113-2" data-line-number="2">unc &lt;-<span class="st"> </span><span class="kw">clusterR</span>(covs, predict, <span class="dt">args=</span><span class="kw">list</span>(<span class="dt">model=</span>model,<span class="dt">what=</span>sd))</a>
<a class="sourceLine" id="cb113-3" data-line-number="3"></a>
<a class="sourceLine" id="cb113-4" data-line-number="4"><span class="co"># OCSKGMlog prediction based in all available data</span></a>
<a class="sourceLine" id="cb113-5" data-line-number="5">mean &lt;-<span class="st"> </span><span class="kw">clusterR</span>(covs, predict, </a>
<a class="sourceLine" id="cb113-6" data-line-number="6">                 <span class="dt">args=</span><span class="kw">list</span>(<span class="dt">model=</span>model, <span class="dt">what=</span>mean))</a>
<a class="sourceLine" id="cb113-7" data-line-number="7"></a>
<a class="sourceLine" id="cb113-8" data-line-number="8"><span class="co"># The total uncertainty is the sum of sensitivity and model </span></a>
<a class="sourceLine" id="cb113-9" data-line-number="9"><span class="co"># uncertainty</span></a>
<a class="sourceLine" id="cb113-10" data-line-number="10">unc &lt;-<span class="st"> </span>unc <span class="op">+</span><span class="st"> </span>sensitivity</a>
<a class="sourceLine" id="cb113-11" data-line-number="11"></a>
<a class="sourceLine" id="cb113-12" data-line-number="12"><span class="co"># Express the uncertainty in percent % (divide by the mean)</span></a>
<a class="sourceLine" id="cb113-13" data-line-number="13">Total_unc_Percent &lt;-<span class="st"> </span><span class="kw">exp</span>(unc)<span class="op">/</span><span class="kw">exp</span>(mean)</a>
<a class="sourceLine" id="cb113-14" data-line-number="14"><span class="kw">endCluster</span>()</a></code></pre></div>
<p>Our final prediction uses all available data, while the total uncertainty (in percent) is represented by the sum of the quantile regression forest standard deviation and the sensitivity map from the previous Section. The total uncertainty is then divided by the prediction to obtain a percent map, which is easier to interpret.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" data-line-number="1"><span class="co"># Plot both maps (the predicted OCSKGM and associated uncertainty)</span></a>
<a class="sourceLine" id="cb114-2" data-line-number="2"><span class="kw">plot</span>(<span class="kw">exp</span>(mean), <span class="dt">main=</span><span class="st">&#39;OCSKGM based in all data&#39;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-62"></span>
<img src="SOCMapping_files/figure-html/unnamed-chunk-62-1.png" alt="OCSKGM quantregForest prediction" width="672" />
<p class="caption">
Figure 6.18: OCSKGM quantregForest prediction
</p>
</div>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1"><span class="kw">plot</span>(Total_unc_Percent, <span class="dt">zlim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="dt">main=</span><span class="st">&#39;Total uncertainty&#39;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-63"></span>
<img src="SOCMapping_files/figure-html/unnamed-chunk-63-1.png" alt="Total uncertainty" width="672" />
<p class="caption">
Figure 6.19: Total uncertainty
</p>
</div>
<p><strong>Step 6 - Save the results as raster format</strong></p>
<p>Finally, the predicted OCSKGM and the total uncertainty can be saved in the working directory in a generic (*.tif) raster format.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1"><span class="co"># Save the resulting maps in separated *.tif files</span></a>
<a class="sourceLine" id="cb116-2" data-line-number="2"><span class="kw">writeRaster</span>(<span class="kw">exp</span>(mean), <span class="dt">file=</span><span class="st">&#39;results/MKD_OCSKGM_quantrf.tif&#39;</span>, </a>
<a class="sourceLine" id="cb116-3" data-line-number="3">            <span class="dt">overwrite=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb116-4" data-line-number="4"><span class="kw">writeRaster</span>(Total_unc_Percent, <span class="dt">file=</span><span class="st">&#39;results/MKD_OCSKGM_rf_unc.tif&#39;</span>,</a>
<a class="sourceLine" id="cb116-5" data-line-number="5">            <span class="dt">overwrite=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<p>We have created two maps in the working directory, one represents the predicted OCSKGM and the second one its uncertainty, which is the sum of the model sensitivity to data variations and the full conditional distribution of the response variable as a function of available prediction factors. The following Chapters of this book will show you how to prepare a stock report based on this soil carbon digital soil maps.</p>


</div>
</div>
<div id="svm" class="section level2">
<h2><span class="header-section-number">6.5</span> Data mining: support vector machines</h2>
<p><em>G.F. Olmedo &amp; M. Guevara</em></p>
<div id="overview-3" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Overview</h3>
<p>Support vector machines is a kernel-based machine learning technique suitable for mapping SOC. SVM use decision surfaces (defined by a kernel function) to map non-linear relationships across a high-dimension induced feature space <span class="citation">(Cortes and Vapnik <a href="#ref-cortes1995support">1995</a>)</span>. SVM is widely used to perform classification and regression analysis on DSM.</p>
<p>According to <span class="citation">Pedregosa et al. (<a href="#ref-scikit">2011</a>)</span> the advantages of SVM are:</p>
<ul>
<li>Effective in high dimensional spaces.</li>
<li>Still effective in cases where the number of dimensions is greater than the number of samples.</li>
<li>Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.</li>
<li>Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.</li>
</ul>
<p>And the disadvantages of SVM include:</p>
<ul>
<li>If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.</li>
<li>SVM do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.</li>
</ul>
<p>In DSM, the problems usually involve working in high dimensional spaces (were the dimensions are the covariates) with a limited number of samples. SVM is a technique mostly used in classification problems, but it can be used to solve regression problems, such as modeling the continuous variability of SOC using environmental covariates. When SVM is used to solve a regression problem, it is called support vector regression.</p>
<p>Support vector regression applies a simple linear method to the data but in a high-dimensional feature space non-linearly related to the input space. It creates <span class="math inline">\(n\)</span> hyperplanes through the <span class="math inline">\(n\)</span>-dimensional spectral-space and each hyperplanes separates numerical data based on a Kernel function (e.g., Gaussian). SVM uses parameters such as <em>gamma</em>, <em>cost</em> and <em>epsilon</em>. These parameters are used to define the shape of the hyperplane, including the margin from the closest point to the hyperplane that divides data with the largest possible margin and defines the tolerance to errors on each single training. Linear models are fitted to the support vectors and used for prediction purposes. The support vectors are the points which fall within each hyperplane <span class="citation">(Guevara et al. <a href="#ref-guevara_2018">2018</a>)</span>.</p>
<p>In the example below, we will use the implementation of SVM in the R package <code>e1071</code> <span class="citation">(Meyer et al. <a href="#ref-e1071">2017</a>)</span>. The package <code>e1071</code> offers an interface to the award-winning C++ implementation by Chih-Chung Chang and Chih-Jen Lin, libsvm (current version: 2.6). For further implementation details on libsvm, see <span class="citation">Chang and Lin (<a href="#ref-chang2001libsvm">2001</a>)</span>.</p>
<blockquote>
<p><strong>SVM</strong>: This approach is a broad research area and for a better understanding of the mathematical background we can recommend the following books: <span class="citation">Vapnik (<a href="#ref-vapnik2013nature">2013</a>)</span>, <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span>, and <span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span>.</p>
</blockquote>
</div>
<div id="technical-steps---fitting-an-svm-model-to-predict-the-soc" class="section level3">
<h3><span class="header-section-number">6.5.2</span> Technical steps - fitting an SVM model to predict the SOC</h3>
<p><strong>Step 1 - Setting working space and initial steps</strong></p>
<p>One of the first steps should be setting our working directory. If you read/write files from/to disk, this takes place in the working directory. If we do not set the working directory, we could easily write files to an undesirable file location. The following example shows how to set the working directory in <strong>R</strong> to our folder which contains data for the study area (point data, covariates).</p>
<p>Note that we must use the forward slash / or double backslash \\ in <strong>R</strong>! Single backslash \ will not work. Now we can check if the working directory has been correctly set by using the <code>getwd()</code> function:</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1"><span class="kw">getwd</span>()</a></code></pre></div>
<p><strong>Step 2 - Data preparation</strong></p>
<p><strong>Point dataset</strong></p>
<p>We previously applied spline function to produce continuous soil information to a given soil depth (0 cm - 30 cm) in the Section <a href="preparation.html#EqualAreaSplines">4.4.2</a>. Spline function basically imports soil profile data (including instances where layers are not contiguous), fits it to a mass-preserving spline and outputs attribute means for a given depth. The output file should contain the profile ID <code>id</code>, upper (surface) and lower depth (30 cm), estimated value for the selected soil attribute (Value) and tmse (estimated mean squared error of the spline).</p>
<p>If you used the Spline Tool V2, the coordinates were not kept in the output file. The coordinates should be added back in the data table. You can use profile IDs to add the <code>X</code> and <code>Y</code> columns back. Once your point dataset is ready, copy this table into your working directory as a .csv file.</p>
<p><strong>Environmental predictors (covariates)</strong></p>
<p>In the Chapter <a href="covariates.html#covariates">5</a>, we presented and prepared several global and continental datasets. In addition to these datasets, numerous covariate layers have been prepared by ISRIC for the GSOCmap project. These are GIS raster layers of various biophysical earth surface properties for each country in the world. Some of these layers will be used as predictors in this Section. Please download the covariates for your own study area from GSOCmap Data Repository as explained in Section <a href="covariates.html#GSOCDataRepo">5.6</a>.</p>
<p>In Section <a href="covariates.html#overlay-soil-covariates">5.9</a>, a table with the points values after data preparation and the values of our spatial predictors was prepared. This step involves loading this table.</p>
<p>Now we will import our point dataset using <code>read.csv()</code> function. The easiest way to create a data frame is to read in data from a file. This is done using the function <code>read.csv()</code>, which works with comma delimited files. Data can be read in from other file formats as well, using different functions, but <code>read.csv()</code> is the most commonly used approach. <strong>R</strong> is very flexible in how it reads in data from text files (<code>read.table()</code>, <code>read.csv()</code>, <code>read.csv2()</code>, <code>read.delim()</code>, <code>read.delim2()</code>). Please type <code>?read.table()</code> for help.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1"><span class="co"># Load data</span></a>
<a class="sourceLine" id="cb118-2" data-line-number="2">dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/MKD_RegMatrix.csv&quot;</span>)</a>
<a class="sourceLine" id="cb118-3" data-line-number="3"></a>
<a class="sourceLine" id="cb118-4" data-line-number="4">dat<span class="op">$</span>LCEE10 &lt;-<span class="st"> </span><span class="kw">as.factor</span>(dat<span class="op">$</span>LCEE10)</a>
<a class="sourceLine" id="cb118-5" data-line-number="5">dat<span class="op">$</span>soilmap &lt;-<span class="st"> </span><span class="kw">as.factor</span>(dat<span class="op">$</span>soilmap)</a>
<a class="sourceLine" id="cb118-6" data-line-number="6"></a>
<a class="sourceLine" id="cb118-7" data-line-number="7"><span class="co"># Explore the data structure</span></a>
<a class="sourceLine" id="cb118-8" data-line-number="8"><span class="kw">str</span>(dat)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    2897 obs. of  23 variables:
##  $ id       : Factor w/ 2897 levels &quot;P0003&quot;,&quot;P0007&quot;,..: 1 2 3 4..
##  $ Y        : num  42 42 42.1 42 42 ...
##  $ X        : num  20.8 20.8 20.8 20.9 20.9 ...
##  $ SOC      : num  26.38 6.15 3.94 3.26 2.29 ...
##  $ BLD      : num  0.73 1.17 1.3 1.34 1.41 ...
##  $ CRFVOL   : num  8 18.6 31.9 21.7 14.5 ...
##  $ OCSKGM   : num  5.32 1.75 1.04 1.03 0.83 ...
##  $ meaERROR : num  2.16 2.85 2.65 3.16 3.63 2.83 2.94 2.49 2.77..
##  $ OCSKGMlog: num  1.6712 0.5591 0.0429 0.0286 -0.1862 ...
##  $ B04CHE3  : num  574 553 693 743 744 ...
##  $ B07CHE3  : num  38.5 37.8 42.1 43.7 43.7 ...
##  $ B13CHE3  : num  111.6 125 99.8 118.1 121 ...
##  $ B14CHE3  : num  59.2 60.3 42.4 39.9 38.7 ...
##  $ DEMENV5  : int  2327 2207 1243 1120 1098 1492 1413 1809 1731..
##  $ LCEE10   : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 3 2 1 2 2 2..
##  $ PRSCHE3  : num  998 1053 780 839 844 ...
##  $ SLPMRG5  : int  13 36 6 25 30 24 15 17 20 43 ...
##  $ TMDMOD3  : int  282 280 285 288 289 287 286 286 287 286 ...
##  $ TMNMOD3  : int  272 270 277 279 279 277 277 273 274 273 ...
##  $ TWIMRG5  : int  61 62 81 66 65 72 68 67 65 59 ...
##  $ VBFMRG5  : int  0 0 14 0 0 0 0 0 0 0 ...
##  $ VDPMRG5  : int  311 823 10048 1963 -173 -400 -9 -692 -1139 2..
##  $ soilmap  : Factor w/ 20 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 6 14 14 3..</code></pre>
<p>Since we will be working with spatial data we need to define the coordinates for the imported data. Using the <code>coordinates()</code> function from the <strong>sp</strong> package we can define the columns in the data frame to refer to spatial coordinates. Here the coordinates are listed in columns <code>X</code> and <code>Y</code>.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1"><span class="kw">library</span>(sp)</a>
<a class="sourceLine" id="cb120-2" data-line-number="2"></a>
<a class="sourceLine" id="cb120-3" data-line-number="3"><span class="co"># Promote to spatialPointsDataFrame</span></a>
<a class="sourceLine" id="cb120-4" data-line-number="4"><span class="kw">coordinates</span>(dat) &lt;-<span class="st"> </span><span class="er">~</span><span class="st"> </span>X <span class="op">+</span><span class="st"> </span>Y</a>
<a class="sourceLine" id="cb120-5" data-line-number="5"></a>
<a class="sourceLine" id="cb120-6" data-line-number="6"><span class="kw">class</span>(dat)</a></code></pre></div>
<pre><code>## [1] &quot;SpatialPointsDataFrame&quot;
## attr(,&quot;package&quot;)
## [1] &quot;sp&quot;</code></pre>
<p><code>SpatialPointsDataFrame</code> structure is essentially the same as a data frame, except that additional <em>spatial</em> elements have been added or partitioned into slots. Some important ones being the bounding box (sort of like the spatial extent of the data), and the coordinate reference system <code>proj4string()</code>, which we need to define for the sample dataset. To define the CRS, we must know where our data are from, and what was the corresponding CRS used when recording the spatial information in the field. For this data set, the CRS used was WGS84 (EPSG:4326).</p>
<p>To clearly tell <strong>R</strong> this information we define the CRS which describes a reference system in a way understood by the <a href="http://trac.osgeo.org/proj/">PROJ.4</a> projection library. An interface to the PROJ.4 library is available in the <strong>rgdal</strong> package. As an alternative to using PROJ.4 character strings, we can use the corresponding yet simpler EPSG code. <strong>rgdal</strong> also recognizes these codes. If you are unsure of the PROJ.4 or EPSG code for the spatial data that you have but know the CRS, you should consult <a href="http://spatialreference.org/" class="uri">http://spatialreference.org/</a> for assistance.</p>
<blockquote>
<p><strong>CRS</strong>: Please note that, when working with spatial data, it is very important that the CRS of the point data and covariates are the same.</p>
</blockquote>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1"><span class="co"># Now, we will define our CRS</span></a>
<a class="sourceLine" id="cb122-2" data-line-number="2">dat<span class="op">@</span>proj4string &lt;-<span class="st"> </span><span class="kw">CRS</span>(<span class="dt">projargs =</span> <span class="st">&quot;+init=epsg:4326&quot;</span>)</a>
<a class="sourceLine" id="cb122-3" data-line-number="3"></a>
<a class="sourceLine" id="cb122-4" data-line-number="4">dat<span class="op">@</span>proj4string</a></code></pre></div>
<pre><code>## CRS arguments:
##  +init=epsg:4326 +proj=longlat +datum=WGS84 +no_defs
## +ellps=WGS84 +towgs84=0,0,0</code></pre>
<p>Now we will import the covariates. When the covariate layers are in common resolution and extent, rather than working with individual rasters it is better to stack them all into a single <strong>R</strong> object. In this example, we use 13 covariates from the GSOCmap Data Repository and a rasterized version of the soil type map. The rasterization of vectorial data was covered in <a href="covariates.html#technical-steps---rasterizing-a-vector-layer-in-r">Technical Steps - Rasterizing a vector layer in R</a>. The file containing all the covariates was prepared at the end of Chapter <a href="covariates.html#covariates">5</a>.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1"><span class="kw">load</span>(<span class="dt">file =</span> <span class="st">&quot;covariates.RData&quot;</span>)</a>
<a class="sourceLine" id="cb124-2" data-line-number="2"></a>
<a class="sourceLine" id="cb124-3" data-line-number="3"><span class="kw">names</span>(covs)</a></code></pre></div>
<pre><code>##  [1] &quot;B04CHE3&quot; &quot;B07CHE3&quot; &quot;B13CHE3&quot; &quot;B14CHE3&quot; &quot;DEMENV5&quot; &quot;LCEE10&quot; 
##  [7] &quot;PRSCHE3&quot; &quot;SLPMRG5&quot; &quot;TMDMOD3&quot; &quot;TMNMOD3&quot; &quot;TWIMRG5&quot; &quot;VBFMRG5&quot;
## [13] &quot;VDPMRG5&quot; &quot;soilmap&quot;</code></pre>
<p><strong>Step 3 - Variable selection using correlation analysis</strong></p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" data-line-number="1"><span class="co"># Plot the names of the covariates</span></a>
<a class="sourceLine" id="cb126-2" data-line-number="2"><span class="kw">names</span>(dat<span class="op">@</span>data)</a></code></pre></div>
<pre><code>##  [1] &quot;id&quot;        &quot;SOC&quot;       &quot;BLD&quot;       &quot;CRFVOL&quot;    &quot;OCSKGM&quot;   
##  [6] &quot;meaERROR&quot;  &quot;OCSKGMlog&quot; &quot;B04CHE3&quot;   &quot;B07CHE3&quot;   &quot;B13CHE3&quot;  
## [11] &quot;B14CHE3&quot;   &quot;DEMENV5&quot;   &quot;LCEE10&quot;    &quot;PRSCHE3&quot;   &quot;SLPMRG5&quot;  
## [16] &quot;TMDMOD3&quot;   &quot;TMNMOD3&quot;   &quot;TWIMRG5&quot;   &quot;VBFMRG5&quot;   &quot;VDPMRG5&quot;  
## [21] &quot;soilmap&quot;</code></pre>
<p>For the variable selection we will use <code>cor()</code> function. <code>x</code> must be a table including only the column with the response variable, and <code>y</code> must be a table including <strong>only</strong> the covariates. Besides, remember <code>dat@data</code> in the <code>data.frame</code> included in the <code>SpatialPointsDataFrame</code>. For <code>y</code>, columns 1 to 7 are out, because they are not covariates. At the same time, correlation analysis cannot be applied to categorical covariates, this means that columns 13 and 21 have to be removed too.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1">selectedCovs &lt;-<span class="st"> </span><span class="kw">cor</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(dat<span class="op">@</span>data[,<span class="dv">5</span>]),</a>
<a class="sourceLine" id="cb128-2" data-line-number="2">           <span class="dt">y =</span> <span class="kw">as.matrix</span>(dat<span class="op">@</span>data[,<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>,<span class="dv">13</span>,<span class="dv">21</span>)]))</a>
<a class="sourceLine" id="cb128-3" data-line-number="3"></a>
<a class="sourceLine" id="cb128-4" data-line-number="4"><span class="co"># Print correlation results</span></a>
<a class="sourceLine" id="cb128-5" data-line-number="5">selectedCovs</a></code></pre></div>
<pre><code>##         B04CHE3    B07CHE3  B13CHE3   B14CHE3   DEMENV5
## [1,] -0.4199537 -0.3926615 0.330696 0.3481847 0.3926275
##        PRSCHE3   SLPMRG5    TMDMOD3    TMNMOD3    TWIMRG5
## [1,] 0.3948779 0.2593964 -0.4077552 -0.2963631 -0.2525764
##         VBFMRG5    VDPMRG5
## [1,] -0.1156285 -0.3001934</code></pre>
<p>Now we used the correlation results to select the top five covariates.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" data-line-number="1"><span class="kw">library</span>(reshape)</a>
<a class="sourceLine" id="cb130-2" data-line-number="2"></a>
<a class="sourceLine" id="cb130-3" data-line-number="3">x &lt;-<span class="st"> </span><span class="kw">subset</span>(<span class="kw">melt</span>(selectedCovs), value <span class="op">!=</span><span class="st"> </span><span class="dv">1</span> <span class="op">|</span><span class="st"> </span>value <span class="op">!=</span><span class="st"> </span><span class="ot">NA</span>)</a>
<a class="sourceLine" id="cb130-4" data-line-number="4">x &lt;-<span class="st"> </span>x[<span class="kw">with</span>(x, <span class="kw">order</span>(<span class="op">-</span><span class="kw">abs</span>(x<span class="op">$</span>value))),]</a>
<a class="sourceLine" id="cb130-5" data-line-number="5"></a>
<a class="sourceLine" id="cb130-6" data-line-number="6">idx &lt;-<span class="st"> </span><span class="kw">as.character</span>(x<span class="op">$</span>X2[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>])</a>
<a class="sourceLine" id="cb130-7" data-line-number="7"></a>
<a class="sourceLine" id="cb130-8" data-line-number="8">dat2 &lt;-<span class="st"> </span>dat[<span class="kw">c</span>(<span class="st">&#39;OCSKGM&#39;</span>, idx)]</a>
<a class="sourceLine" id="cb130-9" data-line-number="9"><span class="kw">names</span>(dat2)</a></code></pre></div>
<pre><code>## [1] &quot;OCSKGM&quot;  &quot;B04CHE3&quot; &quot;TMDMOD3&quot; &quot;PRSCHE3&quot; &quot;B07CHE3&quot; &quot;DEMENV5&quot;</code></pre>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" data-line-number="1">COV &lt;-<span class="st"> </span>covs[[idx]]</a>
<a class="sourceLine" id="cb132-2" data-line-number="2"></a>
<a class="sourceLine" id="cb132-3" data-line-number="3"><span class="co"># Selected covariates</span></a>
<a class="sourceLine" id="cb132-4" data-line-number="4"><span class="kw">names</span>(COV)</a></code></pre></div>
<pre><code>## [1] &quot;B04CHE3&quot; &quot;TMDMOD3&quot; &quot;PRSCHE3&quot; &quot;B07CHE3&quot; &quot;DEMENV5&quot;</code></pre>
<p><strong>Step 4 - Categorical variables in svm models</strong></p>
<p>According to <span class="citation">Hsu et al. (<a href="#ref-hsu2003practical">2003</a>)</span>, SVM requires each variable to be represented by a vector of real numbers. This means that factor variables, like <code>covs$LCEE10</code> and <code>covs$soilmap</code>has to be converted into numeric data. In statistics, this kind of variables are called Boolean indicators or <em>dummy variables</em>.</p>
<p>Dummy variables take a value of 0 or 1 indicating the presence or absence of a specific value/category in our factor covariate, i.e. if we have five categories like in <code>covs$LCEE10</code>, we will have five dummy variables indicating the presence/absence of every category. For converting our covariates to dummies we will have to create a new function that returns the dummy raster stack <code>dummyRaster</code> from the factor version of the raster layer.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" data-line-number="1">dummyRaster &lt;-<span class="st"> </span><span class="cf">function</span>(rast){</a>
<a class="sourceLine" id="cb134-2" data-line-number="2">  rast &lt;-<span class="st"> </span><span class="kw">as.factor</span>(rast)</a>
<a class="sourceLine" id="cb134-3" data-line-number="3">  result &lt;-<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb134-4" data-line-number="4">  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(<span class="kw">levels</span>(rast)[[<span class="dv">1</span>]][[<span class="dv">1</span>]])){</a>
<a class="sourceLine" id="cb134-5" data-line-number="5">    result[[i]] &lt;-<span class="st"> </span>rast <span class="op">==</span><span class="st"> </span><span class="kw">levels</span>(rast)[[<span class="dv">1</span>]][[<span class="dv">1</span>]][i]</a>
<a class="sourceLine" id="cb134-6" data-line-number="6">    <span class="kw">names</span>(result[[i]]) &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">names</span>(rast), </a>
<a class="sourceLine" id="cb134-7" data-line-number="7">                                 <span class="kw">levels</span>(rast)[[<span class="dv">1</span>]][[<span class="dv">1</span>]][i])</a>
<a class="sourceLine" id="cb134-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb134-9" data-line-number="9">  <span class="kw">return</span>(<span class="kw">stack</span>(result))</a>
<a class="sourceLine" id="cb134-10" data-line-number="10">}</a></code></pre></div>
<p>We can use the function we just created to convert our categorical covariates to dummies and then stack all the layers together.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb135-1" data-line-number="1"><span class="co"># Convert soilmap from factor to dummy</span></a>
<a class="sourceLine" id="cb135-2" data-line-number="2">soilmap_dummy &lt;-<span class="st"> </span><span class="kw">dummyRaster</span>(covs<span class="op">$</span>soilmap)</a>
<a class="sourceLine" id="cb135-3" data-line-number="3"></a>
<a class="sourceLine" id="cb135-4" data-line-number="4"><span class="co"># Convert LCEE10 from factor to dummy</span></a>
<a class="sourceLine" id="cb135-5" data-line-number="5">LCEE10_dummy &lt;-<span class="st"> </span><span class="kw">dummyRaster</span>(covs<span class="op">$</span>LCEE10)</a>
<a class="sourceLine" id="cb135-6" data-line-number="6"></a>
<a class="sourceLine" id="cb135-7" data-line-number="7"><span class="co"># Stack the 5 COV layers with the 2 dummies</span></a>
<a class="sourceLine" id="cb135-8" data-line-number="8">COV &lt;-<span class="st"> </span><span class="kw">stack</span>(COV, soilmap_dummy, LCEE10_dummy)</a>
<a class="sourceLine" id="cb135-9" data-line-number="9"></a>
<a class="sourceLine" id="cb135-10" data-line-number="10"><span class="co"># Print the final layer names</span></a>
<a class="sourceLine" id="cb135-11" data-line-number="11"><span class="kw">names</span>(COV)</a></code></pre></div>
<pre><code>##  [1] &quot;B04CHE3&quot;   &quot;TMDMOD3&quot;   &quot;PRSCHE3&quot;   &quot;B07CHE3&quot;   &quot;DEMENV5&quot;  
##  [6] &quot;soilmap1&quot;  &quot;soilmap2&quot;  &quot;soilmap3&quot;  &quot;soilmap4&quot;  &quot;soilmap5&quot; 
## [11] &quot;soilmap6&quot;  &quot;soilmap7&quot;  &quot;soilmap8&quot;  &quot;soilmap9&quot;  &quot;soilmap10&quot;
## [16] &quot;soilmap11&quot; &quot;soilmap12&quot; &quot;soilmap13&quot; &quot;soilmap14&quot; &quot;soilmap15&quot;
## [21] &quot;soilmap16&quot; &quot;soilmap17&quot; &quot;soilmap18&quot; &quot;soilmap19&quot; &quot;soilmap20&quot;
## [26] &quot;LCEE101&quot;   &quot;LCEE102&quot;   &quot;LCEE103&quot;   &quot;LCEE104&quot;</code></pre>
<p>We have to convert the columns with categorical variables in the soil samples <code>data.frame</code> to dummies as well. For doing this we can use function <code>model.matrix()</code>. After this, we use <code>cbind()</code> to merge the resulting <code>data.frame</code>.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb137-1" data-line-number="1"><span class="co"># Convert soilmap column to dummy, the result is a matrix</span></a>
<a class="sourceLine" id="cb137-2" data-line-number="2"><span class="co"># To have one column per category we have to add -1 to the formula</span></a>
<a class="sourceLine" id="cb137-3" data-line-number="3">dat_soilmap_dummy &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>soilmap <span class="dv">-1</span>, <span class="dt">data =</span> dat<span class="op">@</span>data)</a>
<a class="sourceLine" id="cb137-4" data-line-number="4"></a>
<a class="sourceLine" id="cb137-5" data-line-number="5"><span class="co"># Convert the matrix to a data.frame</span></a>
<a class="sourceLine" id="cb137-6" data-line-number="6">dat_soilmap_dummy &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(dat_soilmap_dummy)</a>
<a class="sourceLine" id="cb137-7" data-line-number="7"></a>
<a class="sourceLine" id="cb137-8" data-line-number="8"><span class="co"># Convert LCEE10 column to dummy, the result is a matrix</span></a>
<a class="sourceLine" id="cb137-9" data-line-number="9"><span class="co"># To have one column per category we have to add -1 to the formula</span></a>
<a class="sourceLine" id="cb137-10" data-line-number="10">dat_LCEE10_dummy &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>LCEE10 <span class="dv">-1</span>, <span class="dt">data =</span> dat<span class="op">@</span>data)</a>
<a class="sourceLine" id="cb137-11" data-line-number="11"></a>
<a class="sourceLine" id="cb137-12" data-line-number="12"><span class="co"># Convert the matrix to a data.frame</span></a>
<a class="sourceLine" id="cb137-13" data-line-number="13">dat_LCEE10_dummy &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(dat_LCEE10_dummy)</a>
<a class="sourceLine" id="cb137-14" data-line-number="14"></a>
<a class="sourceLine" id="cb137-15" data-line-number="15">dat<span class="op">@</span>data &lt;-<span class="st"> </span><span class="kw">cbind</span>(dat<span class="op">@</span>data, dat_LCEE10_dummy, dat_soilmap_dummy)</a>
<a class="sourceLine" id="cb137-16" data-line-number="16"></a>
<a class="sourceLine" id="cb137-17" data-line-number="17"><span class="kw">names</span>(dat<span class="op">@</span>data)</a></code></pre></div>
<pre><code>##  [1] &quot;id&quot;        &quot;SOC&quot;       &quot;BLD&quot;       &quot;CRFVOL&quot;    &quot;OCSKGM&quot;   
##  [6] &quot;meaERROR&quot;  &quot;OCSKGMlog&quot; &quot;B04CHE3&quot;   &quot;B07CHE3&quot;   &quot;B13CHE3&quot;  
## [11] &quot;B14CHE3&quot;   &quot;DEMENV5&quot;   &quot;LCEE10&quot;    &quot;PRSCHE3&quot;   &quot;SLPMRG5&quot;  
## [16] &quot;TMDMOD3&quot;   &quot;TMNMOD3&quot;   &quot;TWIMRG5&quot;   &quot;VBFMRG5&quot;   &quot;VDPMRG5&quot;  
## [21] &quot;soilmap&quot;   &quot;LCEE101&quot;   &quot;LCEE102&quot;   &quot;LCEE103&quot;   &quot;LCEE104&quot;  
## [26] &quot;soilmap1&quot;  &quot;soilmap2&quot;  &quot;soilmap3&quot;  &quot;soilmap4&quot;  &quot;soilmap5&quot; 
## [31] &quot;soilmap6&quot;  &quot;soilmap7&quot;  &quot;soilmap8&quot;  &quot;soilmap9&quot;  &quot;soilmap10&quot;
## [36] &quot;soilmap11&quot; &quot;soilmap12&quot; &quot;soilmap13&quot; &quot;soilmap14&quot; &quot;soilmap15&quot;
## [41] &quot;soilmap16&quot; &quot;soilmap17&quot; &quot;soilmap18&quot; &quot;soilmap19&quot; &quot;soilmap20&quot;</code></pre>
<p><strong>Step 5 - Fitting a SVM model</strong></p>
<p>To improve the model performance, the parameters of the SVM can be tuned. In this example, we will show how to tune two parameters using a grid search for hyperparameter optimization using the function <code>tune()</code>.</p>
<p>The first parameter is <em>epsilon</em> which is the insensitive-loss function. The larger <em>epsilon</em> is, the larger errors in the solution are not penalized. The default value for <em>epsilon</em> is 0.1, and we will try 11 different values from 0.05 to 0.12 in 0.1 increments. The second parameter is the cost which is the cost of constraints violation – it is the ‘C’-constant of the regularization term in the Lagrange formulation. The default value for this parameter is 1, and we will try values from 1 to 20 in 5 increments. The value of cost helps us to avoid overfitting. This is a heavy and time consuming computational step since we will try a extensive number of different models in order to find the best parameters for our svm model.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb139-1" data-line-number="1"><span class="kw">library</span>(e1071)</a>
<a class="sourceLine" id="cb139-2" data-line-number="2"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb139-3" data-line-number="3"></a>
<a class="sourceLine" id="cb139-4" data-line-number="4"><span class="co"># Test different values of epsilon and cost</span></a>
<a class="sourceLine" id="cb139-5" data-line-number="5">  tuneResult &lt;-<span class="st"> </span><span class="kw">tune</span>(svm, OCSKGM <span class="op">~</span>.,  <span class="dt">data =</span> dat<span class="op">@</span>data[,<span class="kw">c</span>(<span class="st">&quot;OCSKGM&quot;</span>,</a>
<a class="sourceLine" id="cb139-6" data-line-number="6">                                                         <span class="kw">names</span>(COV))],</a>
<a class="sourceLine" id="cb139-7" data-line-number="7">                     <span class="dt">ranges =</span> <span class="kw">list</span>(<span class="dt">epsilon =</span> <span class="kw">seq</span>(<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.02</span>),</a>
<a class="sourceLine" id="cb139-8" data-line-number="8">                                   <span class="dt">cost =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">15</span>,<span class="dv">20</span>)))</a></code></pre></div>
<p>We can plot the performance of the different models. When the region is darker, the RMSE is closer to zero.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb140-1" data-line-number="1"><span class="kw">plot</span>(tuneResult)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-74"></span>
<img src="SOCMapping_files/figure-html/unnamed-chunk-74-1.png" alt="Performance of the different SVM models in the parameter tuning procedure" width="672" />
<p class="caption">
Figure 6.20: Performance of the different SVM models in the parameter tuning procedure
</p>
</div>
<p><strong>Step 6 - Select the model with the best combination of epsilon and cost</strong></p>
<p>The best model is the one with the lowest mean squared error derived by cross-validation. The parameters for the cross-validation can be defined in the <code>tune.control()</code> function. By default, it uses cross-validation using 10 folds.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" data-line-number="1"><span class="co"># Choose the model with the best combination of epsilon and cost</span></a>
<a class="sourceLine" id="cb141-2" data-line-number="2">tunedModel &lt;-<span class="st"> </span>tuneResult<span class="op">$</span>best.model</a>
<a class="sourceLine" id="cb141-3" data-line-number="3"></a>
<a class="sourceLine" id="cb141-4" data-line-number="4"><span class="kw">print</span>(tunedModel)</a></code></pre></div>
<pre><code>## 
## Call:
## best.tune(method = svm, train.x = OCSKGM ~ ., data = dat@data[, 
##     c(&quot;OCSKGM&quot;, names(COV))], ranges = list(epsilon = seq(0.1, 
##     0.2, 0.02), cost = c(5, 7, 15, 20)))
## 
## 
## Parameters:
##    SVM-Type:  eps-regression 
##  SVM-Kernel:  radial 
##        cost:  5 
##       gamma:  0.03448276 
##     epsilon:  0.2 
## 
## 
## Number of Support Vectors:  2193</code></pre>
<p><strong>Step 7 - Predict the OCS using the model</strong></p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" data-line-number="1"><span class="co"># Use the model to predict the SOC in the covariates space</span></a>
<a class="sourceLine" id="cb143-2" data-line-number="2">OCSsvm &lt;-<span class="st"> </span><span class="kw">predict</span>(COV, tunedModel)</a>
<a class="sourceLine" id="cb143-3" data-line-number="3"></a>
<a class="sourceLine" id="cb143-4" data-line-number="4"><span class="co"># Save the result</span></a>
<a class="sourceLine" id="cb143-5" data-line-number="5"><span class="kw">writeRaster</span>(OCSsvm, <span class="dt">filename =</span> <span class="st">&quot;results/MKD_OCSKGM_svm.tif&quot;</span>,</a>
<a class="sourceLine" id="cb143-6" data-line-number="6">            <span class="dt">overwrite=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb143-7" data-line-number="7"></a>
<a class="sourceLine" id="cb143-8" data-line-number="8"><span class="kw">plot</span>(OCSsvm)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-76"></span>
<img src="SOCMapping_files/figure-html/unnamed-chunk-76-1.png" alt="SOC prediction map for FYROM using a support vector machines model" width="672" />
<p class="caption">
Figure 6.21: SOC prediction map for FYROM using a support vector machines model
</p>
</div>
<p>Finally, we can evaluate the contribution of each covariate to the model <span class="citation">(Guyon and Elisseeff <a href="#ref-guyon2003introduction">2003</a>)</span>.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb144-1" data-line-number="1"><span class="co"># Variable importance in svm</span></a>
<a class="sourceLine" id="cb144-2" data-line-number="2"><span class="co"># Code by: stackoverflow.com/questions/34781495</span></a>
<a class="sourceLine" id="cb144-3" data-line-number="3"><span class="co"># Weight vectors</span></a>
<a class="sourceLine" id="cb144-4" data-line-number="4">w &lt;-<span class="st"> </span><span class="kw">t</span>(tunedModel<span class="op">$</span>coefs) <span class="op">%*%</span><span class="st"> </span>tunedModel<span class="op">$</span>SV</a>
<a class="sourceLine" id="cb144-5" data-line-number="5"></a>
<a class="sourceLine" id="cb144-6" data-line-number="6"><span class="co"># Weight</span></a>
<a class="sourceLine" id="cb144-7" data-line-number="7">w &lt;-<span class="st"> </span><span class="kw">apply</span>(w, <span class="dv">2</span>, <span class="cf">function</span>(v){<span class="kw">sqrt</span>(<span class="kw">sum</span>(v<span class="op">^</span><span class="dv">2</span>))})  </a>
<a class="sourceLine" id="cb144-8" data-line-number="8"></a>
<a class="sourceLine" id="cb144-9" data-line-number="9">w &lt;-<span class="st"> </span><span class="kw">sort</span>(w, <span class="dt">decreasing =</span> T)</a>
<a class="sourceLine" id="cb144-10" data-line-number="10"></a>
<a class="sourceLine" id="cb144-11" data-line-number="11"><span class="kw">print</span>(w)</a></code></pre></div>
<pre><code>##      B04CHE3     soilmap6      TMDMOD3     soilmap7     soilmap1 
## 6.712914e+01 4.443748e+01 4.010740e+01 3.928593e+01 3.882377e+01 
##      DEMENV5      PRSCHE3      B07CHE3      LCEE103     soilmap2 
## 3.824383e+01 3.350877e+01 3.107515e+01 2.646657e+01 2.478332e+01 
##    soilmap19      LCEE104    soilmap16      LCEE101     soilmap5 
## 2.441025e+01 1.529174e+01 1.448675e+01 1.324205e+01 1.309922e+01 
##    soilmap15      LCEE102     soilmap4    soilmap20     soilmap9 
## 1.295171e+01 1.175225e+01 8.849019e+00 8.166970e+00 6.701250e+00 
##    soilmap10     soilmap3    soilmap17    soilmap12     soilmap8 
## 6.046196e+00 5.059617e+00 4.705567e+00 3.552720e+00 2.691221e+00 
##    soilmap18    soilmap14    soilmap11    soilmap13 
## 5.275093e-01 1.498048e-01 7.365523e-02 1.110223e-15</code></pre>
<p>SVM is a powerful technique which represent another welcome possibility to generate reliable and interpretable SOC predictions across different scales of data availability, including country-specific SOC maps.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-hengl2007regression">
<p>Hengl, Tomislav, Gerard BM Heuvelink, and David G Rossiter. 2007. “About Regression-Kriging: From Equations to Case Studies.” <em>Computers &amp; Geosciences</em> 33 (10). Elsevier:1301–15.</p>
</div>
<div id="ref-Breiman1996">
<p>Breiman, Leo. 1996. “Bagging Predictors.” <em>Machine Learning</em> 24 (2):123–40. <a href="https://doi.org/10.1023/A:1018054314350" class="uri">https://doi.org/10.1023/A:1018054314350</a>.</p>
</div>
<div id="ref-Karatzoglou2006">
<p>Karatzoglou, Alexandros, David Meyer, and Kurt Hornik. 2006. “Support Vector Algorithm in R.” <em>Journal of Statistical Software</em> 15 (9):1–28.</p>
</div>
<div id="ref-guevara_2018">
<p>Guevara, M., G. F. Olmedo, E. Stell, Y. Yigini, Y. Aguilar Duarte, C. Arellano Hernández, G. E. Arévalo, et al. 2018. “No Silver Bullet for Digital Soil Mapping: Country-Specific Soil Organic Carbon Estimates Across Latin America.” <em>SOIL Discussions</em> 2018:1–20. <a href="https://doi.org/10.5194/soil-2017-40" class="uri">https://doi.org/10.5194/soil-2017-40</a>.</p>
</div>
<div id="ref-lettens2004soil">
<p>Lettens, Suzanna, J van Orshoven, B Wesemael, and Bart and Muys. 2004. “Soil Organic and Inorganic Carbon Contents of Landscape Units in Belgium Derived Using Data from 1950 to 1970.” <em>Soil Use and Management</em> 20 (1). Wiley Online Library:40–47.</p>
</div>
<div id="ref-hiederer2013mapping">
<p>Hiederer, Roland. 2013. “Mapping Soil Properties for Europe—Spatial Representation of Soil Database Attributes.” <em>Publications Office of the European Union, EUR26082EN Scientific and Technical Research Series. Luxembourg</em>.</p>
</div>
<div id="ref-krasilnikov2013soils">
<p>Krasilnikov, Pavel, Ma del Carmen Gutiérrez-Castorena, Robert J Ahrens, Carlos Omar Cruz-Gaistardo, Sergey Sedov, and Elizabeth Solleiro-Rebolledo. 2013. <em>The Soils of Mexico</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-greve2007generating">
<p>Greve, Mogens H, Mette B Greve, Peder K Bøcher, Thomas Balstrøm, Henrik Breuning-Madsen, and Lars Krogh. 2007. “Generating a Danish Raster-Based Topsoil Property Map Combining Choropleth Maps and Point Information.” <em>Geografisk Tidsskrift-Danish Journal of Geography</em> 107 (2). Taylor &amp; Francis:1–12.</p>
</div>
<div id="ref-kolli2009stocks">
<p>Kolli, Raimo, Olav Ellermäe, Tiina Köster, Illar Lemetti, Endla Asi, and Karin Kauer. 2009. “Stocks of Organic Carbon in Estonian Soils.” <em>Estonian Journal of Earth Sciences</em> 58 (2).</p>
</div>
<div id="ref-arrouays2001carbon">
<p>Arrouays, D, W Deslais, and V Badeau. 2001. “The Carbon Content of Topsoil and Its Geographical Distribution in France.” <em>Soil Use and Management</em> 17 (1). Wiley Online Library:7–11.</p>
</div>
<div id="ref-bhatti2002estimates">
<p>Bhatti, Jagtar S, Michael J Apps, and C Tarnocai. 2002. “Estimates of Soil Organic Carbon Stocks in Central Canada Using Three Different Approaches.” <em>Canadian Journal of Forest Research</em> 32 (5). NRC Research Press:805–12.</p>
</div>
<div id="ref-walkley1934examination">
<p>Walkley, Aldous, and I Armstrong Black. 1934. “An Examination of the Degtjareff Method for Determining Soil Organic Matter, and a Proposed Modification of the Chromic Acid Titration Method.” <em>Soil Science</em> 37 (1). LWW:29–38.</p>
</div>
<div id="ref-breiman1984classification">
<p>Breiman, Leo, Jerome Friedman, Charles J Stone, and Richard A Olshen. 1984. <em>Classification and Regression Trees</em>. CRC press.</p>
</div>
<div id="ref-poggio2013regional">
<p>Poggio, Laura, Alessandro Gimona, and Mark J Brewer. 2013. “Regional Scale Mapping of Soil Properties and Their Uncertainty with a Large Number of Satellite-Derived Covariates.” <em>Geoderma</em> 209. Elsevier:1–14.</p>
</div>
<div id="ref-rad2014updating">
<p>Rad, Mohammad Reza Pahlavan, Norair Toomanian, Farhad Khormali, Colby W Brungard, Chooghi Bayram Komaki, and Patrick Bogaert. 2014. “Updating Soil Survey Maps Using Random Forest and Conditioned Latin Hypercube Sampling in the Loess Derived Soils of Northern Iran.” <em>Geoderma</em> 232. Elsevier:97–106.</p>
</div>
<div id="ref-wiesmeier2011digital">
<p>Wiesmeier, Martin, Frauke Barthold, Benjamin Blank, and Ingrid Kögel-Knabner. 2011. “Digital Mapping of Soil Organic Matter Stocks Using Random Forest Modeling in a Semi-Arid Steppe Ecosystem.” <em>Plant and Soil</em> 340 (1-2). Springer:7–24.</p>
</div>
<div id="ref-Breiman2001">
<p>Breiman, L. 2001. “Random forests.” <em>Machine Learning</em>, 5–32. <a href="https://doi.org/10.1023/A:1010933404324" class="uri">https://doi.org/10.1023/A:1010933404324</a>.</p>
</div>
<div id="ref-hengl2014soilgrids1km">
<p>Hengl, Tomislav, Jorge Mendes de Jesus, Robert A MacMillan, Niels H Batjes, Gerard BM Heuvelink, Eloi Ribeiro, Alessandro Samuel-Rosa, et al. 2014. “SoilGrids1km—Global Soil Information Based on Automated Mapping.” <em>PLoS One</em> 9 (8). Public Library of Science:e105992.</p>
</div>
<div id="ref-hengl2017">
<p>Hengl T., Kempen B., Heuvelink G.B.M. 2017. “Methods to Fit a Regression-Kriging Model.” 2017. <a href="http://gsif.r-forge.r-project.org/fit.gstatModel.html" class="uri">http://gsif.r-forge.r-project.org/fit.gstatModel.html</a>.</p>
</div>
<div id="ref-Bonfatti2016">
<p>Bonfatti, Benito R., Alfred E. Hartemink, Elvio Giasson, Carlos G. Tornquist, and Kabindra Adhikari. 2016. “Digital mapping of soil carbon in a viticultural region of Southern Brazil.” <em>Geoderma</em> 261. Elsevier B.V.:204–21. <a href="https://doi.org/10.1016/j.geoderma.2015.07.016" class="uri">https://doi.org/10.1016/j.geoderma.2015.07.016</a>.</p>
</div>
<div id="ref-breiman2017cutler">
<p>Breiman, L. 2017. “Cutler’s Random Forests for Classification and Regression. 2015. R Package Version 4.6. 12.”</p>
</div>
<div id="ref-mcbratney2003digital">
<p>McBratney, Alex B, ML Mendonça Santos, and Budiman Minasny. 2003. “On Digital Soil Mapping.” <em>Geoderma</em> 117 (1). Elsevier:3–52.</p>
</div>
<div id="ref-Florinsky2012">
<p>Florinsky, I. V. 2012. “The Dokuchaev Hypothesis as a Basis for Predictive Digital Soil Mapping (on the 125th Anniversary of Its Publication).” <em>Eurasian Soil Science</em> 45 (4):445–51. <a href="https://doi.org/10.1134/S1064229312040047" class="uri">https://doi.org/10.1134/S1064229312040047</a>.</p>
</div>
<div id="ref-meinshausen2006quantile">
<p>Meinshausen, Nicolai. 2006. “Quantile Regression Forests.” <em>Journal of Machine Learning Research</em> 7 (Jun):983–99.</p>
</div>
<div id="ref-kuhn2017caret">
<p>Kuhn, Max, Jed Wing, Steve Weston, Andre Williams, Chris Keefer, Allan Engelhardt, and others. 2017. “Caret: Classification and Regression Training. 2016.” <em>R Package Version</em> 4.</p>
</div>
<div id="ref-vaysse2017using">
<p>Vaysse, Kévin, and Philippe Lagacherie. 2017. “Using Quantile Regression Forest to Estimate Uncertainty of Digital Soil Mapping Products.” <em>Geoderma</em> 291. Elsevier:55–64.</p>
</div>
<div id="ref-cortes1995support">
<p>Cortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector Networks.” <em>Machine Learning</em> 20 (3). Springer:273–97.</p>
</div>
<div id="ref-scikit">
<p>Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” <em>Journal of Machine Learning Research</em> 12:2825–30.</p>
</div>
<div id="ref-e1071">
<p>Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2017. <em>E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), Tu Wien</em>. <a href="https://CRAN.R-project.org/package=e1071" class="uri">https://CRAN.R-project.org/package=e1071</a>.</p>
</div>
<div id="ref-chang2001libsvm">
<p>Chang, Chih-Chung, and Chih-Jen Lin. 2001. “LIBSVM: A Library for Support Vector Machines [Eb/Ol].”</p>
</div>
<div id="ref-vapnik2013nature">
<p>Vapnik, Vladimir. 2013. <em>The Nature of Statistical Learning Theory</em>. Springer science &amp; business media.</p>
</div>
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. Springer series in statistics New York.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
<div id="ref-hsu2003practical">
<p>Hsu, Chih-Wei, Chih-Chung Chang, Chih-Jen Lin, and others. 2003. “A Practical Guide to Support Vector Classification.” Taipei.</p>
</div>
<div id="ref-guyon2003introduction">
<p>Guyon, Isabelle, and André Elisseeff. 2003. “An Introduction to Variable and Feature Selection.” <em>Journal of Machine Learning Research</em> 3 (Mar):1157–82.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="covariates.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chvalidation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/FAO-GSP/SOC-Mapping-Cookbook/edit/master/061-mapping.Rmd",
"text": "Edit"
},
"download": ["SOCMapping.pdf", "SOCMapping.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
