\clearpage
## Data minning: Support Vector Machines {#svm}
*GF Olmedo & M Guevara*

### Overview 

Support vector machines (svm) is a machine learning technique used for classification and regression analysis. According to @scikit the advantages of svm are:

* Effective in high dimensional spaces.
* Still effective in cases where the number of dimensions is greater than the number of samples.
* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.
* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.

And the disadvantages of support vector machines include:

* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.
* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).

In digital soil mapping, the problems usually involve working in high dimensional spaces (were the dimensions are the covariates) with a limited number of samples. svm is a technique mostly used in classification problems, but it can be used to solve regression problems, like the modeling of SOC using environmental covariates. When svm is used to solve regression problem is called support vector regression. 

For doing support vector regression, svm applies a simple linear method to the data but in a high-dimensional feature space non-linearly related to the input space. Then, it creates a hyperplane through n-dimensional spectral-space. And separates numerical data based on a kernel function and parameters (e.g. gamma and cost) that maximize the margin from the closest point to the hyperplane that divides data with the largest possible margin, being the support vectors the points which fall within. Then, linear models are fitted to the support vectors [@soil-2017-40].

In the example below, we will use the implementation of svm in the R package `e1071` [@e1071]. The package e1071 offers an interface to the award-winning C++ implementation by Chih-Chung Chang and Chih-Jen Lin, libsvm (current version: 2.6). For further implementation details on libsvm, see @chang2001libsvm.

svm is a huge area of study, we can recommend the following books: @vapnik2013nature, @friedman2001elements, and @james2013introduction.

### Technical Steps - Fitting an SVM Model to Predict the SOC

#### Setting Working Space and Initial Steps

One of the first steps should be setting our working directory. If you read/write files from/ to disk, this takes place in the working directory. If we don’t set the working directory we could easily write files to an undesirable file location. The following example shows how to set the working directory in R to our folder which contains data for the study area (point data, covariates).

Note that we must use the forward slash / or double backslash \\\\ in R! Single backslash \\ will not work. Now we can check if the working directory has been correctly set by using the function:

```{r, eval=FALSE}
getwd()
```


#### Data Preparation

**Point Dataset**

We previously applied spline function to produce continuous soil information to a given soil depth (0-30 cm) in the section \@ref(EqualAreaSplines). Spline function basically imports soil profile data (including instances where layers are not contiguous), fits it to a mass-preserving spline and outputs attribute means for a given depth. The output file should contain profile id, upper (surface) and lower depth (30cm), estimated value for the selected soil attribute (Value) and tmse (estimated mean squared error of the spline). If you used the Spline Tool V2, the coordinates were not kept in the output file. The coordinates should be added back in the data table. You can use Profile IDs to add the X, Y columns back. Once your point dataset is ready, copy this table into your working directory as a .csv file.

**Environmental Predictors (Covariates)**

In the Chapter \@ref(covariates), we presented and prepared several global and continental datasets. In addition to these datasets, numerous covariate layers have been prepared by ISRIC for the GSOC Map project. These are GIS raster layers of various biophysical earth surface properties for each country in the world. Some of these layers will be used as predictors in this section. Please download the covariates for your own study area from GSOCMap Data Repository as explained in Section \@ref(GSOCDataRepo).

In section \@ref(overlay-soil-covariates), a table with the points values after data preparation and the values of our spatial predictors was prepared. This step involves loading this table.

Now we will import our point dataset using `read.csv()` function. The easiest way to create a data frame is to read in data from a file—this is done using the function read.csv, which works with comma delimited files. Data can be read in from other file formats as well, using different functions, but read.csv is the most commonly used approach. R is very flexible in how it reads in data from text files (`read.table`, `read.csv`, `read.csv2`, `read.delim`, `read.delim2`). Please type `?read.table()` for help.

```{r}
# load data
dat <- read.csv("data/MKD_RegMatrix.csv")

dat$LCEE10 <- as.factor(dat$LCEE10)
dat$soilmap <- as.factor(dat$soilmap)

# explore the data structure
str(dat)
```

Since we will be working with spatial data we need to define the coordinates for the imported data. Using the coordinates()  function from the sp package we can define the columns in the data frame to refer to spatial coordinates—here the coordinates are listed in columns X and Y.

```{r}
library(sp)

# Promote to spatialPointsDataFrame
coordinates(dat) <- ~ X + Y

class(dat)
```

SpatialPointsDataFrame structure is essentially the same data frame, except that additional “spatial” elements have been added or partitioned into slots. Some important ones being the bounding box (sort of like the spatial extent of the data), and the coordinate reference system proj4string(), which we need to define for the sample dataset. To define the CRS, we must know where our data are from, and what was the corresponding CRS used when recording the spatial information in the field. For this data set, the CRS used was: WGS84  (EPSG:4326).

To clearly tell R this information we define the CRS which describes a reference system in a way understood by the [PROJ.4 projection library](http://trac.osgeo.org/proj/). An interface to the PROJ.4 library is available in the rgdal package. As an alternative to using Proj4 character strings, we can use the corresponding yet simpler EPSG code (European Petroleum Survey Group). rgdal also recognizes these codes. If you are unsure of the Proj4 or EPSG code for the spatial data that you have but know the CRS, you should consult http://spatialreference.org/ for assistance.

Please also note that, when working with spatial data, it’s very important that the CRS (coordinate reference system) of the point data and covariates are the same.

Now, we will define our CRS:

```{r}
dat@proj4string <- CRS(projargs = "+init=epsg:4326")

dat@proj4string
```

Now we will import the covariates. When the covariate layers are in common resolution and extent, rather than working with individual rasters it is better to stack them all into a single R object. In this example, we use 13 covariates from the GSOCMap Data Repository and a rasterized version of the soil type map. The rasterization of vectorial data was covered in [Technical Steps - Rasterizing a vector layer in R]. The file containing all the covariates was prepared at the end of chapter \@ref(covariates).

```{r}
load(file = "covariates.RData")

names(covs)
```

#### Variable selection using correlation analysis

```{r}
# plot the names of the covariates
names(dat@data)
```

For the variable selection we will use `cor()` function. `x` must be a table including only the column with the response variable, and `y` must be a table including ONLY the covariates. Besides, remember `dat@data` in the `data.frame` included in the `spatialPointsDataFrame`. For `y`, columns 1 to 7 are out, because they are not covariates. At the same time, correlation analysis cannot be applied to categorical covariates, this means that columns 13 and 21 have to be removed too.

```{r variable selection in svm}
selectedCovs <- cor(x = as.matrix(dat@data[,5]),
           y = as.matrix(dat@data[,-c(1:7,13,21)]))

# print correlation results
selectedCovs
```

Now we used the correlation results to select the top five covariates.

```{r var selection in svm continued}
library(reshape)
x <- subset(melt(selectedCovs), value != 1 | value != NA)
x <- x[with(x, order(-abs(x$value))),]

idx <- as.character(x$X2[1:5])

dat2 <- dat[c('OCSKGM', idx)]
names(dat2)

COV <- covs[[idx]]

# Selected covariates
names(COV)
```


#### Categorical variables in svm models

According to @hsu2003practical, svm requires each variable to be represented by a vector of real numbers. This means that factor variables, like `covs$LCEE10` and `covs$soilmap`has to be converted into numeric data. In statistics, this kind of variables are called boolean indicators or dummy variables. Dummy variables take a value of 0 or 1 indicating the presence or absence of a specific value/category in our factor covariate, i.e. if we have 5 categories like in `covs$LCEE10`, we will have 5 dummy variables indicating the presence/absence of every category. For converting our covariates to dummies we will have to create a new function that returns the dummy rasterStack from the factor version of the rasterLayer.

```{r}
dummyRaster <- function(rast){
  rast <- as.factor(rast)
  result <- list()
  for(i in 1:length(levels(rast)[[1]][[1]])){
    result[[i]] <- rast == levels(rast)[[1]][[1]][i]
    names(result[[i]]) <- paste0(names(rast), 
                                 levels(rast)[[1]][[1]][i])
  }
  return(stack(result))
}
```

We can use the function we just created to convert our categorical covariates to dummies and then stack all the layers together.

```{r}
# convert soilmap from factor to dummy
soilmap_dummy <- dummyRaster(covs$soilmap)

# convert LCEE10 from factor to dummy
LCEE10_dummy <- dummyRaster(covs$LCEE10)

# Stack the 5 COV layers with the 2 dummies
COV <- stack(COV, soilmap_dummy, LCEE10_dummy)

# print the final layer names
names(COV)
```

We have to convert the columns with categorical variables in the soil samples `data.frame` to dummies as well. For doing this we can use function `model.matrix()`. After this, we use `cbind()` to merge the resulting `data.frame`.

```{r}
# convert soilmap column to dummy, the result is a matrix
# to have one column per category we had to add -1 to the formula
dat_soilmap_dummy <- model.matrix(~soilmap -1, data = dat@data)
# convert the matrix to a data.frame
dat_soilmap_dummy <- as.data.frame(dat_soilmap_dummy)


# convert LCEE10 column to dummy, the result is a matrix
# to have one column per category we had to add -1 to the formula
dat_LCEE10_dummy <- model.matrix(~LCEE10 -1, data = dat@data)
# convert the matrix to a data.frame
dat_LCEE10_dummy <- as.data.frame(dat_LCEE10_dummy)

dat@data <- cbind(dat@data, dat_LCEE10_dummy, dat_soilmap_dummy)

names(dat@data)
```


#### Fitting a svm model

To improve the model performance the parameters of the svm can be tuned. In this example, we will show how to tune 2 parameters using a grid search for hyperparameter optimization using the function `tune()`. The first parameter is epsilon which is the insensitive-loss function. The default value for epsilon is 0.1, and we will try 11 different value from 0.05 to 0.12 in 0.1 increments. the second parameter is the cost which is the cost of constraints violation -- it is the ‘C’-constant of the regularization term in the Lagrange formulation. The default value for this parameter is 1, and we will try values from 1 to 20 in 5 increments. The value of cost helps us to avoid overfitting. this is a very heavy step in computational terms and can take a lot of time because we will have to try a lot of different models. 

```{r svm RUN.ALL, echo=FALSE, eval=TRUE, results='hide', warning=FALSE}
library(e1071)
library(caret)
if(RUN.ALL == TRUE){
  # run the model
  tuneResult <- tune(svm, OCSKGM ~.,  data = dat@data[,c("OCSKGM",
                                                         names(COV))],
                     ranges = list(epsilon = seq(0.1,0.2,0.02),
                                   cost = c(5,7,15,20)))
  # save the model for later
  saveRDS(object = tuneResult, file = "results/svm.model.Rds")
}
if(RUN.ALL == FALSE){
  # load pre calculated model
  tuneResult <- readRDS("results/svm.model.Rds")
}
```

```{r svm model tunning, eval=FALSE}
library(e1071)
library(caret)

#  Test different values of epsilon and cost
  tuneResult <- tune(svm, OCSKGM ~.,  data = dat@data[,c("OCSKGM",
                                                         names(COV))],
                     ranges = list(epsilon = seq(0.1,0.2,0.02),
                                   cost = c(5,7,15,20)))
```

We can plot the performance of the different models. When the region is darker, the RMSE is closer to zero. 

```{r, fig.cap="Performance of the different svm models in the parameter tuning procedure"}
plot(tuneResult)
```


#### Select the model with the best combination of epsilon and cost

The best model is chosen as the one with the lowest mean squared error using cross-validation. The parameters for the cross-validation can be defined in the `tune.control()` function. By default, it uses cross-validation using 10 partitions.

```{r}
# Choose the model with the best combination of epsilon and cost
tunedModel <- tuneResult$best.model

print(tunedModel)
```

#### Predict the OCS using the model

```{r, fig.cap="SOC prediction using a support vector machines model"}
# Use the model to predict the SOC in the covariates space
OCSsvm <- predict(COV, tunedModel)

# Save the result
writeRaster(OCSsvm, filename = "results/MKD_OCSKGM_svm.tif",
            overwrite=TRUE)

plot(OCSsvm)
```

Finally, we can evaluate the contribution of each covariate to the model [@guyon2003introduction]:

```{r, echo=TRUE, eval=TRUE}
# Variable importance in svm. Code by:
# stackoverflow.com/questions/34781495

w <- t(tunedModel$coefs) %*% tunedModel$SV     # weight vectors
w <- apply(w, 2, function(v){sqrt(sum(v^2))})  # weight

w <- sort(w, decreasing = T)
print(w)
```


