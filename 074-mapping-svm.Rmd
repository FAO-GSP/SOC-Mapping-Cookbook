\clearpage
## Data minning: Support Vector Machines
*GF Olmedo & M Guevara*

SVM apply a simple linear method to the data but in a high-dimensional feature space non-linearly related to the input space. It creates a hyperplane through n-dimensional spectral-space. Then, SVM separates numerical data based on a kernel function and parameters (e.g. gamma and cost) that maximize the margin from the closest point to the hyperplane that divides data with the largest possible margin, being the support vectors the points which fall within. Then, linear models are fitted to the support vectors.

### Technical Steps - Fitting a SVM Model to Predict the SOC

#### Setting Working Space and Initial Steps

One of the first steps should be setting our working directory. If you read/write files from/ to disk, this takes place in the working directory. If we don’t set the working directory we could easily write files to an undesirable file location. The following example shows how to set the working directory in R to our  folder which contains data for the study area (point data, covariates).

Note that we must use the forward slash / or double backslash \\ in R! Single backslash \ will not work. Now we can check if the working directory has been correctly set by using the function:

  Now load the necessary R packages (you may need to install them onto your computer first):

  #### Data Preparation

  **Point Dataset**

  We previously applied spline function to produce continuous soil information to a given soil depth (0-30 cm) in the section 2.4. Spline function basically imports soil profile data (including instances where layers are not contiguous), fits it to a mass-preserving spline and outputs attribute means for a given depth. The output file should contain profile id, upper (surface) and lower depth (30cm), estimated value for the selected soil attribute (Value) and tmse (estimated mean squared error of the spline). If you used the Spline Tool V2, the coordinates were not kept in the output file. The coordinates should be added back in the data table. You can use Profile IDs to add the X, Y columns back. Once your point dataset is ready, copy this table into your working directory as a .csv file.

**Environmental Predictors (Covariates)**

  In the Chapter 3, several global and continental datasets and access information can be found. In addition to these datasets, numerous covariate layers have been prepared by ISRIC for the GSOC Map project. These are GIS raster layers of various biophysical earth surface properties for each country in the world. Some of these layers will  be used  as predictors in this section. Please download the covariates for your own study area from GSOCMap Data Repository

In section [Overlay covariates and spatial data],a table with the points values after data preparation and the values of our spatial predictors was prepared. This step involves loading this table.

Now we will import our point dataset using `read.csv()` function. The easiest way to create a data frame is to read in data from a file—this is done using the function read.csv, which works with comma delimited files. Data can be read in from other file formats as well, using different functions, but read.csv is the most commonly used approach. R is very flexible in how it reads in data from text files (`read.table`, `read.csv`, `read.csv2`, `read.delim`, `read.delim2`). Please type `?read.table()` for help.

```{r}
# load data
dat <- read.csv("data/MKD_RegMatrix.csv")

dat$LCEE10 <- as.factor(dat$LCEE10)
dat$soilmap <- as.factor(dat$soilmap)

# explore the data structure
str(dat)
```

Since we will be working with spatial data we need to define the coordinates for the imported data. Using the coordinates()  function from the sp package we can define the columns in the data frame to refer to spatial coordinates—here the coordinates are listed in columns X and Y.

```{r}
library(sp)

# Promote to spatialPointsDataFrame
coordinates(dat) <- ~ X + Y

class(dat)
```

SpatialPointsDataFrame structure is essentially the same data frame, except that additional “spatial” elements have been added or partitioned into slots. Some important ones being the bounding box (sort of like the spatial extent of the data), and the coordinate reference system proj4string(), which we need to define for the sample dataset. To define the CRS, we must know where our data are from, and what was the corresponding CRS used when recording the spatial information in the field. For this data set the CRS used was: WGS84  (EPSG:4326).

To clearly tell R this information we define the CRS which describes a reference system in a way understood by the PROJ.4 projection library http://trac.osgeo.org/proj/. An interface to the PROJ.4 library is available in the rgdal package. Alternative to using Proj4 character strings, we can use the corresponding yet simpler EPSG code (European Petroleum Survey Group). rgdal also recognizes these codes. If you are unsure of the Proj4 or EPSG code for the spatial data that you have, but know the CRS, you should consult http://spatialreference.org/ for assistance.

Please also note that, when working with spatial data, it’s very important that the CRS (coordinate reference system) of the point data and covariates are the same.

Now, we will define our CRS:

  ```{r}
dat@proj4string <- CRS(projargs = "+init=epsg:4326")

dat@proj4string
```

Now we will import the covariates. When the covariate layers are in common resolution and extent, rather than working with individual rasters it is better to stack them all into a single R object. We will use stack() function from raster package. In this example we use 13 covariates from the GSOCMap Data Repository and a rasterized version of the soil type map. The rasterization of vectorial data was covered in [Technical Steps - Rasterizing a vector layer in R].

```{r}
library(raster)

# list all the itf files in the folder covs/
files <- list.files(path = "covs", pattern = "tif$",
                    full.names = TRUE)

# load all the tif files in one rasterStack object
covs <- stack(files)

# load the vectorial version of the soil map
soilmap <- shapefile("MK_soilmap_simple.shp")

# rasterize using the Symbol layer
soilmap@data$Symbol <- as.factor(soilmap@data$Symbol)
soilmap.r <- rasterize(x = soilmap, y = covs[[1]], field = "Symbol")

# stack the soil map and the other covariates
covs <- stack(covs, soilmap.r)

# correct the name for layer 14
names(covs)[14] <- "soilmap"

# print the names of the 14 layers:
names(covs)
```

#### Variable selection using correlation analysis

```{r}
# plot the names of the covariates
names(dat@data)
```

For the variable selection we will use `cor()` function. `x` must be a table including only the column with the response variable, and `y` must be a table including ONLY the covariates. Besides, remember `dat@data` in the `data.frame` included in the `spatialPointsDataFrame`. For `y`, columns 1 to 7 are out, because they are not covariates. At the same time, correlation analysis cannot be applied to categorical covariates, this means that columns 13 and 21 have to be removed too.

```{r variable selection in svm}
COR <- cor(x = as.matrix(dat@data[,6]),
           y = as.matrix(dat@data[,-c(1:7,13,21)]))

# print correlation results
COR
```

Now we used the correlation results to select the top five covariates.

```{r var selection in svm continued}
library(reshape)
x <- subset(melt(COR), value != 1 | value != NA)
x <- x[with(x, order(-abs(x$value))),]

idx <- as.character(x$X2[1:5])

dat2 <- dat[c('OCSKGM', idx)]
names(dat2)

COV <- covs[[idx]]

# we add back the two categorical covariates: LCEE10 and soilmap
# COV <- stack(COV, covs$LCEE10, covs$soilmap)

# Top 10 covariates
names(COV)
```


#### Fitting a svm model using different epsilon and cost parameters

```{r svm RUN.ALL, echo=FALSE, eval=TRUE}
library(e1071)
library(caret)
if(RUN.ALL == TRUE){
  # run the model
  tuneResult <- tune(svm, OCSKGM ~.,  data = dat@data[,c("OCSKGM",
                                                         names(COV))],
                     ranges = list(epsilon = seq(0,0.5,0.1),
                                   cost = seq(4,6,1)))
  # save the model for later
  saveRDS(object = tuneResult, file = "results/svm.model.Rds")
}
if(RUN.ALL == FALSE){
  # load pre calculated model
  tuneResult <- readRDS("results/svm.model.Rds")
}
```


```{r svm model tunning, eval=FALSE}
library(e1071)
library(caret)

#  Test different values of epsilon and cost
tuneResult <- tune(svm, OCSKGM ~.,  data = dat@data[,c("OCSKGM",
                                                       names(COV))],
                   ranges = list(epsilon = seq(0,0.5,0.1),
                                 cost = seq(4,6,1)))
```

#### Select the model with the best combination of epsilon and cost

```{r}
tuneResult$best.model

# Choose the model with the best combination of epsilon and cost
tunedModel <- tuneResult$best.model
```

#### Predict the OCS using the model

```{r, fig.cap="SOC prediction using a support vector machines model"}
# Use the model to predict the SOC in the covariates space
OCSsvm <- predict(COV, tunedModel)

plot(OCSsvm)
```

Finally, we can evaluate the contribution of each covariate to the model [@guyon2003introduction]:

```{r, echo=TRUE, eval=TRUE}
# Variable importance in svm. Code by:
# stackoverflow.com/questions/34781495/how-to-find-important-factors-
# in-support-vector-machine

w <- t(tunedModel$coefs) %*% tunedModel$SV     # weight vectors
w <- apply(w, 2, function(v){sqrt(sum(v^2))})  # weight

w <- sort(w, decreasing = T)
print(w)
```


